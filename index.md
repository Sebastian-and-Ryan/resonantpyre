<html>
<head>
<meta charset="utf-8">
<meta name="generator" content="hakyll">
<meta name="google-site-verification" content="BOhOQI1uMfsqu_DopVApovk1mJD5ZBLfan0s9go3phk">
<meta name="title" content="Gwern.net Index of Essays">
<meta name="author" content="gwern">
<meta name="description" content="Writer, self-experimenter, and programmer: psychology, statistics, and technology. This index page is a categorized list of gwern.net pages.">
<meta name="keywords" content="statistics, darknet markets, Bitcoin, blinded self-experiments, Quantified Self, dual n-back, spaced repetition, modafinil, literary criticism">
<meta name="dc.date.issued" content="27 January 2009">
<meta name="dcterms.modified" content="2 june 2020">
<link rel="canonical" href="https://www.gwern.net/index">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Essays - Gwern.net</title>

<link rel="preload" href="/images/logo-smooth.svg" as="image" type="image/svg+xml">


<style> /* index-specific: */
      @media only screen and (max-width:64.9ch) { body.siteIndex #markdownBody li { line-height: 1.65; } }

/*  Add some vertical padding for the introduction.
    */
body.siteIndex #markdownBody {
    padding-top: 1.5ch;
}

/*  Make the sections reflow; better than fixed-width tables.
    */
body.siteIndex article section {
    display: inline-block;
    vertical-align: top;
}

/*  No index self-link.
    */
body.siteIndex #sidebar a#logo {
    pointer-events: none;
}
/*  Abstract is not a real abstract.
    */
body.siteIndex #abstract {
    padding: 0;
    border: none;
    margin: 0;
    box-shadow: none;
}

/*  Less fancy headings.
    */
body.siteIndex section:not(.collapse) > h1:first-child {
    font-size: 1.5em;
    line-height: 1.125;
    box-shadow: none;
    text-align: left;
    font-weight: bold;
    margin-left: 0;
    padding: 0;
}

/*  Nice-looking bottom decoration.
    */
body.siteIndex article,
body.tags article {
    position: relative;
    padding-bottom: 4em;
}
body.siteIndex article::after,
body.tags article::after{
    content: "";
    background-color: #fff;
    background-image: url('/images/logo-smooth.svg');
    display: block;
    position: absolute;
    bottom: 0.75em;
    width: 22px;
    height: 30px;
    background-size: contain;
    background-position: center;
    background-repeat: no-repeat;
    left: 0;
    right: 0;
    margin: auto;
    padding: 0 10px 0 11px;
    z-index: 1;
}
body.siteIndex article::before,
body.tags article::before{
    content: "";
    display: block;
    position: absolute;
    bottom: calc(0.75em + 15px);
    height: 1px;
    border-bottom: 1px dotted #000;
    width: 100%;
    opacity: 0.5;
}

/*  Lists on the home page.
    */
body.siteIndex #markdownBody ul,
body.tags #markdownBody ul {
    margin: 1.25em 3em 0 0;
    padding: 0 0 0 1.375em;
}
body.siteIndex #markdownBody li > ul,
body.tags #markdownBody li > ul {
    margin: 0.25em 0 0.25em 0;
}
body.siteIndex #markdownBody li,
body.tags #markdownBody li {
    margin-top: 0;
}
body.siteIndex #markdownBody > section ~ p {
    border-top: 1px dotted #777;
    margin: 2em 0 0 0;
    padding: 1em 0 0 0;
    font-weight: bold;
}

/*  Internal links on the home page need no decoration.
    */
body.siteIndex #markdownBody a[href^="."]:not([href*="/docs/"]):not([href*="/images/"])::after,
body.siteIndex #markdownBody a[href^="https://www.gwern.net/"]:not([href$=".pdf"])::after {
    content: none;
}

@media only screen and (min-width: 120ch) {
    /*  Leave enough horizontal room to show multiple sections simultaneously,
        table-like
        */
    body.siteIndex #markdownBody {
        width: 135ch;
        max-width: calc(50vw + 42ch);
    }
    body.siteIndex p,
    body.siteIndex hr {
        max-width: 97ch;
    }
}
@media only screen and (max-width: 64.9ch) {
    body.siteIndex #sidebar #logo {
        border: 1px dotted transparent;
        align-self: flex-start;
        padding: 7px 5px 6px 5px;
    }
    body.siteIndex #markdownBody section {
        max-width: 100%;
    }
    body.siteIndex #markdownBody ul,
    body.tags #markdownBody ul {
        overflow-wrap: break-word;
        margin-right: 0;
    }
    body.siteIndex #markdownBody li,
    body.tags #markdownBody li {
        padding: 1px 0;
        margin: 2px 0 0 0;
    }
    body.siteIndex #markdownBody ul > li::before,
    body.tags #markdownBody ul > li::before {
        top: 0.25em;
    }
}

      /* general */
      :root{--GW-blockquote-background-color:#f5f5f5}html{padding:0;margin:0;background-color:#fff;color:#000;font-weight:400;font-family:'Source Serif Pro',Baskerville,'Libre Baskerville',serif}body{max-width:112ch}@media only screen and (max-width:64.9ch){html{font-size:18px}}@media only screen and (min-width:65ch){body{padding:0 1.5ch 0 .5ch;margin:0 auto}@media only screen and (min-width:118.5ch){body{padding:0 6ch 0 .5ch}}main{min-height:100vh;display:flex;flex-flow:column}@media only screen and (min-width:176.1ch){main{position:relative;right:4ch}}@supports (-moz-user-focus:normal){@media only screen and (min-width:176.1ch){main{right:0}}}article{flex:1 1 auto}#sidebar{position:absolute}article,header{margin-left:15ch}@media only screen and (max-width:120ch){article,header{margin-left:14.5ch}}@media only screen and (max-width:112ch){article,header{margin-left:14ch}}@media only screen and (max-width:104ch){article,header{margin-left:13.5ch}}@media only screen and (max-width:96ch){article,header{margin-left:13ch}}}@media only screen and (max-width:64.9ch){body{margin:0 1ch}}#sidebar code{border:none;background-color:transparent;padding:0}#sidebar a{display:block}@media only screen and (min-width:65ch){#sidebar{font-variant:small-caps;padding:0 4ch 0 1ch;width:10ch}#sidebar a#logo{margin:1em 0 2em 0}#sidebar a#logo img{width:64px}#sidebar a.patreon{border-top:1px dotted #aaa}#sidebar a.new{box-shadow:0 1.125em 0 0 #fff inset}#sidebar a.patreon{font-size:.9em;margin-top:1.375em;padding-top:1.25em;box-shadow:0 1.25em 0 0 #fff inset;white-space:nowrap}#sidebar a.mail,#sidebar a.r_gwern{line-height:1;padding-left:.25em;font-size:1.05em}#sidebar a.mail::before,#sidebar a.r_gwern::before{content:'Â°\2007';text-shadow:0 0 0 #000;position:relative;top:.25em}#sidebar code{font-variant:none;text-transform:uppercase;font-size:.7em}}@media only screen and (max-width:64.9ch){#sidebar{justify-content:center;margin:0 0 .5em 0}#sidebar a{border:1px dotted #000;padding:3px 10px;text-align:center;margin:1px}#sidebar a#logo{padding:8px 5px 3px 5px}#sidebar,#sidebar-links{display:flex}#sidebar-links{flex-flow:row wrap;margin:.5em 0 0 0}#sidebar a.links,#sidebar a.site{flex:1 1 20%}#sidebar a.mail,#sidebar a.new,#sidebar a.r_gwern{padding:3px 10px;flex:1 1 auto}#sidebar a.patreon{display:none}#sidebar #logo{margin:calc(.5em + 1px) 1px 1px 0}#sidebar #logo img{width:2.5rem}}header{overflow:auto}header h1{margin:.75em 0;text-align:center;text-transform:none;font-variant:small-caps;font-size:2.5em;line-height:1.15;font-weight:600;letter-spacing:-1px}@media only screen and (max-width:64.9ch){header h1{font-size:2em}}#page-metadata hr{display:none}#page-metadata{margin:0 0 2rem 0;overflow:auto;font-size:.95em;line-height:1.5}#page-metadata #page-description{display:block;margin:0 auto .5em auto}#page-metadata #page-description+br{display:none}#page-metadata span:nth-of-type(n+3){white-space:nowrap}#TOC{border:1px solid #ccc;background-color:#f9f9f9;font-family:'Lucida Sans Unicode','Source Sans Pro',Helvetica,'Trebuchet MS',sans-serif;margin:0 2rem 1.5rem 0;line-height:1.25;padding:10px 10px 2px 14px;position:relative;z-index:1}#TOC:empty{display:none}@media only screen and (max-width:128ch){#TOC{font-size:.95rem}}@media only screen and (max-width:120ch){#TOC{font-size:.9rem}}@media only screen and (max-width:112ch){#TOC{font-size:.85rem;margin:0 1.5rem 1.25rem 0}}@media only screen and (max-width:104ch){#TOC{font-size:.8rem;margin:0 1.25rem 1rem 0}}@media only screen and (max-width:96ch){#TOC{margin:0 1rem 1rem 0}}@media only screen and (min-width:90ch){#TOC{float:left;max-width:30ch}}@media only screen and (max-width:90ch){#TOC{float:none;margin:2em auto;font-size:1rem}#TOC>ul>li>ul{column-count:2}}@media only screen and (max-width:64.9ch){#TOC a{display:inline-block}#TOC>ul>li>ul{column-count:1}#TOC li li a{padding:0 0 1px 0}#TOC li li li a{padding:0 0 2px 0}#TOC li li li li a{padding:0 0 3px 0}#TOC li li li li a{padding:0 0 4px 0}}#TOC ul{list-style-type:none;padding-left:0;margin-bottom:0;margin-top:4px;padding-left:1.4em;text-indent:0;padding:0}#TOC ul ul{list-style-type:none;padding-left:.7em;margin-top:2px}#TOC li{font-weight:700;margin:5px 0 10px 0;padding-left:1.125em;position:relative;overflow-wrap:break-word}#TOC li li{margin-bottom:0;font-weight:400;font-size:.9em}#TOC p{margin-top:9px;margin-bottom:3px}#TOC a{border:0;display:block;position:relative}#TOC a:hover{background-color:rgba(0,0,0,.05);color:#000}#TOC a:hover::after{content:'';display:inline-block;position:absolute;left:100%;top:0;background-color:#ccc;width:.25em;height:100%}#TOC code{font-family:inherit;font-size:inherit;border:none;padding:0;background-color:inherit}#TOC>ul{counter-reset:htoc_1}#TOC>ul>li::before{counter-increment:htoc_1;content:counter(htoc_1) '\2006  '}#TOC>ul ul{counter-reset:htoc_2}#TOC>ul ul li::before{counter-increment:htoc_2;content:counter(htoc_1) '.' counter(htoc_2) '\2006  '}#TOC>ul ul ul{counter-reset:htoc_3}#TOC>ul ul ul li::before{counter-increment:htoc_3;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '\2006  '}#TOC>ul ul ul ul{counter-reset:htoc_4}#TOC>ul ul ul ul li::before{counter-increment:htoc_4;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '\2006  '}#TOC>ul ul ul ul ul{counter-reset:htoc_5}#TOC>ul ul ul ul ul li::before{counter-increment:htoc_5;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '.' counter(htoc_5) '\2006  '}#TOC>ul ul ul ul ul ul{counter-reset:htoc_6}#TOC>ul ul ul ul ul ul li::before{counter-increment:htoc_6;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '.' counter(htoc_5) '.' counter(htoc_6) '\2006  '}#TOC ul li::before{position:absolute;right:calc(100% - 1em);width:12ch;text-align:right;font-weight:400;opacity:.4;pointer-events:none}#TOC ul li:hover::before{opacity:.7}#markdownBody{overflow-wrap:break-word}@media only screen and (min-width:176.1ch){#markdownBody{position:relative}}@media only screen and (min-width:65ch){@media only screen and (max-width:100ch){#markdownBody{line-height:1.45}}@media only screen and (min-width:100.1ch) and (max-width:120ch){#markdownBody{line-height:1.5}}@media only screen and (min-width:120.1ch){#markdownBody{line-height:1.55}}}@supports (-webkit-hyphens:auto) or (-ms-hyphens:auto) or (hyphens:auto){#markdownBody li,#markdownBody p{text-align:justify;-webkit-hyphens:auto;-ms-hyphens:auto;hyphens:auto}}#markdownBody p{font-variant-numeric:oldstyle-nums}#abstract blockquote{margin:0 0 1.5em 0;box-shadow:0 0 0 5px #fff inset;border-color:#bbb;padding:.9rem 1.25rem .95rem 1.25rem}h1{margin:1.25em 0 .5em -.75rem;font-weight:700;position:relative}@media only screen and (max-width:65ch){h1{margin:1.25em 0 .5em 0}}h1{font-feature-settings:'smcp';font-size:1.75em;line-height:1.25;letter-spacing:-.75px}a{color:#3c3c3c;text-decoration:none}#markdownBody a{word-wrap:break-word}article>:not(#TOC) a:link{text-decoration:none;background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#333,#333);background-size:.05em 1px,.05em 1px,1px 1px;background-repeat:no-repeat,no-repeat,repeat-x;background-position:0 90%,100% 90%,0 90%;text-shadow:.03em 0 #fff,-.03em 0 #fff,0 .03em #fff,0 -.03em #fff,.06em 0 #fff,-.06em 0 #fff,.09em 0 #fff,-.09em 0 #fff,.12em 0 #fff,-.12em 0 #fff,.15em 0 #fff,-.15em 0 #fff;font-variant-numeric:lining-nums}article>:not(#TOC) a:hover{background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#999,#999)}#markdownBody ol,#markdownBody ul{list-style-type:none;margin:1.25em 0 1.5em 0;padding:0 0 0 2.5em;overflow:hidden}#markdownBody li>ol,#markdownBody li>ul{margin:.5em 0}#markdownBody ol>li,#markdownBody ul>li{position:relative;margin:0}#markdownBody ol>li:nth-of-type(n+2),#markdownBody ul>li:nth-of-type(n+2){margin:.5em 0 0 0}#markdownBody ol>li::before,#markdownBody ul>li::before{position:absolute;z-index:1}@media only screen and (max-width:64.9ch){#markdownBody ol,#markdownBody ul{padding:0 0 0 1.75em}}blockquote{margin:1.625em 0 1.75em 0;border:1px solid #ccc;font-size:.95em;padding:1em 1.25em}@media only screen and (min-width:65ch){blockquote{overflow:hidden}}@media only screen and (max-width:64.9ch){blockquote{margin:1.25em 0 1.5em 0}}blockquote,blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote{z-index:-2;background-color:var(--GW-blockquote-background-color)}blockquote blockquote,blockquote blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote blockquote{background-color:#e6e6e6}article>:not(#TOC) a:link q,article>:not(#TOC) blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote blockquote a:link,article>:not(#TOC) q a:link,article>:not(#TOC) span.quote-mark.open+a:link{text-shadow:.03em 0 var(--GW-blockquote-background-color),-.03em 0 var(--GW-blockquote-background-color),0 .03em var(--GW-blockquote-background-color),0 -.03em var(--GW-blockquote-background-color),.06em 0 var(--GW-blockquote-background-color),-.06em 0 var(--GW-blockquote-background-color),.09em 0 var(--GW-blockquote-background-color),-.09em 0 var(--GW-blockquote-background-color),.12em 0 var(--GW-blockquote-background-color),-.12em 0 var(--GW-blockquote-background-color),.15em 0 var(--GW-blockquote-background-color),-.15em 0 var(--GW-blockquote-background-color)}article>:not(#TOC) blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote blockquote blockquote a:link{text-shadow:.03em 0 #e6e6e6,-.03em 0 #e6e6e6,0 .03em #e6e6e6,0 -.03em #e6e6e6,.06em 0 #e6e6e6,-.06em 0 #e6e6e6,.09em 0 #e6e6e6,-.09em 0 #e6e6e6,.12em 0 #e6e6e6,-.12em 0 #e6e6e6,.15em 0 #e6e6e6,-.15em 0 #e6e6e6}blockquote blockquote{margin:1em 1px}#markdownBody blockquote blockquote:first-child{margin:.25em 1px 1em 1px}#markdownBody blockquote>:last-child,#markdownBody blockquote>:last-child>:last-child,#markdownBody blockquote>:last-child>:last-child>:last-child,#markdownBody blockquote>:last-child>:last-child>:last-child>:last-child{margin-bottom:0}#markdownBody blockquote>:first-child,#markdownBody blockquote>:first-child>:first-child,#markdownBody blockquote>:first-child>:first-child>:first-child,#markdownBody blockquote>:first-child>:first-child>:first-child>:first-child{margin-top:0}#markdownBody blockquote>:last-child>:last-child>:last-child>table:last-child,#markdownBody blockquote>:last-child>:last-child>table:last-child,#markdownBody blockquote>:last-child>table:last-child,#markdownBody blockquote>table:last-child{margin-bottom:.5em}#markdownBody blockquote>:first-child>:first-child>:first-child>table:first-child,#markdownBody blockquote>:first-child>:first-child>table:first-child,#markdownBody blockquote>:first-child>table:first-child,#markdownBody blockquote>table:first-child{margin-top:.5em}#markdownBody blockquote>ol:only-child,#markdownBody blockquote>ul:only-child{margin-left:1.5em}blockquote p>a:first-child code:first-child,blockquote p>code:first-child{border:none;background-color:transparent;font-weight:700;font-family:inherit;padding:0;font-size:inherit}blockquote table{font-size:.7em}p{margin:0}p+p{text-indent:2.5em}@media only screen and (max-width:64.9ch){p+p{text-indent:1em}}code{padding:0 4px;font-family:'Liberation Mono',Consolas,Courier,monospace;font-size:.9em;border:1px solid #c8c8c8;background-color:#fafafa}pre{overflow:auto;margin:1.75em auto;border:1px solid #c8c8c8;background-color:#fafafa;cursor:text;max-height:calc(100vh - 8em)}pre code{display:block;padding:8px 14px;margin:0;border:none;background-color:transparent}span.smallcaps{font-feature-settings:'smcp'}span.allcaps{text-transform:uppercase}hr{border:none;height:0;border-bottom:1px solid #aaa;margin:1em 0}#adsense{display:block;text-align:center;display:none}#adsense a img{max-width:100%}</style>
<link rel="preload" as="style" href="/static/css/default.css">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-SemiboldItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">


<style>
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSerifPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap;
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 600;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Semibold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }

    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSansPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    </style>

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://cdnjs.cloudflare.com">
<link rel="preconnect" href="https://stats.g.doubleclick.net">
<link rel="shortcut icon" type="image/x-icon" href="./static/img/favicon.png">
<style id="popups-styles">
#popup-container {
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    pointer-events: none;
    z-index: 3;
}
#popup-container > * {
    pointer-events: auto;
}
@media not screen and (hover:hover) and (pointer:fine) {
    #popup-container.popup-visible::before {
        content: "";
        position: absolute;
        left: 0;
        right: 0;
        top: 0;
        bottom: 0;
        pointer-events: auto;
        background-color: #000;
        opacity: 0.4;
    }
}

#popupdiv {
    z-index: 10001;
    font-size: 0.8em;
    box-shadow: 0 0 0 2px #fff;
    position: absolute;
    opacity: 1.0;
    transition: none;
    touch-action: none;
    user-select: none;
}
#popupdiv.fading {
    opacity: 0.0;
    transition:
        opacity 0.75s ease-in 0.1s;
}
#popupdiv > div {
    background-color: #fff;
    padding: 12px 16px 14px 16px;
    border: 3px double #aaa;
    line-height: 1.45;
    overflow: auto;
    overscroll-behavior: none;
    touch-action: none;
    user-select: none;
    min-width: 360px;
    max-width: 640px;
    max-height: calc(100vh - 2 * 3px - 26px);
}
/* TODO: the popups should ideally inherit from the regular CSS once the #markdownBody class is rewritten, and the underlining can be removed */
#popupdiv a { text-decoration: underline; }
#popupdiv a:hover { color: #888; }
#popupdiv > div .data-field {
    text-align: left;
    text-indent: 0;
    hyphens: none;
}
#popupdiv > div .data-field + .data-field {
    margin-top: 0.25em;
}
#popupdiv > div .data-field:empty {
    display: none;
}
#popupdiv > div .data-field.title {
    font-weight: bold;
    font-size: 1.125em;
}
#popupdiv > div .data-field.author-plus-date {
    font-style: italic;
}
#popupdiv > div .data-field.abstract {
    text-align: justify;
    text-indent: 2em;
    hyphens: auto;
}
#popupdiv > div.popup-screenshot {
    padding: 0;
    max-width: unset;
}
#popupdiv > div.popup-screenshot img {
    display: block;
}
#popupdiv > div.popup-screenshot a::after {
    content: none;
}
#popupdiv > div.popup-section-embed,
#popupdiv > div.popup-citation-context {
    height: 100%;
    padding: 12px 24px 14px 24px;
    overflow-x: hidden;
}
#popupdiv > div.popup-section-embed > h1:first-child,
#popupdiv > div.popup-section-embed > h2:first-child,
#popupdiv > div.popup-section-embed > h3:first-child,
#popupdiv > div.popup-section-embed > h4:first-child  {
    margin-top: 0;
}
#popupdiv > div.popup-section-embed > :last-child {
    margin-bottom: 12px;
}
#popupdiv > div .icon {
    background-image: none !important;
    position: relative;
    top: 0.15em;
    font-size: 1.125em;
}
#popupdiv > div .icon::after {
    margin: 0 0.175em 0 0;
    width: 1em;
    height: 1em;
    font-size: 1em;
}
#popupdiv > div .icon:not([href*='.pdf'])::after {
    background-position: center center;
    background-size: 100%;
}
#popupdiv > div .title-link::after {
    content: none;
}

/*  Scroll bar styles (Webkit/Blink only).
    */
#popupdiv > div::-webkit-scrollbar {
    width: 14px;
}
#popupdiv > div::-webkit-scrollbar-thumb {
    background-color: #ccc;
    box-shadow:
        0 0 0 3px #fff inset;
}
#popupdiv > div::-webkit-scrollbar-thumb:hover {
    background-color: #999;
}

/*  Popups on mobile.
    */
@media only screen and (max-width: 64.9ch), not screen and (hover:hover) and (pointer:fine) {
    #popupdiv > div {
        max-width: 100%;
    }
}

/*  Image focus interaction.
    */
#markdownBody #popupdiv img {
    filter: none;
    cursor: initial;
    transform: none;
}
#markdownBody #popupdiv .popup-screenshot a img {
    cursor: pointer;
}

#popupdiv .originalURL {
  font-size: 75%;
}

</style><style id="mode-selector-styles">
    #mode-selector {
        position: absolute;
        right: 3px;
        top: 4px;
        display: flex;
        background-color: #fff;
        padding: 0.125em 0.25em;
        border: 3px solid transparent;
        opacity: 0.3;
        transition:
            opacity 2s ease;
    }
    #mode-selector.hidden {
        opacity: 0;
    }
    #mode-selector:hover {
        transition: none;
        opacity: 1.0;
        border: 3px double #aaa;
    }
    #mode-selector button {
        -moz-appearance: none;
        appearance: none;
        border: none;
        background-color: transparent;
        padding: 0.5em;
        margin: 0;
        line-height: 1;
        font-family: Lucida Sans Unicode, Source Sans Pro, Helvetica, Trebuchet MS, sans-serif;
        font-size: 0.75rem;
        text-align: center;
        color: #777;
        position: relative;
    }
    #mode-selector button:hover,
    #mode-selector button.selected {
        box-shadow:
            0 2px 0 6px #fff inset,
            0 1px 0 6px currentColor inset;
    }
    #mode-selector button:not(:disabled):hover {
        color: #000;
        cursor: pointer;
    }
    #mode-selector button:not(:disabled):active {
        transform: translateY(2px);
        box-shadow:
            0 0px 0 6px #fff inset,
            0 -1px 0 6px currentColor inset;
    }
    #mode-selector button.active:not(:hover)::after {
        content: "";
        position: absolute;
        bottom: 0.25em;
        left: 0;
        right: 0;
        border-bottom: 1px dotted currentColor;
        width: calc(100% - 12px);
        margin: auto;
    }
    </style><style id="mode-styles">@media (prefers-color-scheme:dark) {
    :root {
        --GW-blockquote-background-color: #ddd
    }
    body::before,
    body > * {
        filter: invert(90%)
    }
    body::before {
        content: '';
        width: 100vw;
        height: 100%;
        position: fixed;
        left: 0;
        top: 0;
        background-color: #fff;
        z-index: -1
    }
    img,
    video {
        filter: invert(100%);
    }
    #markdownBody, #mode-selector button {
        text-shadow: 0 0 0 #000
    }
    article > :not(#TOC) a:link {
        text-shadow:
                 0      0 #777,
             .03em      0 #fff,
            -.03em      0 #fff,
                 0  .03em #fff,
                 0 -.03em #fff,
             .06em      0 #fff,
            -.06em      0 #fff,
             .09em      0 #fff,
            -.09em      0 #fff,
             .12em      0 #fff,
            -.12em      0 #fff,
             .15em      0 #fff,
            -.15em      0 #fff
    }
    article > :not(#TOC) blockquote a:link {
        text-shadow:
                 0      0 #777,
             .03em      0 var(--GW-blockquote-background-color),
            -.03em      0 var(--GW-blockquote-background-color),
                 0  .03em var(--GW-blockquote-background-color),
                 0 -.03em var(--GW-blockquote-background-color),
             .06em      0 var(--GW-blockquote-background-color),
            -.06em      0 var(--GW-blockquote-background-color),
             .09em      0 var(--GW-blockquote-background-color),
            -.09em      0 var(--GW-blockquote-background-color),
             .12em      0 var(--GW-blockquote-background-color),
            -.12em      0 var(--GW-blockquote-background-color),
             .15em      0 var(--GW-blockquote-background-color),
            -.15em      0 var(--GW-blockquote-background-color)
    }
    #logo img {
        filter: none;
    }
    #mode-selector {
        opacity: 0.6;
    }
    #mode-selector:hover {
        background-color: #fff;
    }
}</style></head>
<body class="siteIndex">
<main>
<div id="sidebar">
<a id="logo" href="/index" title="index: categorized list of articles">
<img alt="Logo [the logo is a Gothic/Fraktur blackletter capital G]" src="/images/logo-smooth.svg">
</a>
<div id="sidebar-links">
<a class="site" href="/About" title="Site ideals, source, content, traffic, examples, license">Site</a>
<a class="links" href="/Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on">Me</a>
<a class="new" href="/Changelog" title="What's new or updated">New:</a>
<a class="mail" href="https://gwern.substack.com/" title="Monthly mailing list: newsletter signup form"><i>mail</i></a>
<a class="r_gwern" href="https://www.reddit.com/r/gwern/" title="Gwern subreddit: link-sharing &amp; commentary (the links typically are included in the monthly newsletter)"><em><code>/r/gwern</code></em></a>
<a class="patreon" href="https://www.patreon.com/gwern" title="Link to Patreon donation profile to support my writing">support on<br><span>PATREON</span></a>
</div>
</div>
<article>
<div id="markdownBody">
<div id="abstract">
<p><span class="smallcaps">This is the website</span> of <strong>Gwern Branwen</strong>. I write about psychology, statistics, and technology; I am best known for work on the <a href="./tags/Silk-Road" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: Silk Road">darknet markets</a> &amp; <a href="./tags/Bitcoin" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: Bitcoin">Bitcoin</a>, blinded self-<a href="./tags/experiments" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: experiments">experiments</a> &amp; Quantified Self analyses, <a href="./tags/DNB" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: DNB">dual n-back</a> &amp; <a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">spaced repetition</a>, and <a href="./Modafinil">modafinil</a>.</p>
<p>For information about my siteâs philosophy, method, traffic statistics, and implementation, see the <em><a href="./About" class="docMetadata" data-popup-title="About This Website" data-popup-author="Gwern Branwen" data-popup-date="01 Oct 2010" data-popup-abstract="Meta page describing gwern.net site ideals of stable long-term essays which improve over time; technical decisions using Markdown and static hosting; idea sources and writing methodology; metadata definitions; semi-annual web traffic statistics; copyright license">About page</a></em>; for information about myself, my use of other websites, and contact information, see the <em><a href="./Links" class="docMetadata" data-popup-title="Links" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Who am I online &amp; what have I done? Contact information; sites I use; computers and software tools; things I've worked on; psychological profiles">Links page</a></em>; for information about new pages, see the <em><a href="./Changelog" class="docMetadata" data-popup-title="Changelog" data-popup-author="Gwern Branwen" data-popup-date="15 Sep 2013" data-popup-abstract="<p>This page is a changelog for <code>gwern.net</code>: a monthly reverse chronological list of recent major writings/changes/additions.</p><p>Following my writing can be a little difficult because it is often so incremental. So every month, in addition to my regular <a href=&quot;https://www.reddit.com/r/gwern/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;/r/gwern subreddit&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2018-10-01&quot; data-popup-abstract=&quot;A subreddit for posting links of interest and also for announcing updates to gwern.net (which can be &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/.rss&amp;quot;&amp;gt;used as a RSS feed&amp;lt;/a&amp;gt;). Submissions are categorized similar to the monthly newsletter and typically will be collated there.&quot;>/r/Gwern</a> subreddit submissions, I write up reasonably-interesting changes and <a href=&quot;https://gwern.substack.com/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter (TinyLetter subscription page)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews. You can also browse &amp;lt;a href=&amp;quot;https://www.gwern.net/tags/newsletter&amp;quot;&amp;gt;the archives since December 2013&amp;lt;/a&amp;gt;.&quot;>send it out to the mailing list</a> in addition to a compilation of links &amp;amp; reviews (<a href=&quot;./tags/newsletter&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter archives&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews.)&quot;>archives</a>).</p>">Changelog</a></em>; to receive updates, news, &amp; reviews, <em><a href="https://gwern.substack.com/" class="docMetadata" data-popup-title="Gwern.net newsletter (TinyLetter subscription page)" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews. You can also browse <a href=&quot;https://www.gwern.net/tags/newsletter&quot;>the archives since December 2013</a>.">subscribe to the newsletter</a></em> (<a href="./tags/newsletter" class="docMetadata" data-popup-title="Gwern.net newsletter archives" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews.)">archives</a>).</p>
</div>
<section>
<h1 id="newest"><a href="/Changelog" title="Newest items (last 10 from Changelog)">Newest</a></h1>
<ul>
<li><p><a href="https://www.gwern.net/newsletter/2020/05" class="docMetadata" data-popup-title="May 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="May 2020 gwern.net newsletter with anime GAN updates, links on AI scaling, discussion of GPT-3, and 1 book review.">May 2020 newsletter</a></p></li>
<li><p><a href="https://www.gwern.net/Faces#danbooru2019e621-256px-biggan" class="docMetadata" data-popup-title="Danbooru2019+e621 256px BigGAN Model Release" data-popup-author="Tensorfork (Gwern Branwen, Shawn Presser et al)" data-popup-date="2020-05-28" data-popup-abstract="<p>Release of a 256px Big<span class=&quot;smallcaps-auto&quot;>GAN</span> model trained on Danbooru2019 &amp;amp; e621. This is a prototype model testing our ability to train a Big<span class=&quot;smallcaps-auto&quot;>GAN</span> stably for hundreds of thousands of iterations on a <span class=&quot;smallcaps-auto&quot;>TPU</span>-256 pod on 3 million+ anime/illustration images. While the generated samples are far from âphotorealisticâ, they serve as proof of concept thatâunlike our failed Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 scaling experimentsâBig<span class=&quot;smallcaps-auto&quot;>GAN</span> can successfully model anime images with great generality, and that we can potentially scale up to 512px or even 1024px and match the DeepMind ImageNet Big<span class=&quot;smallcaps-auto&quot;>GAN</span> for quality.</p>">Prototype anime GAN</a>; <a href="https://www.gwern.net/Crops#danbooru2019-figures" class="docMetadata" data-popup-title="Danbooru2019 Figures: A Large-Scale Anime Character Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="2020-05-31" data-popup-abstract="<p>The <a href=&quot;/Danbooru2019&quot;>Danbooru2019</a> Figures dataset is a large-scale character anime illustration dataset of <em>n</em>=855,880 images (248GB; minimum width 512px) cropped from Danbooru2019 using the <a href=&quot;https://github.com/jerryli27/AniSeg/&quot;>AniSeg</a> anime character detection model. The images are cropped to focus on a single characterâs entire visible body, extending âportraitâ crops to âfigureâ crops. This is useful for tasks focusing on individual characters, such as character classification or for generative tasks (a corpus for weak models like Style<span class=&quot;smallcaps-auto&quot;>GAN</span>, or data augmentation for Big<span class=&quot;smallcaps-auto&quot;>GAN</span>).</p>">Danbooru2019 Figures dataset</a></p></li>
<li><p><a href="https://www.gwern.net/newsletter/2020/04" class="docMetadata" data-popup-title="April 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="April 2020 gwern.net newsletter with links on music generation, data augmentation, terrorism, Dormin, and 1 documentary review.">April 2020 newsletter</a></p></li>
<li><p><a href="https://www.gwern.net/GPT-2-music#generating-midi-with-10k30k-context-windows" class="docMetadata" data-popup-title="Generating MIDI Music With GPT-2: Generating MIDI by converting to ABC and expanding the GPT-2 context windowâworks, if only just" data-popup-author="Gwern Branwen" data-popup-date="2020-04-25" data-popup-abstract="<p>To expand the <span class=&quot;smallcaps-auto&quot;>ABC</span> <span class=&quot;smallcaps-auto&quot;>GPT-2</span> model to cover a wider variety of musical genres, I turn to the next-most compact widespread music encoding format: <strong><span class=&quot;smallcaps-auto&quot;>MIDI</span></strong>. There are hundreds of thousands of <span class=&quot;smallcaps-auto&quot;>MIDI</span>s which can be <a href=&quot;https://en.wikipedia.org/wiki/Decompiler&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Decompiler&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p>A <b>decompiler</b> is a computer program that takes an executable file as input, and attempts to create a high level source file which can be recompiled successfully. It is therefore the opposite of a compiler, which takes a source file and makes an executable. Decompilers are usually unable to perfectly reconstruct the original source code, and as such, will frequently produce obfuscated code. Nonetheless, decompilers remain an important tool in the reverse engineering of computer software.</p>&quot; title=&quot;!Wikipedia: Decompiler&quot;>decompiled</a> to <span class=&quot;smallcaps-auto&quot;>ABC</span> format, averaging ~10k <span class=&quot;smallcaps-auto&quot;>BPE</span>sâwithin <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span>âs feasible context window when trained on <span class=&quot;smallcaps-auto&quot;>TPU</span>s (which permit training of context windows up to 30k wide).</p><p>We compile the <span class=&quot;smallcaps-auto&quot;>ABC</span> from before and 2 large <span class=&quot;smallcaps-auto&quot;>MIDI</span> datasets, and convert to <span class=&quot;smallcaps-auto&quot;>ABC</span>, yielding ~453k usable <span class=&quot;smallcaps-auto&quot;>ABC-MIDI</span> musical files (~5.1GB of text). We trained JanuaryâApril 2020 on our <span class=&quot;smallcaps-auto&quot;>TPU</span> swarm (with many interruptions), achieving a final loss of ~0.2 (underfit).</p><p>Sampling from the final model is hit-or-miss as it is prone to the likelihood repetition trap and it generates instruments one-by-one so it is common for instruments to be cut off or otherwise broken during sampling (indicating that <em>sampling</em> is increasingly a bigger problem than <em>training</em> for long-range sequence modeling). However, successful pieces are possible, and are musically far more diverse than the folk <span class=&quot;smallcaps-auto&quot;>ABC</span> corpus, with many pleasingly complex samples.</p>">Generating <span class="smallcaps-auto">MIDI</span> Music With <span class="smallcaps-auto">GPT-2</span></a></p></li>
<li><p><a href="./newsletter/2020/03" class="docMetadata" data-popup-title="March 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="March 2020 gwern.net newsletter with links on pandemics, politics, DL; one anime review.">March 2020 newsletter</a></p></li>
 <li><p><a href="https://www.gwern.net/Order-statistics#probability-of-bivariate-maximum" class="docMetadata" data-popup-title="Probability of Bivariate Maximum" data-popup-author="Gwern Branwen" data-popup-date="2020-03-11" data-popup-abstract="<p>Given a sample of <em>n</em> pairs of 2 normal variables A &amp;amp; B which are correlated <em>r</em>, what is the probability <em>P</em><sub>max</sub> that the maximum on the first variable A is also the maximum on the second variable B? This is analogous to many testing or screening situations, such as employee hiring (âwhat is the probability the top-scoring applicant on the first exam is the top-scorer on the second as well?â) or athletic contests (âwhat is the probability the current world champ will win the next championship?â).</p><p>Order statistics has long proven that asymptotically, <em>P</em><sub>max</sub> approaches <math display=&quot;inline&quot; xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><semantics><mfrac><mn>1</mn><mi>n</mi></mfrac><annotation encoding=&quot;application/x-tex&quot;></annotation></semantics></math>. Exact answers are hard to find, but confirm the asymptotics; the closest that exists is for an approximation &amp;amp; special-case of the Ali-Mikhail-Haq copula: which roughly indicates that <em>r</em> functions as a constant factor boost in <em>P</em><sub>max</sub>, and the boost from <em>r</em> fades out as <em>n</em> increases.</p><p>As long as <em>r</em>â 1, âthe tails will come apartâ. <em>n</em> increases the difficult too fast for any fixed <em>r</em> to overcome. This has implications for interpreting extremes and test metrics.</p>">Order Statistics: The Probability of a Double Maximum</a></p></li>
<li><p><a href="./newsletter/2020/02" class="docMetadata" data-popup-title="February 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="February 2020 gwern.net newsletter with links on AI scaling and disasters; 1 book, 1 movie, and 2 opera reviews.">February 2020 newsletter</a></p></li>
<li><p><a href="./Subscripts" class="docMetadata" data-popup-title="Subscripts For Citations" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2020" data-popup-abstract="<p>I propose reviving an old General Semantics notation: borrow from scientific notation and use subscripts like âGwern<sub>2020</sub>â for denoting sources (like citation, timing, or medium). Using subscript indices is flexible, compact, universally technically supported, and intuitive. While (currently) unusual, subscripting might be a useful trick for clearer writing, compared to omitting such information or using standard cumbersome circumlocutions.</p>">On Subscripting Citations</a></p></li>
<li><p><a href="./Danbooru2019" class="docMetadata" data-popup-title="Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2015" data-popup-abstract="<p>Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS <span class=&quot;smallcaps&quot;>COCO</span> fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.</p><p>The best known booru, with a focus on quality, is <a href=&quot;https://danbooru.donmai.us/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Danbooru</a>. We provide a torrent/rsync mirror which contains ~3tb of 3.69m images with 108m tag instances (of 392k defined tags, ~29/image) covering Danbooru from 24 May 2005 through 31 December 2019 (final ID: #3,734,659), providing the image files &amp;amp; a <span class=&quot;smallcaps&quot;>JSON</span> export of the metadata. We also provide a smaller torrent of <span class=&quot;smallcaps&quot;>SFW</span> images downscaled to 512x512px <span class=&quot;smallcaps&quot;>JPG</span>s (295GB; 2,828,400 images) for convenience.</p><p>Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.</p>">Danbooru2019 Released</a></p></li>
<li><p><a href="./GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like GPT-2, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ârealisticâ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpusâs average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the âenvironmentâ for a blackbox DRL algorithm like PPO.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on GPT-2. Having previously used <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>GPT-2 for poetry</a> &amp;amp; <a href=&quot;./GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The model &amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with GPT-2 preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>â10,000 ratings, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model directly model the implied utility of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which optimizes sequences to maximize the reward modelâs output.</p>">Preference Learning <span class="smallcaps-auto">GPT-2</span></a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<p><a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.</p><p><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure></p>">This Waifu Does Not Exist</a><a href="./TWDNE#twdnev3" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist, version 3" data-popup-author="Gwern Branwen" data-popup-date="2020-01-20" data-popup-abstract="Discussion of TWDNEv3, launched January 2020. TWDNEv3 upgrades TWDNEv2 to use 100k anime portraits from an <a href=&quot;/Faces#stylegan-2&quot;>anime portrait StyleGAN 2</a>, an improvement to StyleGAN released in December 2019, which removes the blob artifacts and is generally of somewhat higher visual quality. TWDNEv3 provides images in 3 ranges of diversity, showing off both narrow but high quality samples and more wild samples. It replaces the StyleGAN 1 faces and portrait samples.">v3</a></p></li>
<li><p><a href="https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/" class="docMetadata" data-popup-title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots" data-popup-author="disumbrationist" data-popup-date="2020-01-12" data-popup-abstract="<p>When I originally trained the models in May 2019, Iâd used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI <a href=&quot;https://openai.com/blog/gpt-2-1-5b-release/&quot;>finally released the full 1.5 billion parameter model</a>.</p><p>The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by <a href=&quot;https://www.reddit.com/u/gwern&quot;>/u/gwern</a> (<a href=&quot;https://www.patreon.com/gwern&quot;>hereâs his Patreon</a>) and Shawn Presser (<a href=&quot;https://www.reddit.com/u/shawwwn&quot;>/u/shawwwn</a>), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required <a href=&quot;https://twitter.com/gwern/status/1215005375407112193&quot;>around $70K worth of TPU credits</a>, so in hindsight this upgrade definitely wouldnât have been possible for me to do myself, without their assistance.</p><p>Based on my tests of the new model so far, Iâm pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.</p><p>One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from <em>all</em> the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldnât occur with separate models.</p><p>â¦Here is the full list of new bots to be added: /r/capitalismvsocialism Â¶ /r/chess Â¶ /r/conlangs Â¶ /r/dota2 Â¶ /r/etymology Â¶ /r/fiftyfifty Â¶ /r/hobbydrama Â¶ /r/markmywords Â¶ /r/moviedetails Â¶ /r/neoliberal Â¶ /r/obscuremedia Â¶ /r/recipes Â¶ /r/riddles Â¶ /r/stonerphilosophy Â¶ /r/subsimulatorgpt2 Â¶ /r/subsimulatorgpt2meta Â¶ /r/tellmeafact Â¶ /r/twosentencehorror Â¶ /r/ukpolitics Â¶ /r/wordavalanches Â¶ /r/wouldyourather Â¶ /r/zen</p>">GPT-2 Subreddit Simulator</a></p></li>
<li><p><a href="./Search#case-studies" class="docMetadata" data-popup-title="Internet Search Tips: 14 Case Studies" data-popup-author="Gwern Branwen" data-popup-date="2020-01-21" data-popup-abstract="Followup section to the article covering how to search the Internet effectively: 14 case studies of challenging Internet searches drawn from the past 10 years. I present the problem, and step through the process of finding it, and describe my tacit knowledge and implicit strategies. These case studies hopefully make the prior tips more understandable by showing them off in practice.">14 Internet Search Case Studies</a></p></li>
<li><a href="./GPT-2-music" class="docMetadata" data-popup-title="GPT-2 Folk Music" data-popup-author="Gwern Branwen" data-popup-date="1 Nov 2019" data-popup-abstract="<p>In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp; copying of patterns, without problems with the hidden-state bottleneck.</p><p>I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after <a href=&quot;#spaceless-model&quot;>fixing that</a>, I successfully trained it on <em>n</em>=205,304 ABC music pieces taken from The Session &amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.</p><p>The model &amp;amp; dataset are available for download, and I provide for listening selected <a href=&quot;#samples&quot;>music samples</a> as well as medleys of random samples from throughout training.</p>">GPT-2 Folk Music</a></li>
<li><a href="./Causality#overview-the-current-situation" class="docMetadata" data-popup-title="Causality in the Social Sciences: Overview: The Current Situation" data-popup-author="Gwern Branwen" data-popup-date="2019-12-05" data-popup-abstract="<p>Here is how I currently understand the relationship between correlation and causality, and the collective findings of meta-scientific research:</p><ol type=&quot;1&quot;><li><p><a href=&quot;https://www.gwern.net/Replication&quot;><em>The Replication Crisis</em></a>: a shockingly large fraction of psychological research and other fields is simple random noise which cannot be replicated.</p></li><li><p><a href=&quot;https://www.gwern.net/Everything&quot;><em>Everything Is Correlated</em></a>: when we systematically measure many variables at large scale with large <em>n</em>, we find that âeverything is correlatedââeven things which seem to have no causal relationship whatsoever.</p></li><li><p><a href=&quot;https://www.gwern.net/docs/sociology/1987-rossi&quot;><em>The Metallic Laws</em></a>: empirically, most efforts to change human behavior and sociology and economics and education fail in randomized evaluation and the mean effect size of experiments in meta-analyses typically approaches zero, despite promising correlations.</p></li><li><p><a href=&quot;https://www.gwern.net/Correlation&quot;><em>Correlation â  Causation</em></a>: so, we live in a world where research manufactures many spurious results and, even once we see through the fake findings, finding a correlation is meaningless because everything is correlated to begin with and accordingly, they are little better than experimenting at random, which doesnât work well either.</p><p>But <em>why</em> is correlation â  causation?</p></li><li><p><a href=&quot;https://www.gwern.net/Causality#what-a-tangled-net-we-weave-when-first-we-practice-to-believe&quot;><em>Dense Causal Graphs</em></a>: because, if we write down a causal graph consistent with âeverything is correlatedâ and the empirical facts of average null effects + unpredictive correlations, this implies that all variables are part of enormous dense causal graphs where each variable is connected to several others.</p></li><li><p><a href=&quot;https://www.gwern.net/Causality#heuristics-biases&quot;><em>Incorrect Intuitions</em></a>: This inequality between observable correlations and actual useful causal manipulability merely grows with larger networks, and causal networks in fields like economics or biology are far more complex than those in more ordinary everyday fields like âcatching a ballâ.</p><p>Our intuitions, formed in simple domains designed to have sparse causal networks (it would be bad if balls could make you do random things! your brain is carefully designed to control the influence of any outside forces &amp;amp; model the world as simple for planning purposes), turn out to be profoundly misleading in these other domains.</p></li><li><p><em>No, Really, Correlation â  Causation</em>: This cognitive bias is why correlation â  causation is so difficult to internalize and accept, and honored primarily in the breach even by sophisticated researchers, and is why randomized experiments are historically late developed, neglected, counterintuitive, and criticized when run despite routinely debunking conventional wisdom of experts in almost every field.</p></li></ol>">Correlation &amp; Causality</a></li>
<li><a href="./Hydrocephalus" class="docMetadata" data-popup-title="Hydrocephalus and Intelligence: The Hollow Men" data-popup-author="Gwern Branwen" data-popup-date="28 July 2015" data-popup-abstract="<p>Hydrocephalus is a damaging brain disorder where fluids compress the brain, sometimes drastically decreasing its volume. While often extremely harmful or life-threatening when untreated, some people with severe compression nevertheless are relatively normal, and in one case (Lorber) they have been claimed to have IQs as high as 126 with a brain volume 5% of normal brains. A few of these case studies have been used to argue the extraordinary claim that brain volume has little or nothing to do with intelligence; authors have argued that hydrocephalus suggests enormous untapped cognitive potential which are tapped into rarely for repairs and can boost intelligence on net, or that intelligence/consciousness are non-material or tapping into ESP.</p><p>I point out why this claim is almost certainly untrue because it predicts countless phenomena we never observe, and investigate the claimed examples in more detail: the cases turn out to be suspiciously unverifiable (Lorber), likely fraudulent (Oliveira), or actually low intelligence (Feuillet). It is unclear if high-functioning cases of hydrocephalus even have less brain mass, as opposed to lower proxy measures like brain volume.</p><p>I then summarize anthropologist John Hawksâs criticisms of the original hydrocephalus author: his brain imaging data could not have been as precise as claimed, he studied a selective sample, the story of the legendary IQ 126 hydrocephalus patient raises questions as to how normal or intelligent he really was, and hydrocephalus in general appears to be no more anomalous or hard-to-explain than many other kinds of brain injuries, and in a comparison, hemispherectomies, removing or severing a hemisphere, has produced no anomalous reports of above-average intelligence (just deficits), though they ought to be just the same in terms of repairs or ESP.</p><p>That hydrocephalus cases can reach roughly normal levels of functioning, various deficits aside, can be explained by brain size being one of several relevant variables, brain plasticity enabling cognitive flexibility &amp;amp; recovery from gradually-developing conditions, and overparameterization giving robustness to damage and poor environments, and learning ability. The field of deep learning has observed similar phenomenon in training of artificial neural networks. This is consistent with Lorberâs original contention that the brain was more robust, and hydrocephalus was more treatable, than commonly accepted, but does not support any of the more exotic interpretations since put on his findings.</p><p>In short, there is little anomalous to explain, and standard brain-centric accounts appear to account for existing verified observations without much problem or resort to extraordinary claims.</p>">Hydrocephalus and Intelligence</a></li>
</ul>
</section>
<section>
<h1 id="most-popular"><a href="#most-popular" title="Link to section: 'Most popular'">Popular</a></h1>
<ul>
<li><p><a href="./Silk-Road" class="docMetadata" data-popup-title="Silk Road 1: Theory &amp; Practice" data-popup-author="Gwern Branwen" data-popup-date="11 Jul 2011" data-popup-abstract="<p>The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.</p>">Silk Road 1: Theory &amp; Practice</a></p></li>
<li><p><a href="./DNM-archives" class="docMetadata" data-popup-title="Darknet Market Archives (2013-2015)" data-popup-author="Gwern Branwen" data-popup-date="1 Dec 2013" data-popup-abstract="<p>Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.</p><p>From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>usage</a>, <a href=&quot;./DNM-survival&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market mortality risks&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;30 Oct 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;Silk Road 1&amp;lt;/a&amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;lt;/p&amp;gt;&quot;>lifetimes/characteristics</a>, &amp;amp; <a href=&quot;./DNM-arrests&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor DNM-related arrests, 2011-2015&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;14 Jul 2012&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;lt;/p&amp;gt;&quot;>legal riskiness</a>; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp; documents related to the DNMs as I could.</p><p>This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp; 37+ related forums, representing &amp;lt;4,438 mirrors, and is available for any research.</p><p>This page documents the download, contents, interpretation, and technical methods behind the scrapes.</p>">Darknet Market Archives (2013â2015)</a></p></li>
<li><p><a href="./DNM-arrests" class="docMetadata" data-popup-title="Tor DNM-related arrests, 2011-2015" data-popup-author="Gwern Branwen" data-popup-date="14 Jul 2012" data-popup-abstract="<p>I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.</p>">Darknet Market Arrests (2011â2015)</a></p></li>
<li><p><a href="./Modafinil" class="docMetadata" data-popup-title="Modafinil" data-popup-author="Gwern Branwen" data-popup-date="20 Feb 2009" data-popup-abstract="Modafinil is a prescription stimulant drug. I discuss informally, from a cost-benefit-informed perspective, the research up to 2015 on modafinil's cognitive effects, the risks of side-effects and addiction/tolerance and law enforcement, and give a table of current grey-market suppliers and discuss how to order from them.">Modafinil</a></p></li>
<li><p><a href="./LSD-microdosing" class="docMetadata" data-popup-title="LSD microdosing RCT" data-popup-author="Gwern Branwen" data-popup-date="20 Aug 2012" data-popup-abstract="<p>Some early experimental studies with LSD suggested that doses of LSD too small to cause any noticeable effects may improve mood and creativity. Prompted by recent discussion of this claim and the purely anecdotal subsequent evidence for it, I decided to run a well-powered randomized blind trial of 3-day LSD microdoses from September 2012 to March 2013. No beneficial effects reached statistical-significance and there were worrisome negative trends. LSD microdosing did not help me.</p>">LSD microdosing self-experiment</a></p></li>
<li><p><a href="./Zeo" class="docMetadata" data-popup-title="Zeo sleep self-experiments" data-popup-author="Gwern Branwen" data-popup-date="28 Dec 2010" data-popup-abstract="<p>I discuss my beliefs about Quantified Self, and demonstrate with a series of <a href=&quot;https://en.wikipedia.org/wiki/single-subject_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Single-subject design&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In design of experiments, &amp;lt;b&amp;gt;single-subject design&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;single-case research design&amp;lt;/b&amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: single-subject design&quot;>single-subject design</a> self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:</p><ol type=&quot;1&quot;><li>the Zeo headband is wearable long-term</li><li><a href=&quot;#melatonin&quot;>melatonin</a> improves my sleep</li><li><a href=&quot;#one-legged-standing&quot;>one-legged standing</a> does little</li><li>Vitamin D <a href=&quot;#vitamin-d&quot;>at night</a> damages my sleep &amp;amp; Vitamin D in morning does not affect my sleep</li><li>potassium (<a href=&quot;#potassium-day-use&quot;>over the day</a> but not so much <a href=&quot;#potassium-morning-use&quot;>the morning</a>) damages my sleep and does not improve my mood/productivity</li><li>small quantities of <a href=&quot;#alcohol&quot;>alcohol</a> appear to make little difference to my sleep quality</li><li>I may be better off <a href=&quot;#timing&quot;>changing my sleep timing</a> by waking up somewhat earlier &amp;amp; going to bed somewhat earlier</li><li><a href=&quot;#lithium&quot;>lithium orotate</a> does not affect my sleep</li><li><a href=&quot;#redshiftf.lux&quot;>Redshift</a> causes me to go to bed earlier</li><li><a href=&quot;#zma&quot;>ZMA</a>: inconclusive results slightly suggestive of benefits</li></ol>">Zeo sleep self-experiments</a></p></li>
<li><p><a href="./DNB-FAQ" class="docMetadata" data-popup-title="Dual N-Back FAQ" data-popup-author="Gwern Branwen" data-popup-date="25 Mar 2009" data-popup-abstract="A compendium of DNB, WM, IQ information up to 2015">Dual N-Back <span class="smallcaps-auto">FAQ</span></a></p></li>
<li><p><a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">Spaced repetition</a></p></li>
<li><p><a href="./Death-Note-Anonymity" class="docMetadata" data-popup-title="Death Note: L, Anonymity &amp; Eluding Entropy" data-popup-author="Gwern Branwen" data-popup-date="04 May 2011" data-popup-abstract="<p>In the manga <em>Death Note</em>, the protagonist Light Yagami is given the supernatural weapon âDeath Noteâ which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. <em>Death Note</em> is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of Lâs process from the perspective of computer security, cryptography, and information theory, to quantify Lightâs initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest.</p><ol type=&quot;1&quot;><li>Mistake 1: Lightâs fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)</li><li>Mistake 2: Worse, the deaths are non-random in other waysâthey tend to occur at particular times! Just the scheduling of deaths cost Light 6 bits of anonymity</li><li>Mistake 3: Lightâs third mistake was reacting to the blatant provocation of Lind L. Tailor. L narrowed his target down to 1/3 the original Japanese population, for a gain of ~1.6 bits.</li><li>Mistake 4: Lightâs fourth mistake was to use confidential police information stolen using his policeman fatherâs credentials. This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!</li><li>Mistake 5: If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.</li><li>Endgame: At this point in the plot, L resorts to direct measures and enters Lightâs life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.</li></ol><p>Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.</p><p><em>(Note: This essay assumes a familiarity with the early plot of <em><a href=&quot;https://en.wikipedia.org/wiki/Death_Note&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note&quot;>Death Note</a></em> and <a href=&quot;https://en.wikipedia.org/wiki/Light_Yagami&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Light Yagami&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Light Yagami&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a fictional character and the main protagonist of the manga series &amp;lt;i&amp;gt;Death Note&amp;lt;/i&amp;gt;, created by Tsugumi Ohba and Takeshi Obata. He is portrayed as an accomplished yet bored teen genius who finds the Death Note, a supernatural notebook that allows the user to kill anyone by knowing their name and face, after it is dropped by the Shinigami Ryuk. In an effort to create a utopia, Light eventually uses the notebook to murder criminals as the vigilante &amp;lt;b&amp;gt;Kira&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; (ã­ã©)&amp;lt;/span&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Light Yagami&quot;>Light Yagami</a>; if you are unfamiliar with it, see my <a href=&quot;./Death-Note-Ending&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&amp;#39;s Ending&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;29 Sep 2008&quot; data-popup-abstract=&quot;Ambiguous ending means even the victor is unclear; who was right?&quot;><em>Death Note</em> Ending</a> essay or consult <a href=&quot;https://en.wikipedia.org/wiki/Death_Note#Plot_summary&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note#Plot_summary&quot;>Wikipedia</a> or <a href=&quot;http://deathnote.wikia.com/wiki/Rules_of_the_Death_Note&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Rules of the Death Note&quot;>read the DN rules</a>.)</em></p>"><em>Death Note</em>: L, Anonymity &amp; Entropy</a></p></li>
<li><p><a href="./Complement" class="docMetadata" data-popup-title="Laws of Tech: Commoditize Your Complement" data-popup-author="Gwern Branwen" data-popup-date="17 March 2018" data-popup-abstract="<p>Joel Spolsky in 2002 identified a major pattern in technology business &amp;amp; economics: the pattern of <q>âcommoditizing your complementâ</q>, an alternative to vertical integration, where companies seek to secure a chokepoint or quasi-monopoly in products composed of many necessary &amp;amp; sufficient layers by dominating one layer while fostering so much competition in another layer above or below its layer that no competing monopolist can emerge, prices are driven down to marginal costs elsewhere in the stack, total price drops &amp;amp; increases demand, and the majority of the consumer surplus of the final product can be diverted to the quasi-monopolist. A classic example is the commodification of PC hardware by the Microsoft OS monopoly, to the detriment of IBM &amp;amp; benefit of MS.</p><p>This pattern explains many otherwise odd or apparently self-sabotaging ventures by large tech companies into apparently irrelevant fields, such as the high rate of releasing open-source contributions by many Internet companies or the intrusion of advertising companies into smartphone manufacturing &amp;amp; web browser development &amp;amp; statistical software &amp;amp; fiber-optic networks &amp;amp; municipal WiFi &amp;amp; radio spectrum auctions &amp;amp; DNS (Google): they are pre-emptive attempts to commodify another company elsewhere in the stack, or defenses against it being done to them.</p>">Commoditize Your Complement</a></p></li>
<li><p><a href="./Google-shutdowns" class="docMetadata" data-popup-title="Predicting Google closures" data-popup-author="Gwern Branwen" data-popup-date="28 Mar 2013" data-popup-abstract="<p>Prompted by the shutdown of Google Reader, I ponder the evanescence of online services and wonder what is the risk of them disappearing. I collect data on <a href=&quot;#sources&quot;>350 Google products</a> launched before March 2013, looking for <a href=&quot;#variables&quot;>variables predictive of mortality</a> (web hits, service vs software, commercial vs free, FLOSS, social networking, and internal vs acquired). Shutdowns are unevenly distributed over the calendar year or Googleâs history. I use logistic regression &amp;amp; survival analysis (which can deal with right-censorship) to <a href=&quot;#modeling&quot;>model the risk of shutdown over time</a> and examine correlates. The logistic regression indicates socialness, acquisitions, and lack of web hits predict being shut down, but the results may not be right. The survival analysis finds a median lifespan of 2824 days with a roughly Type III survival curve (high early-life mortality); a Cox regression finds similar results as the logistic - socialness, free, acquisition, and long life predict lower mortality. Using the best model, I <a href=&quot;#predictions&quot;>make predictions</a> about probability of shutdown of the most risky and least risky services in the next 5 years (up to March 2018). (All data &amp;amp; R source code is provided.)</p>">Google survival analysis</a></p></li>
<li><p><a href="./Ads" class="docMetadata" data-popup-title="Banner Ads Considered Harmful" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2017" data-popup-abstract="<p>One source of complexity &amp;amp; JavaScript use on <code>gwern.net</code> is the use of Google AdSense advertising to insert banner ads. In considering design &amp;amp; usability improvements, removing the banner ads comes up every time as a possibility, as readers do not like ads, but such removal comes at a revenue loss and itâs unclear whether the benefit outweighs the cost, suggesting I run an A/B experiment. However, ads might be expected to have broader effects on traffic than individual page reading times/bounce rates, affecting <em>total</em> site traffic instead through long-term effects on or spillover mechanisms between readers (eg social media behavior), rendering the usual A/B testing method of per-page-load/session randomization incorrect; instead it would be better to analyze total traffic as a time-series experiment.</p><p>Design: A decision analysis of revenue vs readers yields an maximum acceptable total traffic loss of ~3%. Power analysis of historical <code>gwern.net</code> traffic data demonstrates that the high autocorrelation yields low statistical power with standard tests &amp;amp; regressions but acceptable power with ARIMA models. I design a long-term Bayesian <code>ARIMA(4,0,1)</code> time-series model in which an A/B-test running JanuaryâOctober 2017 in randomized paired 2-day blocks of ads/no-ads uses client-local JS to determine whether to load &amp;amp; display ads, with total traffic data collected in Google Analytics &amp;amp; ad exposure data in Google AdSense. The A/B test ran from 1 January 2017 to 15 October 2017, affecting 288 days with collectively 380,140 pageviews in 251,164 sessions.</p><p>Correcting for a flaw in the randomization, the final results yield a surprisingly large estimate of an expected traffic loss of -9.7% (driven by the subset of users without adblock), with an implied -14% traffic loss if all traffic were exposed to ads (95% credible interval: -13â16%), exceeding my decision threshold for disabling ads &amp;amp; strongly ruling out the possibility of acceptably small losses which might justify further experimentation.</p><p>Thus, banner ads on <code>gwern.net</code> appear to be harmful and AdSense has been removed. If these results generalize to other blogs and personal websites, an important implication is that many websites may be harmed by their use of banner ad advertising without realizing it.</p>">Banner Ads Considered Harmful</a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.<br><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure>">This Waifu Does Not Exist</a> (<a href="./TWDNE" class="docMetadata" data-popup-title="This Waifu Does Not Exist" data-popup-author="Gwern Branwen" data-popup-date="19 Feb 2019" data-popup-abstract="<p>Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, <a href=&quot;https://www.thiswaifudoesnotexist.net/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;ThisWaifuDoesNotExist.net&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-02-19&quot; data-popup-abstract=&quot;&amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;&amp;lt;code&amp;gt;ThisWaifuDoesNotExist.net&amp;lt;/code&amp;gt;&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;https://www.gwern.net/TWDNE&amp;quot;&amp;gt;TWDNE&amp;lt;/a&amp;gt;) is a static website which uses JS to display random &amp;lt;a href=&amp;quot;https://www.gwern.net/Faces&amp;quot;&amp;gt;anime faces generated by StyleGAN&amp;lt;/a&amp;gt; neural networks, along with &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-2&amp;quot;&amp;gt;GPT-2&amp;lt;/a&amp;gt;-generated &amp;#39;anime plot summaries&amp;#39;.&amp;lt;br&amp;gt;&amp;lt;figure&amp;gt;&amp;lt;img src=&amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;quot; alt=&amp;quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;quot; /&amp;gt;&amp;lt;figcaption&amp;gt;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&quot;><q>âThis Waifu Does Not Existâ</q></a> for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or <a href=&quot;https://klimaleksus.github.io/FindTwin/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&quot;>online</a> <a href=&quot;https://github.com/darabos/high-five-trading&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Action stock exchange game for Repl.it Game Jam 2019&quot;>games</a>, uploaded to Pixiv, and used in a research paper (<a href=&quot;https://arxiv.org/abs/1904.01774&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot; data-popup-author=&quot;Atsuhiro Noguchi, Tatsuya Harada&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&quot; title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot;>Noguchi &amp;amp; Harada 2019</a>). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, <a href=&quot;https://waifulabs.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Waifu Labs</a>, which generates even better anime faces than my StyleGAN results.</p>">details</a>)</p></li>
<li><p><a href="./Tanks" class="docMetadata" data-popup-title="The Neural Net Tank Urban Legend" data-popup-author="Gwern Branwen" data-popup-date="20 Sep 2011" data-popup-abstract="<p>A cautionary tale in artificial intelligence tells about researchers training an neural network (NN) to detect tanks in photographs, succeeding, only to realize the photographs had been collected under specific conditions for tanks/non-tanks and the NN had learned something useless like time of day. This story is often told to warn about the limits of algorithms and importance of data collection to avoid <q>âdataset biasâ</q>/<q>âdata leakageâ</q> where the collected data can be solved using algorithms that do not generalize to the true data distribution, but the tank story is usually never sourced.</p><p>I collate many extent versions dating back a quarter of a century to 1992 along with two NN-related anecdotes from the 1960s; their contradictions &amp;amp; details indicate a classic âurban legendâ, with a probable origin in a speculative question in the 1960s by Edward Fredkin at an AI conference about some early NN research, which was subsequently classified &amp;amp; never followed up on.</p><p>I suggest that dataset bias is real but exaggerated by the tank story, giving a misleading indication of risks from deep learning and that it would be better to not repeat it but use real examples of dataset bias and focus on larger-scale risks like AI systems optimizing for wrong utility functions.</p>">The Neural Net/Tank Urban Legend</a></p></li>
<li><p><a href="./Self-decrypting-files" class="docMetadata" data-popup-title="Time-lock encryption" data-popup-author="Gwern Branwen" data-popup-date="24 May 2011" data-popup-abstract="<p>In cryptography, it is easy to adjust encryption of data so that one, some, or all people can decrypt it, or some combination thereof. It is not so easy to achieve adjustable decryptability over <em>time</em>, a âtime-lock cryptoâ: for some uses (data escrow, leaking, insurance, last-resort Bitcoin backups etc), one wants data which is distributed only after a certain point in time.</p><p>I survey techniques for time-lock crypto. Proposals often resort to trusted-third-parties, which are vulnerabilities. A better time-lock crypto proposal replaces trusted-third-parties with forcibly serial proof-of-work using number squaring and guaranteeing unlocking not after a certain point in time but after sufficient computation-time has been spent; itâs unclear how well number-squaring resists optimization or shortcuts. I suggest a new time-lock crypto based on chained hashes; hashes have been heavily attacked for other purposes, and may be safer than number-squaring. Finally, I cover obfuscation &amp;amp; witness-encryption which, combined with proof-of-work, can be said to solve time-lock crypto but currently remain infeasible.</p>">Time-lock cryptography</a></p></li>
<li><p><a href="./Archiving-URLs" class="docMetadata" data-popup-title="Archiving URLs" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2011" data-popup-abstract="<p>Links on the Internet last forever or a year, whichever comes first. This is a major problem for anyone serious about writing with good references, as link rot will cripple several percent of all links each year, and compounding.</p><p>To deal with link rot, I present my multi-pronged archival strategy using a combination of scripts, daemons, and Internet archival services: URLs are regularly dumped from both my web browserâs daily browsing and my website pages into an archival daemon I wrote, which pre-emptively downloads copies locally and attempts to archive them in the Internet Archive. This ensures a copy will be available indefinitely from one of several sources. Link rot is then detected by regular runs of <code>linkchecker</code>, and any newly dead links can be immediately checked for alternative locations, or restored from one of the archive sources.</p><p>As an additional flourish, my local archives are <a href=&quot;./Timestamping&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Easy Cryptographic Timestamping of Files&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;4 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to &amp;lt;em&amp;gt;timestamp&amp;lt;/em&amp;gt; it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp;amp; easily timestamp arbitrary files &amp;amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.&amp;lt;/p&amp;gt;&quot;>efficiently cryptographically timestamped using Bitcoin</a> in case forgery is a concern, and I demonstrate a simple compression trick for <a href=&quot;./Archiving-URLs#sort---key-compression-trick&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; compression trick (CLI folklore)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2014-03-03&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Programming folklore notes that one way to get better lossless compression efficiency is to rearrange files inside the archive to group âsimilarâ files together and expose redundancy to the compressor, in accordance with information-theoretical principles. A particularly easy and broadly-applicable way of doing this, which does not require using any unusual formats or tools and is fully compatible with the default archive methods, is to sort the files by &amp;lt;em&amp;gt;filename&amp;lt;/em&amp;gt; and especially file extension. I show how to do this with the standard Unix command-line &amp;lt;code&amp;gt;sort&amp;lt;/code&amp;gt; tool, using the so-called â&amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trickâ, and give examples of the large space-savings possible from my archiving work for personal website mirrors and for making &amp;lt;a href=&amp;quot;/DNM-archives&amp;quot;&amp;gt;darknet market mirror datasets&amp;lt;/a&amp;gt; where the redundancy at the file level is particularly extreme and the &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trick shines compared to the naive approach.&amp;lt;/p&amp;gt;&quot;>substantially reducing sizes of large web archives</a> such as crawls (particularly useful for repeated crawls such as my <a href=&quot;./DNM-archives&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market Archives (2013-2015)&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;1 Dec 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;usage&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;./DNM-survival&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Darknet Market mortality risks&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;amp;lt;a href=&amp;amp;quot;./Silk-Road&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;Silk Road 1: Theory &amp;amp;amp;amp; Practice&amp;amp;quot; data-popup-author=&amp;amp;quot;11 Jul 2011&amp;amp;quot; data-popup-date=&amp;amp;quot;gwern&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;div id=&amp;amp;amp;quot;abstract&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;blockquote&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;/blockquote&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;Silk Road 1&amp;amp;lt;/a&amp;amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;lifetimes/characteristics&amp;lt;/a&amp;gt;, &amp;amp;amp; &amp;lt;a href=&amp;quot;./DNM-arrests&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Tor DNM-related arrests, 2011-2015&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;legal riskiness&amp;lt;/a&amp;gt;; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp;amp; documents related to the DNMs as I could.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp;amp; 37+ related forums, representing &amp;amp;lt;4,438 mirrors, and is available for any research.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This page documents the download, contents, interpretation, and technical methods behind the scrapes.&amp;lt;/p&amp;gt;&quot;>DNM archives</a>).</p>">Archiving <span class="smallcaps-auto">URL</span>s</a></p></li>
<li><p><a href="./Terrorism-is-not-about-Terror" class="docMetadata" data-popup-title="Terrorism Is Not About Terror" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="<p>Statistical analysis of terrorist groupsâ longevity, aims, methods and successes reveal that groups are self-contradictory and self-sabotaging, generally ineffective; common stereotypes like terrorists being poor or ultra-skilled are false. Superficially appealing counter-examples are discussed and rejected. Data on motivations and the dissolution of terrorist groups are brought into play and the surprising conclusion reached: terrorism is a form of socialization or status-seeking.</p>">Terrorism is not about Terror</a></p></li>
<li><p><a href="./The-Melancholy-of-Subculture-Society" class="docMetadata" data-popup-title="The Melancholy of Subculture Society" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2009" data-popup-abstract="Internet links small groups, helping dissolve big groups; good, bad? But a bit sad.">The Melancholy of Subculture Society</a></p></li>
</ul>
</section>
<section>
<h1 id="notable"><a href="#notable" title="Link to section: 'Notable'">Notable</a></h1>
<ul>
<li><a href="./Causality" class="docMetadata" data-popup-title="Why Correlation Usually â  Causation: Causal Nets Cause Common Confounding" data-popup-author="Gwern Branwen" data-popup-date="24 Jun 2014" data-popup-abstract="<p>It is widely understood that statistical correlation between two variables â  causation. But despite this admonition, people are routinely overconfident in claiming correlations to support particular causal interpretations and are surprised by the results of randomized experiments, suggesting that they are biased &amp;amp; systematically underestimating the prevalence of confounds/common-causation. I speculate that in realistic causal networks or DAGs, the number of possible correlations grows faster than the number of possible causal relationships. So confounds really are that common, and since people do not think in DAGs, the imbalance also explains overconfidence.</p>">Why Correlation â  Causation</a></li>
<li><a href="./Embryo-selection" class="docMetadata" data-popup-title="Embryo selection for intelligence" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>With genetic predictors of a phenotypic trait, it is possible to select embryos during an in vitro fertilization process to increase or decrease that trait. Extending the work of <a href=&quot;./docs/iq/2014-shulman.pdf&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;417&quot; title=&quot;Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?&quot;>Shulman &amp;amp; Bostrom 2014</a>/<a href=&quot;https://arxiv.org/abs/1408.3421&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;On the genetic architecture of intelligence and other quantitative  traits&quot; data-popup-author=&quot;Stephen D. H. Hsu&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;How do genes affect cognitive ability or other human quantitative traits such as height or disease risk? Progress on this challenging question is likely to be significant in the near future. I begin with a brief review of psychometric measurements of intelligence, introducing the idea of a &amp;quot;general factor&amp;quot; or g score. The main results concern the stability, validity (predictive power), and heritability of adult g. The largest component of genetic variance for both height and intelligence is additive (linear), leading to important simplifications in predictive modeling and statistical estimation. Due mainly to the rapidly decreasing cost of genotyping, it is possible that within the coming decade researchers will identify loci which account for a significant fraction of total g variation. In the case of height analogous efforts are well under way. I describe some unpublished results concerning the genetic architecture of height and cognitive ability, which suggest that roughly 10k moderately rare causal variants of mostly negative effect are responsible for normal population variation. Using results from Compressed Sensing (L1-penalized regression), I estimate the statistical power required to characterize both linear and nonlinear models for quantitative traits. The main unknown parameter s (sparsity) is the number of loci which account for the bulk of the genetic variation. The required sample size is of order 100s, or roughly a million in the case of cognitive ability.&quot; title=&quot;On the genetic architecture of intelligence and other quantitative traits&quot;>Hsu 2014</a>, I consider the case of human intelligence using SNP-based genetic prediction, finding:</p><ul><li>a meta-analysis of <a href=&quot;https://en.wikipedia.org/wiki/GCTA&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Genome-wide complex trait analysis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Genome-wide complex trait analysis (GCTA) Genome-based restricted maximum likelihood (GREML)&amp;lt;/b&amp;gt; is a statistical method for variance component estimation in genetics which quantifies the total narrow-sense (additive) contribution to a trait&amp;#39;s heritability of a particular subset of genetic variants. This is done by directly quantifying the chance genetic similarity of unrelated individuals and comparing it to their measured similarity on a trait; if two unrelated individuals are relatively similar genetically and also have similar trait measurements, then the measured genetics are likely to causally influence that trait, and the correlation can to some degree tell how much. This can be illustrated by plotting the squared pairwise trait differences between individuals against their estimated degree of relatedness. The GCTA framework can be applied in a variety of settings. For example, it can be used to examine changes in heritability over aging and development.. It can also be extended to analyse bivariate genetic correlations between traits. There is an ongoing debate about whether GCTA generates reliable or stable estimates of heritability when used on current SNP data. The method is based on the outdated and false dichotomy of genes versus the environment. It also suffers from serious methodological weaknesses, such as susceptibility to population stratification.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: GCTA&quot;>GCTA</a> results indicates that SNPs can explain &amp;gt;33% of variance in current intelligence scores, and &amp;gt;44% with better-quality phenotype testing</li><li>this sets an upper bound on the effectiveness of selection: a gain of 9 IQ points when selecting the top embryo out of 10</li><li>the best 2016 polygenic score could achieve a gain of ~3 IQ points when selecting out of 10</li><li>the marginal cost of embryo selection (assuming IVF is already being done) is modest, at $1500 + $200 per embryo, with the sequencing cost projected to drop rapidly</li><li>a model of the IVF process, incorporating number of extracted eggs, losses to abnormalities &amp;amp; vitrification &amp;amp; failed implantation &amp;amp; miscarriages from 2 real IVF patient populations, estimates feasible gains of 0.39 &amp;amp; 0.68 IQ points</li><li>embryo selection is currently unprofitable (mean: -$358) in the USA under the lowest estimate of the value of an IQ point, but profitable under the highest (mean: $6230). The main constraints on selection profitability is the polygenic score; under the highest value, the NPV EVPI of a perfect SNP predictor is $24b and the EVSI per education/SNP sample is $71k</li><li>under the worst-case estimate, selection can be made profitable with a better polygenic score, which would require <em>n</em>&amp;gt;237,300 using education phenotype data (and much less using fluid intelligence measures)</li><li>selection can be made more effective by selecting on multiple phenotype traits: considering an example using 7 traits (IQ/height/BMI/diabetes/ADHD/bipolar/schizophrenia), there is a factor gain over IQ alone; the outperformance of multiple selection remains after adjusting for genetic correlations &amp;amp; polygenic scores and using a broader set of 16 traits. </li></ul>">Embryo selection for intelligence</a></li>
<li><a href="./Mail-delivery" class="docMetadata" data-popup-title="When Should I Check The Mail?" data-popup-author="Gwern Branwen" data-popup-date="21 June 2015" data-popup-abstract="<p>Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual âinterval-censored dataâ. I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (survival analysis with Gaussian regression using <code>survival</code> library), MCMC (Bayesian model in JAGS), and likelihood-free Bayesian inference (custom ABC, using the simulation). This allows estimation of the distribution of mail delivery times. I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking &amp;amp; personal observation, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen. Finally, I define a loss function on mail checks, enabling: a choice of optimal time to check the mailbox to minimize loss (exploitation); optimal time to check to maximize information gain (exploration); Thompson sampling (balancing exploration &amp;amp; exploitation indefinitely), and estimates of the value-of-information of another datapoint (to estimate when to stop exploration and start exploitation after a finite amount of data).</p>">When Should I Check The Mail?</a></li>
<li><a href="./Tool-AI" class="docMetadata" data-popup-title="Why Tool AIs Want to Be Agent AIs: The Power of Agency" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2016" data-popup-abstract="<p>Autonomous AI systems (Agent AIs) trained using <a href=&quot;https://en.wikipedia.org/wiki/reinforcement_learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reinforcement learning&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Reinforcement learning&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;RL&amp;lt;/b&amp;gt;) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reinforcement learning&quot;>reinforcement learning</a> can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved &amp;amp; executed by humans, giving equivalently superintelligent results without the risk.</p><p>I argue that this is not an effective solution for two major reasons. First, because Agent AIs will by definition be better at <em>actions</em> than Tool AIs, giving an economic advantage. Secondly, because Agent AIs will be better at <em>inference &amp;amp; learning</em> than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs will be inferior to Agent AIs in both actions and intelligence, implying use of Tool AIs is a even more highly unstable equilibrium than previously argued, as users of Agent AIs will be able to outcompete them on two dimensions (and not just one).</p>">Why Tool AIs Want to Be Agent AIs</a></li>
<li><a href="./Complexity-vs-AI" class="docMetadata" data-popup-title="Complexity no Bar to AI" data-popup-author="Gwern Branwen" data-popup-date="1 June 2014" data-popup-abstract="<p>Computational complexity theory describes the steep increase in computing power required for many algorithms to solve larger problems; frequently, the increase is large enough to render problems a few times larger totally intractable. Many of these algorithms are used in AI-relevant contexts. It has been argued that this implies that AIs will fundamentally be limited in accomplishing real-world tasks better than humans because they will run into the same computational complexity limit as humans, and so the consequences of developing AI will be small, as it is impossible for there to be any large fast global changes due to human or superhuman-level AIs. I examine the assumptions of this argument and find it neglects the many conditions under which computational complexity theorems are valid and so the argument doesnât work: problems can be solved more efficiently than complexity classes would imply, large differences in problem solubility between humans and AIs is possible, greater resource consumption is possible, the real-world consequences of small differences on individual tasks can be large on agent impacts, such consequences can compound, and many agents can be created; any of these independent objections being true destroys the argument.</p>">Complexity no Bar to AI</a></li>
<li><a href="./Faces" class="docMetadata" data-popup-title="Making Anime Faces With StyleGAN" data-popup-author="Gwern Branwen" data-popup-date="4 Feb 2019" data-popup-abstract="<p>Generative neural networks, such as GANs, have <a href=&quot;#why-dont-gans-work&quot;>struggled for years</a> to generate decent-quality anime faces, despite their great success with photographic imagery such as real human faces. The task has now been effectively solved, for anime faces as well as many other domains, by the development of a new generative adversarial network, <a href=&quot;https://arxiv.org/abs/1812.04948&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Style-Based Generator Architecture for Generative Adversarial Networks&quot; data-popup-author=&quot;Tero Karras, Samuli Laine, Timo Aila&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&quot; title=&quot;&amp;#39;A Style-Based Generator Architecture for Generative Adversarial Networks&amp;#39;, Karras et al 2018&quot;><em>StyleGAN</em></a>, whose <a href=&quot;https://github.com/NVlabs/stylegan&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>source code</a> was released in February 2019.</p><p>I <a href=&quot;#examples&quot;>show off</a> my StyleGAN anime faces &amp;amp; videos, provide downloads, provide the âmissing manualâ &amp;amp; explain how I trained them based on <a href=&quot;./Danbooru2019&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;15 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The best known booru, with a focus on quality, is &amp;lt;a href=&amp;quot;https://danbooru.donmai.us/&amp;quot;&amp;gt;Danbooru&amp;lt;/a&amp;gt;. We create &amp;amp;amp; provide a torrent which contains ~2.5tb of 3.33m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.&amp;lt;/p&amp;gt;&quot;>Danbooru2017/2018</a> with source code for the <a href=&quot;#data-preparation&quot;>data preprocessing</a>, document <a href=&quot;#installation&quot;>installation</a> &amp;amp; <a href=&quot;#configuration&quot;>configuration</a> &amp;amp; <a href=&quot;#running&quot;>training tricks</a>.</p><p>For application, I document various scripts for generating <a href=&quot;#sampling&quot;>images &amp;amp; videos</a>, briefly <a href=&quot;#twdne&quot;>describe the website</a> <a href=&quot;https://www.thiswaifudoesnotexist.net&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><q>âThis Waifu Does Not Existâ</q></a> <a href=&quot;./TWDNE&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;This Waifu Does Not Exist&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;19 Feb 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, &amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;âThis Waifu Does Not Existâ&amp;lt;/a&amp;gt; for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or &amp;lt;a href=&amp;quot;https://klimaleksus.github.io/FindTwin/&amp;quot; title=&amp;quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&amp;quot;&amp;gt;online&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://github.com/darabos/high-five-trading&amp;quot; title=&amp;quot;Action stock exchange game for Repl.it Game Jam 2019&amp;quot;&amp;gt;games&amp;lt;/a&amp;gt;, uploaded to Pixiv, and used in a research paper (&amp;lt;a href=&amp;quot;https://arxiv.org/abs/1904.01774&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot; data-popup-author=&amp;quot;Atsuhiro Noguchi, Tatsuya Harada&amp;quot; data-popup-date=&amp;quot;2019-08-21&amp;quot; data-popup-abstract=&amp;quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&amp;quot; title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot;&amp;gt;Noguchi &amp;amp;amp; Harada 2019&amp;lt;/a&amp;gt;). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, &amp;lt;a href=&amp;quot;https://waifulabs.com/&amp;quot;&amp;gt;Waifu Labs&amp;lt;/a&amp;gt;, which generates even better anime faces than my StyleGAN results.&amp;lt;/p&amp;gt;&quot;>I set up</a> as a public demo, discuss how the trained models can be <a href=&quot;#transfer-learning&quot;>used for transfer learning</a> such as generating high-quality faces of anime characters with small datasets (eg <a href=&quot;#holo&quot;>Holo</a> or <a href=&quot;#asuka&quot;>Asuka Souryuu Langley</a>), and touch on <a href=&quot;#reversing-stylegan-to-control-modify-images&quot;>more advanced StyleGAN applications</a> like encoders &amp;amp; controllable generation.</p><p>The <a href=&quot;#appendix&quot;>appendix</a> gives samples of my failures with earlier GANs for anime face generation, and I provide samples &amp;amp; model from a relatively large-scale <a href=&quot;#biggan&quot;>BigGAN</a> training run suggesting that BigGAN may be the next step forward to generating full-scale anime images.</p><p>A minute of reading could save an hour of debugging!</p>">Making Anime Faces w/Neural Nets</a></li>
<li><a href="./Death-Note-script" class="docMetadata" data-popup-title="Who wrote the 'Death Note' script?" data-popup-author="Gwern Branwen" data-popup-date="2 Nov 2009" data-popup-abstract="<p>I give a history of the 2009 leaked script, discuss internal &amp;amp; external evidence for its realness including stylometrics; and then give a simple step-by-step Bayesian analysis of each point. We finish with high confidence in the script being real, discussion of how this analysis was surprisingly enlightening, and what followup work the analysis suggests would be most valuable.</p>"><em>Death Note</em> script authenticity</a></li>
<li><a href="./The-Existential-Risk-of-Mathematical-Error" class="docMetadata" data-popup-title="The Existential Risk of Math Errors" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Mathematical mistake/error-rates limit our understanding of rare risks and ability to defend against them">The Existential Risk of Math Errors</a></li>
<li><a href="./Iodine" class="docMetadata" data-popup-title="Iodine and Adult IQ meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="29 Feb 2012" data-popup-abstract="<p>Iodization is one of the great success stories of public health intervention: iodizing salt costs pennies per ton, but as demonstrated in randomized &amp;amp; natural experiments, prevents goiters, cretinism, and can boost population IQs by a fraction of a standard deviation in the most iodine-deficient populations.</p><p>These experiments are typically done on pregnant women, and results suggest that the benefits of iodization diminish throughout the trimesters of a pregnancy. So does iodization benefit normal healthy <em>adults</em>, potentially even ones in relatively iodine-sufficient Western countries?</p><p>Compiling existing post-natal iodization studies which use cognitive tests, I find thatâoutliers asideâthe benefit appears to be nearly zero, and so likely it does not help normal healthy adults, particularly in Western adults.</p>">Iodine &amp; Adult IQ meta-analysis</a></li>
<li><a href="./The-Melancholy-of-Kyon" class="docMetadata" data-popup-title="The Melancholy of Kyon" data-popup-author="Gwern Branwen" data-popup-date="08 Jun 2009" data-popup-abstract="<p>The light novel series <em>The Melancholy of Haruhi Suzumiya</em>, featuring a character named Haruhi who is a god unawares and her search for novelty, has a number of anomalies and unclear overarching plot. I argue that these anomalies can be resolved, and greater literary depth achieved, by interpreting the first-person protagonist Kyon as the actual unaware god.</p>">The Melancholy of Kyon</a></li>
<li><a href="./In-Defense-Of-Inclusionism" class="docMetadata" data-popup-title="In Defense of Inclusionism" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="<p>English Wikipedia is in decline. As a long-time editor &amp;amp; former admin, I was deeply dismayed by the process. Here, I discuss UI principles, changes in Wikipedian culture, the large-scale statistical evidence of decline, run small-scale experiments demonstrating the harm, and conclude with parting thoughts.</p>">In Defense of Wikipedia Inclusionism</a></li>
<li><a href="./Bitcoin-is-Worse-is-Better" class="docMetadata" data-popup-title="Bitcoin Is Worse Is Better" data-popup-author="Gwern Branwen" data-popup-date="27 May 2011" data-popup-abstract="2011 essay on how Bitcoin's long gestation and early opposition indicates it is an example of the 'Worse is Better' paradigm in which an ugly complex design with few attractive theoretical properties compared to purer competitors nevertheless successfully takes over a niche, survives, and becomes gradually refined.">Bitcoin is Worse is Better</a></li>
<li><a href="./Sunk-cost" class="docMetadata" data-popup-title="Are Sunk Costs Fallacies?" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2012" data-popup-abstract="Human and animal sunk costs often aren't, and sunk cost bias may be useful on an individual level to encourage learning. Convincing examples of sunk cost bias typically operate on organizational levels and are probably driven by non-psychological causes like competition.">Are Sunk Costs Fallacies?</a></li>
</ul>
</section>
<p>By topic:</p>
<section id="statistics" class="level1">
<h1><a href="#statistics" title="Link to section: 'Statistics'">Statistics</a></h1>
<ul>
<li><a href="./Causality" class="docMetadata" data-popup-title="Why Correlation Usually â  Causation: Causal Nets Cause Common Confounding" data-popup-author="Gwern Branwen" data-popup-date="24 Jun 2014" data-popup-abstract="<p>It is widely understood that statistical correlation between two variables â  causation. But despite this admonition, people are routinely overconfident in claiming correlations to support particular causal interpretations and are surprised by the results of randomized experiments, suggesting that they are biased &amp;amp; systematically underestimating the prevalence of confounds/common-causation. I speculate that in realistic causal networks or DAGs, the number of possible correlations grows faster than the number of possible causal relationships. So confounds really are that common, and since people do not think in DAGs, the imbalance also explains overconfidence.</p>">Why Correlation Usually â  Causation</a></li>
<li><a href="./Mail-delivery" class="docMetadata" data-popup-title="When Should I Check The Mail?" data-popup-author="Gwern Branwen" data-popup-date="21 June 2015" data-popup-abstract="<p>Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual âinterval-censored dataâ. I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (survival analysis with Gaussian regression using <code>survival</code> library), MCMC (Bayesian model in JAGS), and likelihood-free Bayesian inference (custom ABC, using the simulation). This allows estimation of the distribution of mail delivery times. I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking &amp;amp; personal observation, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen. Finally, I define a loss function on mail checks, enabling: a choice of optimal time to check the mailbox to minimize loss (exploitation); optimal time to check to maximize information gain (exploration); Thompson sampling (balancing exploration &amp;amp; exploitation indefinitely), and estimates of the value-of-information of another datapoint (to estimate when to stop exploration and start exploitation after a finite amount of data).</p>">When Does The Mail Come? Decision Analysis</a></li>
<li><a href="./Google-shutdowns" class="docMetadata" data-popup-title="Predicting Google closures" data-popup-author="Gwern Branwen" data-popup-date="28 Mar 2013" data-popup-abstract="<p>Prompted by the shutdown of Google Reader, I ponder the evanescence of online services and wonder what is the risk of them disappearing. I collect data on <a href=&quot;#sources&quot;>350 Google products</a> launched before March 2013, looking for <a href=&quot;#variables&quot;>variables predictive of mortality</a> (web hits, service vs software, commercial vs free, FLOSS, social networking, and internal vs acquired). Shutdowns are unevenly distributed over the calendar year or Googleâs history. I use logistic regression &amp;amp; survival analysis (which can deal with right-censorship) to <a href=&quot;#modeling&quot;>model the risk of shutdown over time</a> and examine correlates. The logistic regression indicates socialness, acquisitions, and lack of web hits predict being shut down, but the results may not be right. The survival analysis finds a median lifespan of 2824 days with a roughly Type III survival curve (high early-life mortality); a Cox regression finds similar results as the logistic - socialness, free, acquisition, and long life predict lower mortality. Using the best model, I <a href=&quot;#predictions&quot;>make predictions</a> about probability of shutdown of the most risky and least risky services in the next 5 years (up to March 2018). (All data &amp;amp; R source code is provided.)</p>">Predicting Google service closures</a></li>
<li><a href="./Prediction-markets" class="docMetadata" data-popup-title="Prediction Markets" data-popup-author="Gwern Branwen" data-popup-date="10 Jan 2009" data-popup-abstract="My prediction/betting strategies and track record, reflections on rationality, prediction judgments">Predicting &amp; Prediction Markets</a></li>
<li><a href="./Ads" class="docMetadata" data-popup-title="Banner Ads Considered Harmful" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2017" data-popup-abstract="<p>One source of complexity &amp;amp; JavaScript use on <code>gwern.net</code> is the use of Google AdSense advertising to insert banner ads. In considering design &amp;amp; usability improvements, removing the banner ads comes up every time as a possibility, as readers do not like ads, but such removal comes at a revenue loss and itâs unclear whether the benefit outweighs the cost, suggesting I run an A/B experiment. However, ads might be expected to have broader effects on traffic than individual page reading times/bounce rates, affecting <em>total</em> site traffic instead through long-term effects on or spillover mechanisms between readers (eg social media behavior), rendering the usual A/B testing method of per-page-load/session randomization incorrect; instead it would be better to analyze total traffic as a time-series experiment.</p><p>Design: A decision analysis of revenue vs readers yields an maximum acceptable total traffic loss of ~3%. Power analysis of historical <code>gwern.net</code> traffic data demonstrates that the high autocorrelation yields low statistical power with standard tests &amp;amp; regressions but acceptable power with ARIMA models. I design a long-term Bayesian <code>ARIMA(4,0,1)</code> time-series model in which an A/B-test running JanuaryâOctober 2017 in randomized paired 2-day blocks of ads/no-ads uses client-local JS to determine whether to load &amp;amp; display ads, with total traffic data collected in Google Analytics &amp;amp; ad exposure data in Google AdSense. The A/B test ran from 1 January 2017 to 15 October 2017, affecting 288 days with collectively 380,140 pageviews in 251,164 sessions.</p><p>Correcting for a flaw in the randomization, the final results yield a surprisingly large estimate of an expected traffic loss of -9.7% (driven by the subset of users without adblock), with an implied -14% traffic loss if all traffic were exposed to ads (95% credible interval: -13â16%), exceeding my decision threshold for disabling ads &amp;amp; strongly ruling out the possibility of acceptably small losses which might justify further experimentation.</p><p>Thus, banner ads on <code>gwern.net</code> appear to be harmful and AdSense has been removed. If these results generalize to other blogs and personal websites, an important implication is that many websites may be harmed by their use of banner ad advertising without realizing it.</p>">Banner Ads Considered Harmful (Here)</a></li>
<li><a href="./Research-criticism" class="docMetadata" data-popup-title="How Should We Critique Research?" data-popup-author="Gwern Branwen" data-popup-date="19 May 2019" data-popup-abstract="<p>Scientific and statistical research must be read with a critical eye to understand how credible the claims are. The Reproducibility Crisis and the growth of meta-science have demonstrated that much research is of low quality and often false. But there are so many possible things any given study could be criticized for, falling short of an unobtainable ideal, that it becomes unclear which possible criticism is important and they may degenerate into mere rhetoric. How do we separate fatal flaws from unfortunate caveats from specious quibbling?</p><p>I think that what makes a criticism important is how much it could change a result if corrected and how much that would then change our decisions or actions: to what extent it is a <q>âdifference which makes a differenceâ</q>. This is why issues of research fraud, causal inference, or biases yielding overestimates are universally important, because a âcausalâ effect turning out to be zero effect or overestimated by a factor will change almost all decisions based on such research; while on the other hand, other issues like measurement error or distributional assumptions, which are equally common, are often <em>not</em> important as they typically yield much smaller changes in conclusions, and hence decisions.</p><p>If we regularly ask whether a criticism would make this kind of difference, it will be clearer which ones are important criticisms, and which ones risk being rhetorical distractions and obstructing meaningful evaluation of research.</p>">How To Critique Research?</a></li>
<li><a href="./2012-election-predictions" class="docMetadata" data-popup-title="2012 election predictions" data-popup-author="Gwern Branwen" data-popup-date="5 Nov 2012" data-popup-abstract="<p>Statistically analyzing in <code>R</code> hundreds of predictions compiled for ~10 forecasters of the 2012 American Presidential election, and ranking them by Brier, RMSE, &amp;amp; log scores; the best overall performance seems to be by Drew Linzer and Wang &amp;amp; Holbrook, while Nate Silver appears as somewhat over-rated and the famous Intrade prediction market turning in a disappointing overall performance.</p>">Judging the 2012 election forecasters</a></li>
<li><a href="./Candy-Japan" class="docMetadata" data-popup-title="Candy Japan's new box A/B test" data-popup-author="Gwern Branwen" data-popup-date="6 May 2016" data-popup-abstract="<p>I analyze an A/B test from a mail-order company of two different kinds of box packaging from a Bayesian decision-theory perspective, balancing posterior probability of improvements &amp;amp; greater profit against the cost of packaging &amp;amp; risk of worse results, finding that as the companyâs analysis suggested, the new box is unlikely to be sufficiently better than the old. Calculating expected values of information shows that it is not worth experimenting on further, and that such fixed-sample trials are unlikely to ever be cost-effective for packaging improvements. However, adaptive experiments may be worthwhile.</p>">Candy Japanâs new-box A/B test</a></li>
<li><a href="./Resorter" class="docMetadata" data-popup-title="Resorting Media Ratings" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2015" data-popup-abstract="<p>User-created datasets using ordinal scales (such as media ratings) have tendencies to drift or âclumpâ towards the extremes and fail to be informative as possible, falling prey to ceiling effects and making it difficult to distinguish between the mediocre and truly excellent. This can be counteracted by rerating the dataset to create a uniform (and hence, informative) distribution of ratings, but such manual rerating is difficult. I provide an anytime CLI program, <code>resorter</code>, which keeps track of comparisons, infers underlying ratings assuming that they are noisy in the ELO-like Bradley-Terry model, and interactively &amp;amp; intelligently queries the user with comparisons of the media with the most uncertain current ratings, until the user ends the session and a fully rescaled set of ratings are output.</p>">Interactively Resorting Lists</a></li>
<li><a href="./Google-Alerts" class="docMetadata" data-popup-title="Alerts Over Time" data-popup-author="Gwern Branwen" data-popup-date="1 July 2013" data-popup-abstract="<p>Has Google Alerts been sending fewer results the past few years? Yes. Responding to rumors of its demise, I investigate the number of results in my personal Google Alerts notifications 2007-2013, and find no overall trend of decline until I look at a transition in <a href=&quot;#it-was-mid-2011&quot;>mid-2011</a> where the results fall dramatically. I speculate about <a href=&quot;#panda&quot;>the cause</a> and <a href=&quot;#conclusion&quot;>implications</a> for Alertsâs future.</p>">Google Alerts Over Time</a></li>
<li><a href="./Order-statistics" class="docMetadata" data-popup-title="Calculating in R The Expected Maximum of a Gaussian Sample using Order Statistics" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>In generating a sample of <em>n</em> datapoints drawn from a normal/Gaussian distribution with a particular mean/SD, how big on average the biggest datapoint is will depend on how large <em>n</em> is. Knowing this average is useful in a number of areas like sports or breeding or manufacturing, as it defines how bad/good the worst/best datapoint will be (eg the score of the winner in a multi-player game).</p><p>The <a href=&quot;https://en.wikipedia.org/wiki/order_statistic&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Order statistic&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, the &amp;lt;i&amp;gt;k&amp;lt;/i&amp;gt;th &amp;lt;b&amp;gt;order statistic&amp;lt;/b&amp;gt; of a statistical sample is equal to its &amp;lt;i&amp;gt;k&amp;lt;/i&amp;gt;th-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: order statistic&quot;>order statistic</a> of the mean/average/expectation of the maximum of a draw of <em>n</em> samples from a normal distribution has no exact formula, unfortunately, and is generally not built into any programming languageâs libraries.</p><p>I implement &amp;amp; compare some of the approaches to estimating this order statistic in the R programming language, for both the maximum and the general order statistic. The overall best approach is to calculate the exact order statistics for the <em>n</em> range of interest using numerical integration via <code>lmomco</code> and cache them in a lookup table, rescaling the mean/SD as necessary for arbitrary normal distributions; next best is a polynomial regression approximation; finally, the Elfving correction to the Blom 1958 approximation is fast, easily implemented, and accurate for reasonably large <em>n</em> such as <em>n</em>&amp;gt;100.</p>">Expected Maximum of Gaussians</a></li>
<li><a href="./Everything" class="docMetadata" data-popup-title="Everything Is Correlated" data-popup-author="Gwern Branwen" data-popup-date="12 Sep 2014" data-popup-abstract="<p>Statistical folklore asserts that <q>âeverything is correlatedâ</q>: in any real-world dataset, most or all measured variables will have non-zero correlations, even between variables which appear to be completely independent of each other, and that these correlations are not merely sampling error flukes but will appear in large-scale datasets to arbitrarily designated levels of statistical-significance or posterior probability.</p><p>This raises serious questions for null-hypothesis statistical-significance testing, as it implies the null hypothesis of 0 will always be rejected with sufficient data, meaning that a failure to reject only implies insufficient data, and provides no actual test or confirmation of a theory. Even a directional prediction is minimally confirmative since there is a 50% chance of picking the right direction at random.</p><p>It also has implications for conceptualizations of theories &amp;amp; causal models, interpretations of structural models, and other statistical principles such as the <q>âsparsity principleâ</q>.</p>">Everything Is Correlated</a></li>
<li><a href="./docs/statistics/order/beanmachine-multistage/index.html" class="docMetadata" data-popup-title="Multi-Stage Bean Machine Visualization: Advantages of Repeated Optimization" data-popup-author="Rafe Kennedy, Gwern Branwen" data-popup-date="2018-12-17" data-popup-abstract="An interactive JavaScript of order statistics visualized as a Galton bean machine, showing difference in means &amp; maxima between single stage of selection and multiple stages.<br />This is an interactive JS-based visualization of the difference in optimization potentials of a single-stage pipeline vs a multi-stage pipeline, in which new samples/measurements can be generated at each step (such as in evolutionary processes).</p><p>Because it optimizes over multiple steps, the multi-stage pipeline âratchets upwardâ and can attain far more extreme maxima than a single-stage pipeline, even with the same total number of samples - the single-stage process quickly hits âdiminishing returnsâ, where large increases in sample count result in only small increases in the expected maximum. This means that small gains per stage, or a few stages, or a few generations of evolution, can result in large increases of sample means, compared to a single-stage process. Due to <a href=&quot;https://en.wikipedia.org/wiki/Order_statistic&quot;>order statistics</a>, the increases in means can cause larger increases in the probability of samples passing thresholds such as âtop 1%â/â¥2.32Ï, or yielding extremes. And the more stages, the greater differences can be (single-stage selection increases logarithmically, while multi-stage increases linearly).</p><p>These increases can be counterintuitively large, but the gains/losses are relevant to understanding many processes, such as the clinical drug discovery pipeline (eg <a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147215&quot; title=&quot;When Quality Beats Quantity: Decision Theory, Drug Discovery, and the Reproducibility Crisis&quot;>Scannell &amp;amp; Bosley 2016</a>).</p><p>The visualization metaphor here is <a href=&quot;https://en.wikipedia.org/wiki/Bean_machine&quot;>Francis Galtonâs <em>quincunx</em> or âbean machineâ</a>: a ball (sample) falls from the top (zero-mean), and is affected by sets of pins (stochastic variables) which bounce the ball left/right with 50-50 probability (increase/decrease it) as it falls to the bottom (final total). The resulting binomial distribution approximates a normal distribution. The bean machine visually &amp;amp; concretely illustrates the sampling distribution of how a normally-distributed final variable can emerge out of the sum of many individual small discrete effects, without requiring any mathematics.</p><p>In this visualization, we generalize Galtonâs âbean machineâ by allowing stacking of bean machines. To stack bean machines, we select the ball which is the <em>maximum</em> within each sample. How large is it? In the single-stage bean machine, selection stops there. In the multi-stage bean machine, <em>another</em> bean machine begins with the maximum serving as the seed &amp;amp; new average, and another round of generation &amp;amp; selection begins, and so on, until a final sample is selected, and we can see how large it is. The gains turn out to be larger the more samples we use total, unsurprisingly, but also the more stages we specify; the maximum possible maximum turns out to be when we have so many stages that there are just 2 samples per stage.</p><p><img style=&quot;border: 1px solid #666;&quot; alt=&quot;Screenshot of the multi-stage bean machine, showing selection in progress in a 3x3 pipeline&quot; width=&quot;300&quot; src=&quot;/docs/statistics/order/beanmachine-multistage/beanmachine-demo.pngbeanmachine-demo.png&quot; title=&quot;Multi-Stage Bean Machine&quot;></p>">Multi-Stage Selection JS Demo</a></li>
<li><a href="./Milk" class="docMetadata" data-popup-title="The Power of Twins: Revisiting Student's Scottish Milk Experiment Example" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2016" data-popup-abstract="<p>Randomized experiments require more subjects the more variable each datapoint is to overcome the noise which obscures any effects of the intervention. Reducing noise enables better inferences with the same data, or less data to be collected, which can be done by balancing observed characteristics between control and experimental datapoints.</p><p>A particularly dramatic example of this approach is running experiments on identical twins rather than regular people, because twins vary far less from each other than random people due to shared genetics &amp;amp; family environment. In 1931, the great statistician Student (William Sealy Gosset) noted problems with an extremely large (<em>n</em>=20,000) Scottish experiment in feeding children milk (to see if they grew more in height or weight), and claimed that the experiment could have been done far more cost-effectively with an extraordinary reduction of &amp;gt;95% fewer children if it had been conducted using twins, and claimed that 100 identical twins would have been <em>more</em> accurate than 20,000 children. He, however, did not provide any calculations or data demonstrating this.</p><p>I revisit the issue and run a power calculation on height indicating that Studentâs claims were correct and that the experiment would have required ~97% fewer children if run with twins.</p><p>This reduction is not unique to the Scottish milk experiment on height/weight, and in general, one can expect a reduction of 89% in experiment sample sizes using twins rather than regular people, demonstrating the benefits of using behavioral genetics in <a href=&quot;https://en.wikipedia.org/wiki/experiment_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Design of experiments&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;design of experiments&amp;lt;/b&amp;gt; is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: experiment design&quot;>experiment design</a>/<a href=&quot;https://en.wikipedia.org/wiki/Power_%28statistics%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Power (statistics)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;power&amp;lt;/b&amp;gt; of a binary hypothesis test is the probability that the test rejects the null hypothesis (H&amp;lt;sub&amp;gt;0&amp;lt;/sub&amp;gt;) when a specific alternative hypothesis (H&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;) is true. The statistical power ranges from 0 to 1, and as statistical power increases, the probability of making a type II error (wrongly failing to reject the null hypothesis) decreases. For a type II error probability of Î², the corresponding statistical power is 1 â Î². For example, if experiment 1 has a statistical power of 0.7, and experiment 2 has a statistical power of 0.95, then there is a stronger probability that experiment 1 had a type II error than experiment 2, and experiment 2 is more reliable than experiment 1 due to the reduction in probability of a type II error. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;) when it is trueâthat is, the ability of a test to detect a specific effect, if that specific effect actually exists. That is,&amp;lt;/p&amp;gt;&amp;lt;dl&amp;gt;&amp;lt;dd&amp;gt;&amp;lt;span class=&amp;quot;mwe-math-element&amp;quot;&amp;gt;&amp;lt;img src=&amp;quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/0f98c579718d2bfaafcbaea6c921fd9ba19ce268&amp;quot; class=&amp;quot;mwe-math-fallback-image-inline&amp;quot; aria-hidden=&amp;quot;true&amp;quot; style=&amp;quot;vertical-align:-1.005ex;width:35.813ex;height:3.176ex&amp;quot; /&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/dd&amp;gt;&amp;lt;/dl&amp;gt;&quot; title=&quot;Wikipedia: Power (statistics)&quot;>power analysis</a>.</p>">Power of Twins: the Milk Experiment</a></li>
<li><a href="./Hunter" class="docMetadata" data-popup-title="'Genius Revisited': On the Value of High IQ Elementary Schools" data-popup-author="Gwern Branwen" data-popup-date="19 June 2016" data-popup-abstract="<p><em>Genius Revisited</em> documents the longitudinal results of a high-IQ/gifted-and-talented elementary school, Hunter College Elementary School (HCES); one of the most striking results is the general high education &amp;amp; income levels, but absence of great accomplishment on a national or global scale (eg a Nobel prize). The authors suggest that this may reflect harmful educational practices at their elementary school or the low predictive value of IQ.</p><p>I suggest that there is no puzzle to this absence nor anything for HCES to be blamed for, as the absence is fully explainable by their making two statistical errors: <a href=&quot;https://en.wikipedia.org/wiki/base-rate_neglect&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Base rate fallacy&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;base rate fallacy&amp;lt;/b&amp;gt;, also called &amp;lt;b&amp;gt;base rate neglect&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;base rate bias&amp;lt;/b&amp;gt;, is a fallacy. If presented with related base rate information and specific information, the mind tends to ignore the former and focus on the latter.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: base-rate neglect&quot;>base-rate neglect</a>, and <a href=&quot;https://en.wikipedia.org/wiki/regression_to_the_mean&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Regression toward the mean&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, &amp;lt;b&amp;gt;regression toward&amp;lt;/b&amp;gt; &amp;lt;b&amp;gt;the mean&amp;lt;/b&amp;gt; is the phenomenon that arises if a random variable is extreme on its first measurement but closer to the mean or average on its second measurement and if it is extreme on its second measurement but closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called &amp;lt;b&amp;gt;reversion to the mean&amp;lt;/b&amp;gt; and &amp;lt;b&amp;gt;reversion to mediocrity&amp;lt;/b&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: regression to the mean&quot;>regression to the mean</a>.</p><p>First, their standards fall prey to a base-rate fallacy and even extreme predictive value of IQ would not predict 1 or more Nobel prizes because Nobel prize odds are measured at 1 in millions, and with a small total sample size of a few hundred, it is highly likely that there would simply be no Nobels.</p><p>Secondly, and more seriously, the lack of accomplishment is inherent and unavoidable as it is driven by the <a href=&quot;https://en.wikipedia.org/wiki/regression_to_the_mean&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Regression toward the mean&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, &amp;lt;b&amp;gt;regression toward&amp;lt;/b&amp;gt; &amp;lt;b&amp;gt;the mean&amp;lt;/b&amp;gt; is the phenomenon that arises if a random variable is extreme on its first measurement but closer to the mean or average on its second measurement and if it is extreme on its second measurement but closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called &amp;lt;b&amp;gt;reversion to the mean&amp;lt;/b&amp;gt; and &amp;lt;b&amp;gt;reversion to mediocrity&amp;lt;/b&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: regression to the mean&quot;>regression to the mean</a> caused by the relatively low correlation of early childhood with adult IQsâwhich means their sample is far less elite as adults than they believe. Using early-childhood/adult IQ correlations, regression to the mean implies that HCES students will fall from a mean of 157 IQ in kindergarten (when selected) to somewhere around 133 as adults (and possibly lower). Further demonstrating the role of regression to the mean, in contrast, HCESâs associated high-IQ/gifted-and-talented high school, Hunter High, which has access to the adolescentsâ more predictive IQ scores, has much higher achievement in proportion to its lesser regression to the mean (despite dilution by Hunter elementary students being grandfathered in).</p><p>This unavoidable statistical fact undermines the main rationale of HCES: extremely high-IQ adults cannot be accurately selected as kindergartners on the basis of a simple test. This greater-regression problem can be lessened by the use of additional variables in admissions, such as parental IQs or high-quality genetic polygenic scores; unfortunately, these are either politically unacceptable or dependent on future scientific advances. This suggests that such elementary schools may not be a good use of resources and HCES students should not be assigned scarce magnet high school slots.</p>"><em>Genius Revisited</em>: High-IQ School Flaws</a></li>
<li><a href="./hpmor#analysis" class="docMetadata" data-popup-title="'HP: Methods of Rationality' review statistics" data-popup-author="Gwern Branwen" data-popup-date="3 Nov 2012" data-popup-abstract="<p>The unprecedented gap in <a href=&quot;http://www.hpmor.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><em>Methods of Rationality</em></a> updates prompts musing about whether readership is increasing enough &amp;amp; what statistics one would use; I write code to download FF.net reviews, clean it, parse it, load into R, summarize the data &amp;amp; depict it graphically, run linear regression on a subset &amp;amp; all reviews, note the poor fit, develop a quadratic fit instead, and use it to predict future review quantities.</p><p>Then, I run a similar analysis on a competing fanfiction to find out when they will have equal total review-counts. A try at logarithmic fits fails; fitting a linear model to the previous 100 days of <em>MoR</em> and the competitor works much better, and they predict a convergence in &amp;lt;5 years.</p><p>A survival analysis finds no major anomalies in reviewer lifetimes, but an apparent increase in mortality for reviewers who started reviewing with later chapters, consistent with (but far from proving) the original theory that the later chaptersâ delays are having negative effects.</p>">Modeling fiction review rates</a></li>
<li><a href="./GoodReads" class="docMetadata" data-popup-title="The Most Abandoned Books on GoodReads" data-popup-abstract="<p>What books are hardest for a reader who starts them to finish, and most likely to be abandoned? I scrape a crowdsourced tag, <code>abandoned</code>, from the GoodReads book social network to estimate conditional probability of being abandoned.</p> <p>The default GoodReads tag interface presents only raw counts of tags, not counts divided by total ratings (=reads). This conflates popularity with probability of being abandoned: a popular but rarely-abandoned book may have more <code>abandoned</code> tags than a less popular but often-abandoned book. There is also residual error from the winnerâs curse where books with fewer ratings are more mis-estimated than popular books.</p> <p>Correcting for both changes the top-5 ranking completely, from (<a href=&quot;#data&quot;>raw counts</a>):</p> <ol type=&quot;1&quot;> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Casual_Vacancy&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Casual Vacancy&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Casual Vacancy</b></i> is a 2012 novel written by J. K. Rowling. The book was published worldwide by the Little, Brown Book Group on 27 September 2012. A paperback edition was released on 23 July 2013. It was Rowling's first publication since the <i>Harry Potter</i> series, her first apart from that series, and her first novel for adult readership.</p>&quot; title=&quot;Wikipedia: The Casual Vacancy&quot;>The Casual Vacancy</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/J.K._Rowling&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;J. K. Rowling&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Joanne Rowling</b>, better known by her pen name <b>J. K. Rowling</b>, is a British author, film producer, television producer, screenwriter, and philanthropist. She is best known for writing the <i>Harry Potter</i> fantasy series, which has won multiple awards and sold more than 500 million copies, becoming the best-selling book series in history. The books are the basis of a popular film series, over which Rowling had overall approval on the scripts and was a producer on the final films. She also writes crime fiction under the name <b>Robert Galbraith</b>.</p>&quot; title=&quot;Wikipedia: J.K. Rowling&quot;>J.K. Rowling</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Catch-22&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catch-22&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Catch-22</b></i> is a satirical war novel by American author Joseph Heller. He began writing it in 1953; the novel was first published in 1961. Often cited as one of the most significant novels of the twentieth century, it uses a distinctive non-chronological third-person omniscient narration, describing events from the points of view of different characters. The separate storylines are out of sequence so the timeline develops along with the plot.</p>&quot; title=&quot;Wikipedia: Catch-22&quot;>Catch-22</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Joseph_Heller&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Joseph Heller&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Joseph Heller</b> was an American author of novels, short stories, plays, and screenplays. His best-known work is the novel <i>Catch-22</i>, a satire on war and bureaucracy, whose title has become a synonym for an absurd or contradictory choice.</p>&quot; title=&quot;Wikipedia: Joseph Heller&quot;>Joseph Heller</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/American_Gods&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;American Gods&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>American Gods</b></i> (2001) is a fantasy novel by British author Neil Gaiman. The novel is a blend of Americana, fantasy, and various strands of ancient and modern mythology, all centering on the mysterious and taciturn Shadow.</p>&quot; title=&quot;Wikipedia: American Gods&quot;>American Gods</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Neil_Gaiman&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Neil Gaiman&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Neil Richard MacKinnon Gaiman</b> is an English author of short fiction, novels, comic books, graphic novels, nonfiction, audio theatre, and films. His works include the comic book series <i>The Sandman</i> and novels <i>Stardust</i>, <i>American Gods</i>, <i>Coraline</i>, and <i>The Graveyard Book</i>. He has won numerous awards, including the Hugo, Nebula, and Bram Stoker awards, as well as the Newbery and Carnegie medals. He is the first author to win both the Newbery and the Carnegie medals for the same work, <i>The Graveyard Book</i> (2008). In 2013, <i>The Ocean at the End of the Lane</i> was voted Book of the Year in the British National Book Awards.</p>&quot; title=&quot;Wikipedia: Neil Gaiman&quot;>Neil Gaiman</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/A_Game_of_Thrones&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Game of Thrones&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>A Game of Thrones</b></i> is the first novel in <i>A Song of Ice and Fire</i>, a series of fantasy novels by the American author George R. R. Martin. It was first published on August 1, 1996. The novel won the 1997 Locus Award and was nominated for both the 1997 Nebula Award and the 1997 World Fantasy Award. The novella <i>Blood of the Dragon</i>, comprising the Daenerys Targaryen chapters from the novel, won the 1997 Hugo Award for Best Novella. In January 2011, the novel became a <span><i>New York Times</i> Bestseller</span> and reached #1 on the list in July 2011.</p>&quot; title=&quot;Wikipedia: A Game of Thrones&quot;>A Game of Thrones</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/George_R.R._Martin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;George R. R. Martin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>George Raymond Richard Martin</b>, also known as <b>GRRM</b>, is an American novelist and short story writer in the fantasy, horror, and science fiction genres, screenwriter, and television producer. He is best known for his series of epic fantasy novels, <i>A Song of Ice and Fire</i>, which was adapted into the HBO series <i>Game of Thrones</i> (2011â2019).</p>&quot; title=&quot;Wikipedia: George R.R. Martin&quot;>George R.R. Martin</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Book_Thief&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Book Thief&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Book Thief</b></i> is a historical novel by Australian author Markus Zusak and is his most popular work.</p>&quot; title=&quot;Wikipedia: The Book Thief&quot;>The Book Thief</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Markus_Zusak&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Markus Zusak&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Markus Zusak</b> is an Australian writer of German origin. He is best known for <i>The Book Thief</i> and <i>The Messenger</i>, two novels for young adults which became international bestsellers. He won the Margaret A. Edwards Award in 2014 for his contributions to young-adult literature published in the United States.</p>&quot; title=&quot;Wikipedia: Markus Zusak&quot;>Markus Zusak</a></li> </ol> <p>to (<a href=&quot;#bayesian-modeling&quot;>shrunken posterior proportions</a>):</p> <ol type=&quot;1&quot;> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Black_Leopard%2C_Red_Wolf&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Black Leopard, Red Wolf&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Black Leopard, Red Wolf</b></i> is a 2019 fantasy novel by writer Marlon James. It is the first book of a planned trilogy. The novel draws on African history and mythology, blended into the landscape of the North Kingdom and the South Kingdom, and the political tensions between these two warring states, as well as various city-states and tribes in the surrounding landscape. The rights to produce a film adaptation were purchased by Michael B. Jordan in February 2019 prior to release of the book.</p>&quot; title=&quot;Wikipedia: Black Leopard, Red Wolf&quot;>Black Leopard, Red Wolf</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Marlon_James&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Marlon James&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Marlon James</b> may refer to:</p><ul><li>Marlon James (novelist), Jamaican writer, winner of the 2015 Man Booker Prize</li> <li>Marlon James (footballer), Vincentian retired footballer</li></ul> &quot; title=&quot;Wikipedia: Marlon James&quot;>Marlon James</a></li> <li><a href=&quot;https://en.wikipedia.org/wiki/Space_Opera_%28Valente_novel%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Space Opera (Valente novel)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Space Opera</b></i> is a 2018 science fiction novel by Catherynne Valente, about a galactic version of the Eurovision Song Contest. It was first published by Saga Press.</p>&quot; title=&quot;Wikipedia: Space Opera (Valente novel)&quot;><em>Space Opera</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Catherynne_M._Valente&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catherynne M. Valente&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Catherynne M. Valente</b> is an American fiction writer, poet, and literary critic. For her speculative fiction novels she has won the annual James Tiptree, Andre Norton, and Mythopoeic Fantasy Awards. Her short fiction has appeared in <i>Clarkesworld Magazine</i>, the World Fantasy Awardâwinning anthologies <i>Salon Fantastique</i> and <i>Paper Cities</i>, along with numerous <i>Year's Best</i> volumes. Her critical work has appeared in the <i>International Journal of the Humanities</i> as well as in numerous essay collections.</p>&quot; title=&quot;Wikipedia: Catherynne M. Valente&quot;>Catherynne M. Valente</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Little%2C_Big&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Little, Big&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Little, Big: or, The Fairies' Parliament</b></i> is a modern fantasy novel by John Crowley, published in 1981. It won the World Fantasy Award in 1982.</p>&quot; title=&quot;Wikipedia: Little, Big&quot;>Little, Big</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/John_Crowley&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;John Crowley&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>John Crowley</b> may refer to:</p><ul><li>John Crowley (1891-1942), Irish revolutionary and hunger striker</li> <li>John Crowley (author), American author</li> <li>John Crowley (baseball) (1862â1896), American Major League catcher</li> <li>John Crowley, American biotechnology executive</li> <li>John Crowley (bishop), former bishop of Middlesbrough</li> <li>John Crowley (director), Irish theatre and film director</li> <li>John Crowley (politician) (1870â1934), Irish Sinn FÃ©in politician</li> <li>John Crowley (1659â1728), British politician</li> <li>John Powers Crowley (1936â1989), U.S. federal judge</li> <li>Johnny Crowley, Irish hurler</li> <li>Johnny Crowley, Gaelic footballer with Kerry GAA</li> <li>John Crowley, founder of Meadow Hall Ironworks, now the site of Meadowhall shopping centre</li></ul> &quot; title=&quot;Wikipedia: John Crowley&quot;>John Crowley</a></li> <li><a href=&quot;https://www.amazon.com/Witches-Suspicion-Betrayal-Hysteria-Salem/dp/031620059X?tag=gwernnet-20&quot;><em>The Witches: Salem, 1692</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Stacy_Schiff&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Stacy Schiff&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Stacy Madeleine Schiff</b> is an American nonfiction author residing in New York.</p>&quot; title=&quot;Wikipedia: Stacy Schiff&quot;>Stacy Schiff</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Tender_Morsels&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tender Morsels&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Tender Morsels</b></i> (2008) is a novel by Australian author Margo Lanagan. It won the Ditmar Award in 2009 for Best Novel and was joint winner of the 2009 World Fantasy Award for Best Novel.</p>&quot; title=&quot;Wikipedia: Tender Morsels&quot;>Tender Morsels</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Margo_Lanagan&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Margo Lanagan&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Margo Lanagan</b> is an Australian writer of short stories and young adult fiction.</p>&quot; title=&quot;Wikipedia: Margo Lanagan&quot;>Margo Lanagan</a></li> </ol> <p>I also consider <a href=&quot;#modeling-with-covariates&quot;>a model adjusting for covariates</a> (author/average-rating/year), to see what books are most surprisingly often-abandoned given their pedigrees &amp; rating etc. Abandon rates increase the newer a book is, and the lower the average rating.</p> <p>Adjusting for those, the top-5 are:</p> <ol type=&quot;1&quot;> <li><em>The Casual Vacancy</em>, J.K. Rowling</li> <li><a href=&quot;https://www.amazon.com/Chemist-Stephenie-Meyer/dp/0316387843?tag=gwernnet-20&quot;><em>The Chemist</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Stephenie_Meyer&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Stephenie Meyer&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Stephenie Meyer</b> is an American novelist and film producer, best known for her vampire romance series <i>Twilight</i>. The <i>Twilight</i> novels have sold over 100 million copies, with translations into 37 different languages. Meyer was the bestselling author of 2008 and 2009 in the US, having sold over 29 million books in 2008, and 26.5 million in 2009. <i>Twilight</i> was the best-selling book of 2008 in the US.</p>&quot; title=&quot;Wikipedia: Stephenie Meyer&quot;>Stephenie Meyer</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Infinite_Jest&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Infinite Jest&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Infinite Jest</b></i> is a 1996 novel by American writer David Foster Wallace. The novel is widely noted for its unconventional narrative structure and its experimental use of endnotes. It has been categorized as an encyclopedic novel and made <i>TIME</i> magazine's list of the 100 best English-language novels published between 1923 and 2005.</p>&quot; title=&quot;Wikipedia: Infinite Jest&quot;>Infinite Jest</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/David_Foster_Wallace&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;David Foster Wallace&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>David Foster Wallace</b> was an American writer and university professor in the disciplines of English and creative writing. His novel <i>Infinite Jest</i> (1996) was listed by <i>Time</i> magazine as one of the 100 best English-language novels published between 1923 and 2005. His last novel, <i>The Pale King</i> (2011), was a finalist for the Pulitzer Prize for Fiction in 2012.</p>&quot; title=&quot;Wikipedia: David Foster Wallace&quot;>David Foster Wallace</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Glass_Bead_Game&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Glass Bead Game&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Glass Bead Game</b></i> is the last full-length novel of the German author Hermann Hesse. It was begun in 1931 and published in Switzerland in 1943 after being rejected for publication in Germany due to Hesse's anti-Fascist views. A few years later, in 1946, Hesse won the Nobel Prize in Literature. In honoring him in its Award Ceremony Speech, the Swedish Academy said that the novel &quot;occupies a special position&quot; in Hesse's work.</p>&quot; title=&quot;Wikipedia: The Glass Bead Game&quot;>The Glass Bead Game</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Hermann_Hesse&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Hermann Hesse&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Hermann Karl Hesse</b> was a German-born poet, novelist, and painter. His best-known works include <i>Demian</i>, <i>Steppenwolf</i>, <i>Siddhartha</i>, and <i>The Glass Bead Game</i>, each of which explores an individual's search for authenticity, self-knowledge and spirituality. In 1946, he received the Nobel Prize in Literature.</p>&quot; title=&quot;Wikipedia: Hermann Hesse&quot;>Hermann Hesse</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Theft_by_Finding%3A_Diaries_%281977%E2%80%932002%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Theft by Finding: Diaries (1977â2002)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Theft by Finding: Diaries (1977â2002)</b></i> is an edited compilation of diary entries by David Sedaris published on May 30, 2017. Sedaris shares selected entries spanning from his days as a 20-year-old hitchhiking through Oregon to living in London just shy of his 46th birthday. It has been released in advance of <i>David Sedaris Diaries: A Visual Compendium</i>, which is scheduled to debut on October 10, 2017.</p>&quot; title=&quot;Wikipedia: Theft by Finding: Diaries (1977â2002)&quot;>Theft by Finding: Diaries (1977â2002)</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/David_Sedaris&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;David Sedaris&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>David Raymond Sedaris</b> is an American humorist, comedian, author, and radio contributor. He was publicly recognized in 1992 when National Public Radio broadcast his essay &quot;Santaland Diaries&quot;. He published his first collection of essays and short stories, <i>Barrel Fever,</i> in 1994. He is the brother and writing collaborator of actor Amy Sedaris.</p>&quot; title=&quot;Wikipedia: David Sedaris&quot;>David Sedaris</a></li> </ol> <p>Books at the top of the adjusted list appear to reflect a mix of highly-popular authors changing genres, and âprestigeâ books which are highly-rated but a slog to read.</p> <p>These results are interesting for how they highlight how people read books for many reasons (such as marketing campaigns, literary prestige, or following a popular author), and this is reflected in their decision whether to continue reading or to abandon a book.</p>" data-popup-date="2019-12-09" data-popup-author="Gwern Branwen">Modeling fiction drop rates</a></li>
<li><a href="./EA-donations" class="docMetadata" data-popup-title="LWer Effective Altruism donations, 2013-2014" data-popup-author="Gwern Branwen" data-popup-date="12 May 2015" data-popup-abstract="<p>A LW critic noted that the annual LW survey reported a median donation for âeffective altruistsâ of $0, though the EA movement encourages strongly donations. I look closer at the 2013-2014 LW surveys and find in multiple regression that identifying as an EA does predict more donations after controlling for age and income, suggesting that the low EA median donation may be due to EAers having low income and youth (eg being a student) rather than being unusually or even averagely selfish.</p>">LW Effective Altruism donations, 2013â2014</a></li>
<li><a href="./Statistical-notes" class="docMetadata" data-popup-title="Statistical Notes" data-popup-author="Gwern Branwen" data-popup-date="17 July 2014" data-popup-abstract="Miscellaneous statistical stuff">Miscellaneous</a></li>
</ul>
</section>
<section id="cryptobitcoin" class="level1">
<h1><a href="#cryptobitcoin" title="Link to section: 'Crypto/Bitcoin'">Crypto/Bitcoin</a></h1>
<ul>
<li><a href="./Self-decrypting-files" class="docMetadata" data-popup-title="Time-lock encryption" data-popup-author="Gwern Branwen" data-popup-date="24 May 2011" data-popup-abstract="<p>In cryptography, it is easy to adjust encryption of data so that one, some, or all people can decrypt it, or some combination thereof. It is not so easy to achieve adjustable decryptability over <em>time</em>, a âtime-lock cryptoâ: for some uses (data escrow, leaking, insurance, last-resort Bitcoin backups etc), one wants data which is distributed only after a certain point in time.</p><p>I survey techniques for time-lock crypto. Proposals often resort to trusted-third-parties, which are vulnerabilities. A better time-lock crypto proposal replaces trusted-third-parties with forcibly serial proof-of-work using number squaring and guaranteeing unlocking not after a certain point in time but after sufficient computation-time has been spent; itâs unclear how well number-squaring resists optimization or shortcuts. I suggest a new time-lock crypto based on chained hashes; hashes have been heavily attacked for other purposes, and may be safer than number-squaring. Finally, I cover obfuscation &amp;amp; witness-encryption which, combined with proof-of-work, can be said to solve time-lock crypto but currently remain infeasible.</p>">Time-lock encryption</a></li>
<li><a href="./Bitcoin-is-Worse-is-Better" class="docMetadata" data-popup-title="Bitcoin Is Worse Is Better" data-popup-author="Gwern Branwen" data-popup-date="27 May 2011" data-popup-abstract="2011 essay on how Bitcoin's long gestation and early opposition indicates it is an example of the 'Worse is Better' paradigm in which an ugly complex design with few attractive theoretical properties compared to purer competitors nevertheless successfully takes over a niche, survives, and becomes gradually refined.">Bitcoin is Worse is Better</a></li>
<li><a href="./Silk-Road" class="docMetadata" data-popup-title="Silk Road 1: Theory &amp; Practice" data-popup-author="Gwern Branwen" data-popup-date="11 Jul 2011" data-popup-abstract="<p>The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.</p>">A Silk Road 1 Guide</a></li>
<li><a href="./DNM-survival" class="docMetadata" data-popup-title="Darknet Market mortality risks" data-popup-author="Gwern Branwen" data-popup-date="30 Oct 2013" data-popup-abstract="<p>I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>Silk Road 1</a>, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.</p>"><span class="smallcaps-auto">DNM</span> census/lifetimes</a></li>
<li><a href="./DNM-archives" class="docMetadata" data-popup-title="Darknet Market Archives (2013-2015)" data-popup-author="Gwern Branwen" data-popup-date="1 Dec 2013" data-popup-abstract="<p>Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.</p><p>From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>usage</a>, <a href=&quot;./DNM-survival&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market mortality risks&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;30 Oct 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;Silk Road 1&amp;lt;/a&amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;lt;/p&amp;gt;&quot;>lifetimes/characteristics</a>, &amp;amp; <a href=&quot;./DNM-arrests&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor DNM-related arrests, 2011-2015&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;14 Jul 2012&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;lt;/p&amp;gt;&quot;>legal riskiness</a>; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp; documents related to the DNMs as I could.</p><p>This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp; 37+ related forums, representing &amp;lt;4,438 mirrors, and is available for any research.</p><p>This page documents the download, contents, interpretation, and technical methods behind the scrapes.</p>"><span class="smallcaps-auto">DNM</span> archives (2013â2015)</a></li>
<li><a href="./DNM-arrests" class="docMetadata" data-popup-title="Tor DNM-related arrests, 2011-2015" data-popup-author="Gwern Branwen" data-popup-date="14 Jul 2012" data-popup-abstract="<p>I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.</p>"><span class="smallcaps-auto">DNM</span> arrests (2011â2015)</a></li>
<li><a href="./Timestamping" class="docMetadata" data-popup-title="Easy Cryptographic Timestamping of Files" data-popup-author="Gwern Branwen" data-popup-date="4 Dec 2015" data-popup-abstract="<p>Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to <em>timestamp</em> it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp; easily timestamp arbitrary files &amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.</p>">Timestamping w/Bitcoin</a></li>
</ul>
</section>
<section id="AI" class="level1">
<h1><a href="#ai" title="Link to section: 'AI'">AI</a></h1>
<ul>
<li><p><a href="./Complexity-vs-AI" class="docMetadata" data-popup-title="Complexity no Bar to AI" data-popup-author="Gwern Branwen" data-popup-date="1 June 2014" data-popup-abstract="<p>Computational complexity theory describes the steep increase in computing power required for many algorithms to solve larger problems; frequently, the increase is large enough to render problems a few times larger totally intractable. Many of these algorithms are used in AI-relevant contexts. It has been argued that this implies that AIs will fundamentally be limited in accomplishing real-world tasks better than humans because they will run into the same computational complexity limit as humans, and so the consequences of developing AI will be small, as it is impossible for there to be any large fast global changes due to human or superhuman-level AIs. I examine the assumptions of this argument and find it neglects the many conditions under which computational complexity theorems are valid and so the argument doesnât work: problems can be solved more efficiently than complexity classes would imply, large differences in problem solubility between humans and AIs is possible, greater resource consumption is possible, the real-world consequences of small differences on individual tasks can be large on agent impacts, such consequences can compound, and many agents can be created; any of these independent objections being true destroys the argument.</p>">Complexity no Bar to AI</a></p></li>
<li><p><a href="./Tool-AI" class="docMetadata" data-popup-title="Why Tool AIs Want to Be Agent AIs: The Power of Agency" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2016" data-popup-abstract="<p>Autonomous AI systems (Agent AIs) trained using <a href=&quot;https://en.wikipedia.org/wiki/reinforcement_learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reinforcement learning&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Reinforcement learning&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;RL&amp;lt;/b&amp;gt;) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reinforcement learning&quot;>reinforcement learning</a> can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved &amp;amp; executed by humans, giving equivalently superintelligent results without the risk.</p><p>I argue that this is not an effective solution for two major reasons. First, because Agent AIs will by definition be better at <em>actions</em> than Tool AIs, giving an economic advantage. Secondly, because Agent AIs will be better at <em>inference &amp;amp; learning</em> than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs will be inferior to Agent AIs in both actions and intelligence, implying use of Tool AIs is a even more highly unstable equilibrium than previously argued, as users of Agent AIs will be able to outcompete them on two dimensions (and not just one).</p>">Tool AIs Want To Be Agent AIs</a></p></li>
<li><p><a href="./Danbooru2019" class="docMetadata" data-popup-title="Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2015" data-popup-abstract="<p>Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.</p><p>The best known booru, with a focus on quality, is <a href=&quot;https://danbooru.donmai.us/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Danbooru</a>. We create &amp;amp; provide a torrent which contains ~3tb of 3.69m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.</p><p>Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.</p>">Danbooru2019 Anime Image Dataset</a></p></li>
<li><a href="./Faces" class="docMetadata" data-popup-title="Making Anime Faces With StyleGAN" data-popup-author="Gwern Branwen" data-popup-date="4 Feb 2019" data-popup-abstract="<p>Generative neural networks, such as GANs, have <a href=&quot;#why-dont-gans-work&quot;>struggled for years</a> to generate decent-quality anime faces, despite their great success with photographic imagery such as real human faces. The task has now been effectively solved, for anime faces as well as many other domains, by the development of a new generative adversarial network, <a href=&quot;https://arxiv.org/abs/1812.04948&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Style-Based Generator Architecture for Generative Adversarial Networks&quot; data-popup-author=&quot;Tero Karras, Samuli Laine, Timo Aila&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&quot; title=&quot;&amp;#39;A Style-Based Generator Architecture for Generative Adversarial Networks&amp;#39;, Karras et al 2018&quot;><em>StyleGAN</em></a>, whose <a href=&quot;https://github.com/NVlabs/stylegan&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>source code</a> was released in February 2019.</p><p>I <a href=&quot;#examples&quot;>show off</a> my StyleGAN anime faces &amp;amp; videos, provide downloads, provide the âmissing manualâ &amp;amp; explain how I trained them based on <a href=&quot;./Danbooru2019&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;15 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The best known booru, with a focus on quality, is &amp;lt;a href=&amp;quot;https://danbooru.donmai.us/&amp;quot;&amp;gt;Danbooru&amp;lt;/a&amp;gt;. We create &amp;amp;amp; provide a torrent which contains ~3tb of 3.69m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.&amp;lt;/p&amp;gt;&quot;>Danbooru2017/2018/2019</a> with source code for the <a href=&quot;#data-preparation&quot;>data preprocessing</a>, document <a href=&quot;#installation&quot;>installation</a> &amp;amp; <a href=&quot;#configuration&quot;>configuration</a> &amp;amp; <a href=&quot;#running&quot;>training tricks</a>.</p><p>For application, I document various scripts for generating <a href=&quot;#sampling&quot;>images &amp;amp; videos</a>, briefly <a href=&quot;#twdne&quot;>describe the website</a> <a href=&quot;https://www.thiswaifudoesnotexist.net&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><q>âThis Waifu Does Not Existâ</q></a> <a href=&quot;./TWDNE&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;This Waifu Does Not Exist&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;19 Feb 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, &amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;âThis Waifu Does Not Existâ&amp;lt;/a&amp;gt; for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or &amp;lt;a href=&amp;quot;https://klimaleksus.github.io/FindTwin/&amp;quot; title=&amp;quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&amp;quot;&amp;gt;online&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://github.com/darabos/high-five-trading&amp;quot; title=&amp;quot;Action stock exchange game for Repl.it Game Jam 2019&amp;quot;&amp;gt;games&amp;lt;/a&amp;gt;, uploaded to Pixiv, and used in a research paper (&amp;lt;a href=&amp;quot;https://arxiv.org/abs/1904.01774&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot; data-popup-author=&amp;quot;Atsuhiro Noguchi, Tatsuya Harada&amp;quot; data-popup-date=&amp;quot;2019-08-21&amp;quot; data-popup-abstract=&amp;quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&amp;quot; title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot;&amp;gt;Noguchi &amp;amp;amp; Harada 2019&amp;lt;/a&amp;gt;). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, &amp;lt;a href=&amp;quot;https://waifulabs.com/&amp;quot;&amp;gt;Waifu Labs&amp;lt;/a&amp;gt;, which generates even better anime faces than my StyleGAN results.&amp;lt;/p&amp;gt;&quot;>I set up</a> as a public demo, discuss how the trained models can be <a href=&quot;#transfer-learning&quot;>used for transfer learning</a> such as generating high-quality faces of anime characters with small datasets (eg <a href=&quot;#holo&quot;>Holo</a> or <a href=&quot;#asuka&quot;>Asuka Souryuu Langley</a>), and touch on <a href=&quot;#reversing-stylegan-to-control-modify-images&quot;>more advanced StyleGAN applications</a> like encoders &amp;amp; controllable generation.</p><p>The <a href=&quot;#appendix&quot;>appendix</a> gives samples of my failures with earlier GANs for anime face generation, and I provide samples &amp;amp; model from a relatively large-scale <a href=&quot;#biggan&quot;>BigGAN</a> training run suggesting that BigGAN may be the next step forward to generating full-scale anime images.</p><p>A minute of reading could save an hour of debugging!</p>">Making Anime Faces with Neural Nets</a></li>
<li><p><a href="./Coin-flip" class="docMetadata" data-popup-title="The Kelly Coin-Flipping Game: Exact Solutions via Decision Trees" data-popup-author="Gwern Branwen, Arthur B., nshepperd, FeepingCreature, Gurkenglas" data-popup-date="19 Jan 2017" data-popup-abstract="<p>Haghani &amp;amp; Dewey 2016 experiment with a double-or-nothing coin-flipping game where the player starts with $25 and has an edge of 60%, and can play 300 times, choosing how much to bet each time, winning up to a maximum ceiling of $250. Most of their subjects fail to play well, earning an average $91, compared to Haghani &amp;amp; Dewey 2016âs heuristic benchmark of ~$240 in winnings achievable using a modified Kelly Criterion as their strategy. The KC, however, is not optimal for this problem as it ignores the ceiling and limited number of plays.</p><p>We solve the problem of the value of optimal play exactly by using decision trees &amp;amp; dynamic programming for calculating the value function, with implementations in R, Haskell, and C. We also provide a closed-form exact value formula in R &amp;amp; Python, several approximations using Monte Carlo/random forests/neural networks, visualizations of the value function, and a Python implementation of the game for the OpenAI Gym collection. We find that optimal play yields $246.61 on average (rather than ~$240), and so the human players actually earned only 36.8% of what was possible, losing $155.6 in potential profit. Comparing decision trees and the Kelly criterion for various horizons (bets left), the relative advantage of the decision tree strategy depends on the horizon: it is highest when the player can make few bets (at <em>b</em>=23, with a difference of ~$36), and decreases with number of bets as more strategies hit the ceiling.</p><p>In the Kelly game, the maximum winnings, number of rounds, and edge are fixed; we describe a more difficult generalized version in which the 3 parameters are drawn from Pareto, normal, and beta distributions and are unknown to the player (who can use Bayesian inference to try to estimate them during play). Upper and lower bounds are estimated on the value of this game. In the variant of this game where subjects are not told the exact edge of 60%, a Bayesian decision tree approach shows that performance can closely approach that of the decision tree, with a penalty for 1 plausible prior of only $1. Two deep reinforcement learning agents, DQN &amp;amp; DDPG, are implemented but DQN fails to learn and DDPG doesnât show acceptable performance, indicating better deep RL methods may be required to solve the generalized Kelly game.</p>">Kelly Coin-Flip Game: Solutions</a></p></li>
<li><p><a href="./Backstop" class="docMetadata" data-popup-title="Evolution as Backstop for Reinforcement Learning" data-popup-author="Gwern Branwen" data-popup-date="6 Dec 2018" data-popup-abstract="<p>One defense of free markets notes the inability of non-market mechanisms to solve planning &amp;amp; optimization problems. This has difficulty with Coaseâs paradox of the firm, and I note that the difficulty is increased by the fact that with improvements in computers, algorithms, and data, ever larger planning problems <em>are</em> solved. Expanding on some Cosma Shalizi comments, I suggest a multi-level optimization paradigm: many systems can be seen as having two (or more) levels where a slow sample-inefficient but ground-truth âouterâ loss such as death, bankruptcy, or reproductive fitness, trains &amp;amp; constrains a fast sample-efficient but possibly misguided âinnerâ loss which is used by learned mechanisms such as neural networks or linear programming group selection perspective. So, one reason for free-market or evolutionary or Bayesian methods in general is that while poorer at planning/optimization in the short run, they have the advantage of simplicity and operating on ground-truth values, and serve as a constraint on the more sophisticated non-market mechanisms. I illustrate by discussing corporations, multicellular life, reinforcement learning &amp;amp; meta-learning in AI, and pain in humans. This view suggests that are inherent balances between market/non-market mechanisms which reflect the relative advantages between a slow unbiased method and faster but potentially arbitrarily biased methods.</p>">Evolution as Backstop for Learning</a></p></li>
<li><p><a href="./Tanks" class="docMetadata" data-popup-title="The Neural Net Tank Urban Legend" data-popup-author="Gwern Branwen" data-popup-date="20 Sep 2011" data-popup-abstract="<p>A cautionary tale in artificial intelligence tells about researchers training an neural network (NN) to detect tanks in photographs, succeeding, only to realize the photographs had been collected under specific conditions for tanks/non-tanks and the NN had learned something useless like time of day. This story is often told to warn about the limits of algorithms and importance of data collection to avoid <q>âdataset biasâ</q>/<q>âdata leakageâ</q> where the collected data can be solved using algorithms that do not generalize to the true data distribution, but the tank story is usually never sourced.</p><p>I collate many extent versions dating back a quarter of a century to 1992 along with two NN-related anecdotes from the 1960s; their contradictions &amp;amp; details indicate a classic âurban legendâ, with a probable origin in a speculative question in the 1960s by Edward Fredkin at an AI conference about some early NN research, which was subsequently classified &amp;amp; never followed up on.</p><p>I suggest that dataset bias is real but exaggerated by the tank story, giving a misleading indication of risks from deep learning and that it would be better to not repeat it but use real examples of dataset bias and focus on larger-scale risks like AI systems optimizing for wrong utility functions.</p>">The Neural Net Tank Urban Legend</a></p></li>
<li><p>Neural net poems: <a href="./RNN-metadata" class="docMetadata" data-popup-title="RNN metadata for mimicking individual author style" data-popup-author="Gwern Branwen" data-popup-date="12 Sep 2015" data-popup-abstract="<p>Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. An experiment using <code>torch-rnn</code> on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.</p><p>I further try &amp;amp; fail to train <a href=&quot;#geocities-char-rnn&quot;>a char-RNN on Geocities HTML</a> for unclear reasons.</p><p>More successfully, <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>I experiment with a recently-developed alternative to char-RNNs</a>, the Transformer NN architecture, by finetuning training OpenAIâs GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp; lines with inline metadata (the source book). The poetry generated is of considerably higher quality than my char-RNNs.</p>"><span class="smallcaps-auto">RNN</span>s</a>/<a href="./GPT-2" class="docMetadata" data-popup-title="GPT-2 Neural Network Poetry" data-popup-author="Gwern Branwen" data-popup-date="3 March 2019" data-popup-abstract="<p>In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: <q>âGPT-2-poetryâ</q>, trained on the poems as a continuous stream of text, and <q>âGPT-2-poetry-prefixâ</q>, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.</p><p>With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.</p>"><span class="smallcaps-auto">GPT-2</span>s</a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.<br><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure>">This Waifu Does Not Exist</a> (<a href="./TWDNE" class="docMetadata" data-popup-title="This Waifu Does Not Exist" data-popup-author="Gwern Branwen" data-popup-date="19 Feb 2019" data-popup-abstract="<p>Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, <a href=&quot;https://www.thiswaifudoesnotexist.net/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;ThisWaifuDoesNotExist.net&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-02-19&quot; data-popup-abstract=&quot;&amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;&amp;lt;code&amp;gt;ThisWaifuDoesNotExist.net&amp;lt;/code&amp;gt;&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;https://www.gwern.net/TWDNE&amp;quot;&amp;gt;TWDNE&amp;lt;/a&amp;gt;) is a static website which uses JS to display random &amp;lt;a href=&amp;quot;https://www.gwern.net/Faces&amp;quot;&amp;gt;anime faces generated by StyleGAN&amp;lt;/a&amp;gt; neural networks, along with &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-2&amp;quot;&amp;gt;GPT-2&amp;lt;/a&amp;gt;-generated &amp;#39;anime plot summaries&amp;#39;.&amp;lt;br&amp;gt;&amp;lt;figure&amp;gt;&amp;lt;img src=&amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;quot; alt=&amp;quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;quot; /&amp;gt;&amp;lt;figcaption&amp;gt;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&quot;><q>âThis Waifu Does Not Existâ</q></a> for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or <a href=&quot;https://klimaleksus.github.io/FindTwin/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&quot;>online</a> <a href=&quot;https://github.com/darabos/high-five-trading&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Action stock exchange game for Repl.it Game Jam 2019&quot;>games</a>, uploaded to Pixiv, and used in a research paper (<a href=&quot;https://arxiv.org/abs/1904.01774&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot; data-popup-author=&quot;Atsuhiro Noguchi, Tatsuya Harada&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&quot; title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot;>Noguchi &amp;amp; Harada 2019</a>). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, <a href=&quot;https://waifulabs.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Waifu Labs</a>, which generates even better anime faces than my StyleGAN results.</p>">details</a>)</p></li>
<li><p><a href="./GPT-2-music" class="docMetadata" data-popup-title="GPT-2 Folk Music" data-popup-author="Gwern Branwen" data-popup-date="2019-11-01" data-popup-abstract="<p>In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp; copying of patterns, without problems with the hidden-state bottleneck.</p> <p>I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after <a href=&quot;/GPT-2-music#spaceless-model&quot;>fixing that</a>, I successfully trained it on <em>n</em>=205,304 ABC music pieces taken from The Session &amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.</p> <p>The model &amp; dataset are available for download, and I provide for listening selected <a href=&quot;/GPT-2-music#samples&quot;>music samples</a> as well as medleys of random samples from throughout training.</p>"><span class="smallcaps-auto">GPT</span>-2 Folk Music</a></p></li>
<li><p><a href="./GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like GPT-2, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ârealisticâ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpusâs average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the âenvironmentâ for a blackbox DRL algorithm like PPO.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on GPT-2. Having previously used <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>GPT-2 for poetry</a> &amp;amp; <a href=&quot;./GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The model &amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with GPT-2 preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>â10,000 ratings, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model directly model the implied utility of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which optimizes sequences to maximize the reward modelâs output.</p>">Preference learning GPT-2 music</a></p></li>
<li><p><a href="./Hyperbolic-Time-Chamber" class="docMetadata" data-popup-title="The Hyperbolic Time Chamber as Brain Emulation Analogy" data-popup-author="Gwern Branwen" data-popup-date="29 Aug 2012" data-popup-abstract="<p>A time dilation tool from an anime is discussed for its practical use on Earth; there seem surprisingly few uses and none that will change the world, due to the severe penalties humans would incur while using it, and basic constraints like Amdahlâs law limit the scientific uses. A comparison with the position of an Artificial Intelligence such as an emulated human brain seems fair, except most of the time dilation disadvantages do not apply or can be ameliorated and hence any speedups could be quite effectively exploited. I suggest that skeptics of the idea that speedups give advantages are implicitly working off the crippled time dilation tool and not making allowance for the <em>dis</em>analogies.</p>">Hyperbolic Time Chambers as Brain Ems</a></p></li>
</ul>
</section>
<section id="cs" class="level1">
<h1><a href="#cs" title="Link to section: 'CS'">CS</a></h1>
<ul>
<li><a href="./haskell/Summer-of-Code" class="docMetadata" data-popup-title="Summers of Code, 2006-2013" data-popup-author="Gwern Branwen" data-popup-date="11 Feb 2009" data-popup-abstract="<p>A compilation of Haskell-related student projects 2006-2013, with evaluations of their usefulness to the Haskell community, thoughts on what makes a good project, and predictions for 2011-2013.</p>">Haskell Summer of Code</a></li>
<li><a href="./Archiving-URLs" class="docMetadata" data-popup-title="Archiving URLs" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2011" data-popup-abstract="<p>Links on the Internet last forever or a year, whichever comes first. This is a major problem for anyone serious about writing with good references, as link rot will cripple several percent of all links each year, and compounding.</p><p>To deal with link rot, I present my multi-pronged archival strategy using a combination of scripts, daemons, and Internet archival services: URLs are regularly dumped from both my web browserâs daily browsing and my website pages into an archival daemon I wrote, which pre-emptively downloads copies locally and attempts to archive them in the Internet Archive. This ensures a copy will be available indefinitely from one of several sources. Link rot is then detected by regular runs of <code>linkchecker</code>, and any newly dead links can be immediately checked for alternative locations, or restored from one of the archive sources.</p><p>As an additional flourish, my local archives are <a href=&quot;./Timestamping&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Easy Cryptographic Timestamping of Files&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;4 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to &amp;lt;em&amp;gt;timestamp&amp;lt;/em&amp;gt; it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp;amp; easily timestamp arbitrary files &amp;amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.&amp;lt;/p&amp;gt;&quot;>efficiently cryptographically timestamped using Bitcoin</a> in case forgery is a concern, and I demonstrate a simple compression trick for <a href=&quot;./Archiving-URLs#sort---key-compression-trick&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; compression trick (CLI folklore)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2014-03-03&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Programming folklore notes that one way to get better lossless compression efficiency is to rearrange files inside the archive to group âsimilarâ files together and expose redundancy to the compressor, in accordance with information-theoretical principles. A particularly easy and broadly-applicable way of doing this, which does not require using any unusual formats or tools and is fully compatible with the default archive methods, is to sort the files by &amp;lt;em&amp;gt;filename&amp;lt;/em&amp;gt; and especially file extension. I show how to do this with the standard Unix command-line &amp;lt;code&amp;gt;sort&amp;lt;/code&amp;gt; tool, using the so-called â&amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trickâ, and give examples of the large space-savings possible from my archiving work for personal website mirrors and for making &amp;lt;a href=&amp;quot;/DNM-archives&amp;quot;&amp;gt;darknet market mirror datasets&amp;lt;/a&amp;gt; where the redundancy at the file level is particularly extreme and the &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trick shines compared to the naive approach.&amp;lt;/p&amp;gt;&quot;>substantially reducing sizes of large web archives</a> such as crawls (particularly useful for repeated crawls such as my <a href=&quot;./DNM-archives&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market Archives (2013-2015)&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;1 Dec 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;usage&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;./DNM-survival&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Darknet Market mortality risks&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;amp;lt;a href=&amp;amp;quot;./Silk-Road&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;Silk Road 1: Theory &amp;amp;amp;amp; Practice&amp;amp;quot; data-popup-author=&amp;amp;quot;11 Jul 2011&amp;amp;quot; data-popup-date=&amp;amp;quot;gwern&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;div id=&amp;amp;amp;quot;abstract&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;blockquote&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;/blockquote&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;Silk Road 1&amp;amp;lt;/a&amp;amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;lifetimes/characteristics&amp;lt;/a&amp;gt;, &amp;amp;amp; &amp;lt;a href=&amp;quot;./DNM-arrests&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Tor DNM-related arrests, 2011-2015&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;legal riskiness&amp;lt;/a&amp;gt;; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp;amp; documents related to the DNMs as I could.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp;amp; 37+ related forums, representing &amp;amp;lt;4,438 mirrors, and is available for any research.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This page documents the download, contents, interpretation, and technical methods behind the scrapes.&amp;lt;/p&amp;gt;&quot;>DNM archives</a>).</p>">Archiving <span class="smallcaps-auto">URL</span>s</a></li>
<li><a href="./The-3-Grenades" class="docMetadata" data-popup-title="The Three Grenades" data-popup-author="Gwern Branwen" data-popup-date="24 Nov 2008" data-popup-abstract="CMU game connected to error coding theory">The 3 Grenades</a></li>
<li><a href="./Turing-complete" class="docMetadata" data-popup-title="Surprisingly Turing-Complete" data-popup-author="Gwern Branwen" data-popup-date="9 Dec 2012" data-popup-abstract="<p>âComputersâ, in the sense of being Turing-complete, are extremely common. Almost any system of sufficient complexityâunless carefully engineered otherwiseâmay be found to âaccidentallyâ support Turing-complete somewhere inside it, even systems which would appear to have not the slightest thing to do with computation. Software systems are especially susceptible to this, which often leads to serious security problems as the Turing-complete components can be used to run attacks on the rest of the system.</p><p>I provide a running catalogue of systems which have been, surprisingly, demonstrated to be Turing-complete.</p>">Surprisingly Turing-Complete</a></li>
<li><a href="./haskell/Archiving-GitHub" class="docMetadata" data-popup-title="Archiving GitHub" data-popup-author="Gwern Branwen" data-popup-date="20 Mar 2011" data-popup-abstract="Scraping and downloading Haskell-related repositories from GitHub">Archiving GitHub</a></li>
<li><a href="./Simulation-inferences" class="docMetadata" data-popup-title="Simulation Inferences" data-popup-author="Gwern Branwen" data-popup-date="29 May 2009" data-popup-abstract="How small must be the computer simulating the universe?">Simulation inferences</a></li>
<li><a href="./Choosing-Software" class="docMetadata" data-popup-title="Choosing Software" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Criteria making software useful long-term &amp; worth learning in detail">Choosing Software</a></li>
<li><a href="./haskell/Run-Length-Encoding" class="docMetadata" data-popup-title="Golfing Run Length Encoding in Haskell" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Haskell: step by step refactoring to concision">Elegant Run Length Encoding</a></li>
<li><a href="./haskell/Wikipedia-Archive-Bot" class="docMetadata" data-popup-title="Writing a Wikipedia Link Archive Bot" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="<p>This is a 2008 tutorial demonstrating how to write a Haskell program to automatically archive Internet links into WebCite &amp;amp; Internet Archive to avoid linkrot, by parsing WP dumps, downloading &amp;amp; parsing WP articles for external links with the TagSoup HTML parsing library, using the WebCite/IA APIs to archive them, and optimizing runtime. This approach is suitable for one-off crawls but not for live archiving using the RSS feed, see <a href=&quot;../haskell/Wikipedia-RSS-Archive-Bot&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Writing a Wikipedia RSS Link Archive Bot&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;02 Nov 2009&quot; data-popup-abstract=&quot;Archiving using Wikipedia Recent Changes RSS feed&quot;>Wikipedia RSS Archive Bot</a> for a demonstration of how one could write a RSS-oriented daemon. These Haskell scripts are (hopefully) obsoleted by later IA/WMF initiatives dealing with linkrot. For a more general approach suitable for personal use, see the writeup of <code>archiver-bot</code> in <a href=&quot;../Archiving-URLs&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Archiving URLs&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;10 Mar 2011&quot; data-popup-abstract=&quot;Archiving the Web, because nothing lasts forever: statistics, online archive services, extracting URLs automatically from browsers, and creating a daemon to regularly back up URLs to multiple sources.&quot;>Archiving URLs</a>.</p>">Wikipedia Archive Bot</a></li>
<li><a href="./haskell/Wikipedia-RSS-Archive-Bot" class="docMetadata" data-popup-title="Writing a Wikipedia RSS Link Archive Bot" data-popup-author="Gwern Branwen" data-popup-date="02 Nov 2009" data-popup-abstract="Archiving using Wikipedia Recent Changes RSS feed"><span class="smallcaps-auto">WP</span> <span class="smallcaps-auto">RSS</span> bot</a></li>
<li><a href="./Resilient-Haskell-Software" class="docMetadata" data-popup-title="Resilient Haskell Software" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Lessons learned about bitrot in Haskell software">Resilient Haskell Software</a></li>
<li><a href="./Evolutionary-Licenses" class="docMetadata" data-popup-title="Evolutionary Software Licenses" data-popup-author="Gwern Branwen" data-popup-date="27 Jan 2009" data-popup-abstract="Game theory on BSD vs. GPL: partnership? Is GPL an evolutionary stable strategy against invasion by proprietary copyright strategies?">Evolutionary Licenses</a></li>
<li><em><span class="smallcaps-auto">SICP</span></em> <a href="./sicp/Introduction" class="docMetadata" data-popup-title="SICP Introduction" data-popup-author="Gwern Branwen" data-popup-date="14 Mar 2009" data-popup-abstract="Links to various resources for SICP studying">Introduction</a> (<a href="./sicp/Chapter-1.1" class="docMetadata" data-popup-title="SICP Chapter 1.1 notes" data-popup-author="Gwern Branwen" data-popup-date="14 Mar 2009" data-popup-abstract="Syntax, function definitions">ch1.1</a>, <a href="./sicp/Chapter-1.2" class="docMetadata" data-popup-title="SICP Chapter 1.2 notes" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="recursion into iteration; primality testing">1.2</a>, <a href="./sicp/Chapter-1.3" class="docMetadata" data-popup-title="SICP Chapter 1.3" data-popup-author="Gwern Branwen" data-popup-date="09 Jan 2010" data-popup-abstract="Generalizing functions with hardwired values">1.3</a>)</li>
<li><a href="./WiFi" class="docMetadata" data-popup-title="Internet WiFi improvement" data-popup-author="Gwern Branwen" data-popup-date="20 Oct 2016" data-popup-abstract="<p>My laptop in my apartment receives Internet via a WiFi repeater to another house, yielding slow speeds and frequent glitches. I replaced the obsolete WiFi router and increased connection speeds somewhat but still inadequate. For a better solution, I used a directional antenna to connect directly to the new WiFi router, which, contrary to my expectations, yielded a ~6x increase in speed. Extensive benchmarking of all possible arrangements of laptops/dongles/repeaters/antennas/routers/positions shows that the antenna+router is inexpensive and near optimal speed, and that the only possible improvement would be a hardwired Ethernet line, which I installed a few weeks later after learning it was not as difficult as I thought it would be.</p>">Internet WiFi improvement</a></li>
<li><a href="./AB-testing" class="docMetadata" data-popup-title="A/B testing long-form readability on gwern.net" data-popup-author="Gwern Branwen" data-popup-date="16 Jun 2012" data-popup-abstract="<p>To gain some statistical &amp;amp; web development experience and to improve my readersâ experiences, I have been running a series of CSS A/B tests since June 2012. As expected, most do not show any meaningful difference.</p>">A/B testing CSS &amp; HTML</a></li>
</ul>
</section>
<section id="psychology" class="level1">
<h1><a href="#psychology" title="Link to section: 'Psychology'">Psychology</a></h1>
<ul>
<li><a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">Spaced Repetition &amp; Learning</a></li>
<li><a href="./DNB-FAQ" class="docMetadata" data-popup-title="Dual N-Back FAQ" data-popup-author="Gwern Branwen" data-popup-date="25 Mar 2009" data-popup-abstract="A compendium of DNB, WM, IQ information up to 2015">Dual N-Back <span class="smallcaps-auto">FAQ</span></a> (<a href="./DNB-meta-analysis" class="docMetadata" data-popup-title="Dual N-Back Meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="20 May 2012" data-popup-abstract="<p>I meta-analyze the &amp;gt;19 studies up to 2016 which measure IQ after an <a href=&quot;./DNB-FAQ&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Dual N-Back FAQ&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;25 Mar 2009&quot; data-popup-abstract=&quot;A compendium of DNB, WM, IQ information up to 2015&quot;>n-back</a> intervention, finding (over all studies) a net <a href=&quot;#analysis&quot;>gain</a> (medium-sized) on the post-training IQ tests.</p><p>The size of this increase on IQ test score correlates highly with the methodological concern of whether a study used <a href=&quot;#control-groups&quot;>active or passive control groups</a>. This indicates that the medium effect size is due to methodological problems and that n-back training does not increase subjectsâ underlying fluid intelligence but the gains are due to the motivational effect of passive control groups (who did not train on anything) not trying as hard as the n-back-trained experimental groups on the post-tests. The remaining studies using active control groups find a small positive effect (but this may be due to matrix-test-specific training, undetected publication bias, smaller motivational effects, etc.)</p><p>I also investigate several other n-back claims, criticisms, and indicators of bias, finding:</p><ul><li><a href=&quot;#paymentextrinsic-motivation&quot;>payment reducing performance</a> claim: possible</li><li><a href=&quot;#training-time&quot;>dose-response relationship</a> of n-back training time &amp;amp; IQ gains claim: not found</li><li><a href=&quot;#training-type&quot;>kind of n-back</a> matters: not found</li><li><a href=&quot;#biases&quot;>publication bias</a> criticism: not found</li><li><a href=&quot;#iq-test-time&quot;>speeding of IQ tests</a> criticism: not found</li></ul>">meta-analysis</a>)</li>
<li><a href="./Embryo-selection" class="docMetadata" data-popup-title="Embryo selection for intelligence" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>With genetic predictors of a phenotypic trait, it is possible to select embryos during an in vitro fertilization process to increase or decrease that trait. Extending the work of <a href=&quot;./docs/iq/2014-shulman.pdf&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;417&quot; title=&quot;Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?&quot;>Shulman &amp;amp; Bostrom 2014</a>/<a href=&quot;https://arxiv.org/abs/1408.3421&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;On the genetic architecture of intelligence and other quantitative  traits&quot; data-popup-author=&quot;Stephen D. H. Hsu&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;How do genes affect cognitive ability or other human quantitative traits such as height or disease risk? Progress on this challenging question is likely to be significant in the near future. I begin with a brief review of psychometric measurements of intelligence, introducing the idea of a &amp;quot;general factor&amp;quot; or g score. The main results concern the stability, validity (predictive power), and heritability of adult g. The largest component of genetic variance for both height and intelligence is additive (linear), leading to important simplifications in predictive modeling and statistical estimation. Due mainly to the rapidly decreasing cost of genotyping, it is possible that within the coming decade researchers will identify loci which account for a significant fraction of total g variation. In the case of height analogous efforts are well under way. I describe some unpublished results concerning the genetic architecture of height and cognitive ability, which suggest that roughly 10k moderately rare causal variants of mostly negative effect are responsible for normal population variation. Using results from Compressed Sensing (L1-penalized regression), I estimate the statistical power required to characterize both linear and nonlinear models for quantitative traits. The main unknown parameter s (sparsity) is the number of loci which account for the bulk of the genetic variation. The required sample size is of order 100s, or roughly a million in the case of cognitive ability.&quot; title=&quot;On the genetic architecture of intelligence and other quantitative traits&quot;>Hsu 2014</a>, I consider the case of human intelligence using SNP-based genetic prediction, finding:</p><ul><li>a meta-analysis of <a href=&quot;https://en.wikipedia.org/wiki/GCTA&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Genome-wide complex trait analysis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Genome-wide complex trait analysis (GCTA) Genome-based restricted maximum likelihood (GREML)&amp;lt;/b&amp;gt; is a statistical method for variance component estimation in genetics which quantifies the total narrow-sense (additive) contribution to a trait&amp;#39;s heritability of a particular subset of genetic variants. This is done by directly quantifying the chance genetic similarity of unrelated individuals and comparing it to their measured similarity on a trait; if two unrelated individuals are relatively similar genetically and also have similar trait measurements, then the measured genetics are likely to causally influence that trait, and the correlation can to some degree tell how much. This can be illustrated by plotting the squared pairwise trait differences between individuals against their estimated degree of relatedness. The GCTA framework can be applied in a variety of settings. For example, it can be used to examine changes in heritability over aging and development.. It can also be extended to analyse bivariate genetic correlations between traits. There is an ongoing debate about whether GCTA generates reliable or stable estimates of heritability when used on current SNP data. The method is based on the outdated and false dichotomy of genes versus the environment. It also suffers from serious methodological weaknesses, such as susceptibility to population stratification.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: GCTA&quot;>GCTA</a> results indicates that SNPs can explain &amp;gt;33% of variance in current intelligence scores, and &amp;gt;44% with better-quality phenotype testing</li><li>this sets an upper bound on the effectiveness of selection: a gain of 9 IQ points when selecting the top embryo out of 10</li><li>the best 2016 polygenic score could achieve a gain of ~3 IQ points when selecting out of 10</li><li>the marginal cost of embryo selection (assuming IVF is already being done) is modest, at $1500 + $200 per embryo, with the sequencing cost projected to drop rapidly</li><li>a model of the IVF process, incorporating number of extracted eggs, losses to abnormalities &amp;amp; vitrification &amp;amp; failed implantation &amp;amp; miscarriages from 2 real IVF patient populations, estimates feasible gains of 0.39 &amp;amp; 0.68 IQ points</li><li>embryo selection is currently unprofitable (mean: -$358) in the USA under the lowest estimate of the value of an IQ point, but profitable under the highest (mean: $6230). The main constraints on selection profitability is the polygenic score; under the highest value, the NPV EVPI of a perfect SNP predictor is $24b and the EVSI per education/SNP sample is $71k</li><li>under the worst-case estimate, selection can be made profitable with a better polygenic score, which would require <em>n</em>&amp;gt;237,300 using education phenotype data (and much less using fluid intelligence measures)</li><li>selection can be made more effective by selecting on multiple phenotype traits: considering an example using 7 traits (IQ/height/BMI/diabetes/ADHD/bipolar/schizophrenia), there is a factor gain over IQ alone; the outperformance of multiple selection remains after adjusting for genetic correlations &amp;amp; polygenic scores and using a broader set of 16 traits. </li></ul>">Embryo selection for intelligence</a></li>
<li>Catnip:
<ul>
<li><a href="./Catnip" class="docMetadata" data-popup-title="Catnip immunity and alternatives" data-popup-author="Gwern Branwen" data-popup-date="7 Nov 2015" data-popup-abstract="<p>Not all cats respond to the catnip stimulant; the rate of responders is generally estimated at ~70% of cats. A meta-analysis of catnip response experiments since the 1940s indicates the true value is ~62%. The low quality of studies and the reporting of their data makes examination of possible moderators like age, sex, and country difficult. Catnip responses have been recorded for a number of species both inside and outside the <em>Felidae</em> family; of them, there is evidence for a catnip response in the Felidae, and, more uncertainly, the Paradoxurinae, and Herpestinae.</p>">Response rate meta-analysis</a></li>
<li><a href="./Catnip-survey" class="docMetadata" data-popup-title="World Catnip Surveys" data-popup-author="gwern" data-popup-date="15 Nov 2015" data-popup-abstract="<p>In compiling a meta-analysis of reports of <a href=&quot;./Catnip&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catnip immunity and alternatives&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;7 Nov 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Not all cats respond to the catnip stimulant; the rate of responders is generally estimated at ~70% of cats. A meta-analysis of catnip response experiments since the 1940s indicates the true value is ~62%. The low quality of studies and the reporting of their data makes examination of possible moderators like age, sex, and country difficult. Catnip responses have been recorded for a number of species both inside and outside the &amp;lt;em&amp;gt;Felidae&amp;lt;/em&amp;gt; family; of them, there is evidence for a catnip response in the Felidae, and, more uncertainly, the Paradoxurinae, and Herpestinae.&amp;lt;/p&amp;gt;&quot;>catnip response rats in domestic cats</a>, yielding an meta-analytic average of ~<span class=&quot;mjpage&quot;><span class=&quot;mjx-chtml&quot;><span class=&quot;mjx-math&quot; aria-label=&quot;Equation&quot;><span class=&quot;mjx-mrow&quot; aria-hidden=&quot;true&quot;><span class=&quot;mjx-semantics&quot;><span class=&quot;mjx-mfrac&quot;><span class=&quot;mjx-box MJXc-stacked&quot; style=&quot;width: 0.495em; padding: 0px 0.12em;&quot;><span class=&quot;mjx-numerator&quot; style=&quot;font-size: 70.7%; width: 0.7em; top: -1.372em;&quot;><span class=&quot;mjx-mn&quot; style><span class=&quot;mjx-char MJXc-TeX-main-R&quot; style=&quot;padding-top: 0.372em; padding-bottom: 0.372em;&quot;>2</span></span></span><span class=&quot;mjx-denominator&quot; style=&quot;font-size: 70.7%; width: 0.7em; bottom: -0.686em;&quot;><span class=&quot;mjx-mn&quot; style><span class=&quot;mjx-char MJXc-TeX-main-R&quot; style=&quot;padding-top: 0.372em; padding-bottom: 0.372em;&quot;>3</span></span></span><span style=&quot;border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;&quot; class=&quot;mjx-line&quot;></span></span><span style=&quot;height: 1.456em; vertical-align: -0.485em;&quot; class=&quot;mjx-vsize&quot;></span></span></span></span></span></span></span>, the available data suggests heterogeneity from cross-country differences in rates (possibly for genetic reasons) but is insufficient to definitively demonstrate the existence of or estimate those differences (particularly a possible extremely high catnip response rate in Japan). I use Google Surveys August-September 2017 to conduct a brief 1-question online survey of a proportional population sample of 9 countries about cat ownership &amp;amp; catnip use, specifically: Canada, the USA, UK, Japan, Germany, Brazil, Spain, Australia, &amp;amp; Mexico. in total, I surveyed <em>n</em>=31,471 people, of whom <em>n</em>=9,087 are cat owners, of whom <em>n</em>=4,402 report having used catnip on their cat, and of whom <em>n</em>=2996 report a catnip response.</p><p>The survey yields catnip response rates of Canada (82%), USA (79%), UK (74%), Japan (71%), Germany (57%), Brazil (56%), Spain (54%), Australia (53%), and Mexico (52%). The differences are substantial and of high posterior probability, supporting the existence of large cross-country differences. In additional analysis, the other conditional probabilities of cat ownership and trying catnip with a cat appear to correlate with catnip response rates; this intercorrelation suggests a âcat factorâ of some sort influencing responses, although what causal relationship there might be between proportion of cat owners and proportion of catnip-responder cats is unclear.</p><p>An additional survey of a convenience sample of primarily US Internet users about catnip is reported, although the improbable catnip response rates compared to the population survey suggest the respondents are either highly unrepresentative or the questions caused demand bias.</p>">Surveys</a></li>
</ul></li>
<li><a href="./Sunk-cost" class="docMetadata" data-popup-title="Are Sunk Costs Fallacies?" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2012" data-popup-abstract="Human and animal sunk costs often aren't, and sunk cost bias may be useful on an individual level to encourage learning. Convincing examples of sunk cost bias typically operate on organizational levels and are probably driven by non-psychological causes like competition.">Are Sunk Costs Fallacies?</a></li>
<li><a href="./Iodine" class="docMetadata" data-popup-title="Iodine and Adult IQ meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="29 Feb 2012" data-popup-abstract="<p>Iodization is one of the great success stories of public health intervention: iodizing salt costs pennies per ton, but as demonstrated in randomized &amp;amp; natural experiments, prevents goiters, cretinism, and can boost population IQs by a fraction of a standard deviation in the most iodine-deficient populations.</p><p>These experiments are typically done on pregnant women, and results suggest that the benefits of iodization diminish throughout the trimesters of a pregnancy. So does iodization benefit normal healthy <em>adults</em>, potentially even ones in relatively iodine-sufficient Western countries?</p><p>Compiling existing post-natal iodization studies which use cognitive tests, I find thatâoutliers asideâthe benefit appears to be nearly zero, and so likely it does not help normal healthy adults, particularly in Western adults.</p>">Iodine/IQ meta-analysis</a></li>
<li><a href="./SMPY" class="docMetadata" data-popup-title="SMPY Bibliography" data-popup-author="Gwern Branwen" data-popup-date="28 July 2018" data-popup-abstract="<p>SMPY (Study of Mathematically Precocious Youth) is a long-running longitudinal survey of extremely mathematically-talented or intelligent youth, which has been following high-IQ cohorts since the 1970s. It has provided the largest and most concrete findings about the correlates and predictive power of screening extremely intelligent children, and revolutionized gifted &amp; talented educational practices.</p><p>Because it has been running for over 40 years, SMPY-related publications are difficult to find; many early papers were published only in long-out-of-print books and are not available in any other way. Others are digitized and more accessible, but one must already know they exist. Between these barriers, SMPY information is less widely available &amp; used than it should be given its importance.</p><p>To fix this, I have been gradually going through all SMPY citations and making fulltext copies available online with occasional commentary.</p>"><span class="smallcaps-auto">SMPY</span> Bibliography</a></li>
<li><a href="./Conscientiousness-and-online-education" class="docMetadata" data-popup-title="Conscientiousness &amp; Online Education" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Technology-driven shift in demand for Conscientiousness, not intelligence">Conscientiousness &amp; Online Schools</a></li>
<li><a href="./Bakewell" class="docMetadata" data-popup-title="Origins of Innovation: Bakewell &amp; Breeding" data-popup-author="Gwern Branwen" data-popup-date="28 Oct 2018" data-popup-abstract="<p>Like anything else, the idea of âbreedingâ had to be <em>invented</em>. That traits are genetically-influenced broadly equally by both parents subject to considerable randomness and can be selected for over many generations to create large average population-wide increases had to be discovered the hard way, with many wildly wrong theories discarded along the way. Animal breeding is a case in point, as reviewed by an intellectual history of animal breeding, <em>Like Engendâring Like</em>, which covers mistaken theories of conception &amp;amp; inheritance from the ancient Greeks to perhaps the first truly successful modern animal breeder, <a href=&quot;https://en.wikipedia.org/wiki/Robert_Bakewell_%28agriculturalist%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Robert Bakewell (agriculturalist)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Robert Bakewell&amp;lt;/b&amp;gt; was a British agriculturalist, now recognized as one of the most important figures in the British Agricultural Revolution. In addition to work in agronomy, Bakewell is particularly notable as the first to implement systematic selective breeding of livestock. His advancements not only led to specific improvements in sheep, cattle and horses, but contributed to general knowledge of artificial selection.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Robert Bakewell (agriculturalist)&quot;>Robert Bakewell</a>.</p><p>Why did it take thousands of years to begin developing useful animal breeding techniques, a topic of interest to almost all farmers everywhere, a field which has no prerequisites such as advanced mathematics or special chemicals or mechanical tools, and seemingly requires only close observation and patience? This question can be asked of many innovations early in the Industrial Revolution, such as the flying shuttle.</p><p>Some veins in economics history and sociology suggest that at least one ingredient is an <em>improving attitude</em>: a detached outsiderâs attitude which asks whether there is any way to optimize something, in defiance of âthe wisdom of traditionâ, and looks for improvements. A relevant English example is the English Royal Society of Arts, founded not too distant in time from Bakewell, specifically to spur competition and imitation and new inventions. Psychological barriers may be as important as anything like per capita wealth or peace in innovation.</p>">Robert Bakewell &amp; Inventing Breeding</a></li>
<li><a href="./Clone" class="docMetadata" data-popup-title="Dog Cloning For Special Forces: Breed All You Can Breed" data-popup-author="Gwern Branwen" data-popup-date="18 Sep 2018" data-popup-abstract="<p>Cloning is widely used in animal &amp;amp; plant breeding despite steep costs due to its advantages; more unusual recent applications include creating entire polo horse teams and reported trials of cloning in elite police/Special Forces war dogs. Given the cost of dog cloning, however, can this ever make more sense than standard screening methods for selecting from working dog breeds, or would the increase in successful dog training be too low under all reasonable models to turn a profit?</p><p>I model the question as one of expected cost per dog with the trait of successfully passing training, success in training being a dichotomous liability threshold with a polygenic genetic architecture; given the extreme level of selection possible in selecting the best among already-elite Special Forces dogs and a range of heritabilities, this predicts clonesâ success probabilities. To approximate the relevant parameters, I look at some reported training costs and success rates for regular dog candidates, broad dog heritabilities, and the few current dog cloning case studies reported in the media.</p><p>Since none of the relevant parameters are known with confidence, I run the cost-benefit equation for many hypothetical scenarios, and find that in a large fraction of them covering most plausible values, dog cloning would improve training yields enough to be profitable (in addition to its other advantages).</p><p>As further illustration of the use-case of screening for an extreme outcome based on a partial predictor, I consider the question of whether height PGSes could be used to screen the US population for people of NBA height, which turns out to be <a href=&quot;#nba-screening-scenario&quot;>reasonably doable</a> with current &amp;amp; future PGSes.</p>">Dog Cloning For Special Forces</a></li>
<li><a href="./Questions" class="docMetadata" data-popup-title="Open Questions" data-popup-author="Gwern Branwen" data-popup-date="17 Oct 2018" data-popup-abstract="<p><span class=&quot;smallcaps&quot;>A list of some questions</span> which are not necessarily important, but do puzzle me or where I find existing âanswersâ to be unsatisfying, categorized by subject (along the lines of <a href=&quot;https://patrickcollison.com/questions&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;512&quot;>Patrick Collisonâs list</a> &amp;amp; <a href=&quot;https://guzey.com/personal/research-ideas/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Alex Guzey</a>; see also <a href=&quot;./Statistical-notes#someone-should-do-something-wishlist-of-miscellaneous-project-ideas&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Statistical Notes&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;17 July 2014&quot; data-popup-abstract=&quot;Miscellaneous statistical stuff&quot;>my list of project ideas</a>).</p>">Open Questions</a></li>
<li><a href="./Morning-writing" class="docMetadata" data-popup-title="What Is The Morning Writing Effect?" data-popup-author="Gwern Branwen" data-popup-date="11 May 2011" data-popup-abstract="<p>Ericsson 1993 notes that many major writers or researchers prioritized writing by making it the first activity of their day, often getting up early in the morning. This is based largely on writers anecdotally reporting they write best first thing early in the morning, apparently even if they are not morning people, although there is some additional survey/software-logging evidence of morning writing being effective. I compile all the anecdotes of writers discussing their writing times I have come across thus far. Do they, and why?</p>">What is the morning-writing effect?</a></li>
<li><a href="./Cat-Sense" class="docMetadata" data-popup-title="Cat Psychology &amp; Domestication: Are We Good Owners?" data-popup-author="Gwern Branwen" data-popup-date="3 Nov 2018" data-popup-abstract="<p>I review John Bradshawâs book on cat psychology, <em>Cat Sense</em>, after difficulties dealing with my own cat. Bradshaw reviews the history of domestic cats from their apparent Middle Eastern origins as a small solitary desert predator to their domestication in Ancient Egypt where breeding millions of cats for sacrifice may have played a critical role (as opposed to any unique role as a vermin exterminator) through to the modern day and psychological studies of the learning abilities and personalities of cats, with particular emphasis on cat social skills in <q>âcat coloniesâ</q> &amp;amp; plasticity in kittenhood. As Bradshaw diagnoses it, these are responsible for what ability they have to modern pet life, even though they are not bred for this like dogs; every tame cat still has the feral cat in them, and are in many ways unsuited for contemporary living, with disturbing hints that human lack of selective breeding plus recent large-scale spay/neuter population control efforts may be producing a subtle <em>dysgenic</em> effect on domestication, and this double neglect &amp;amp; backfire may be responsible for disturbingly high rates of cat maladaptation &amp;amp; chronic stress diseases.</p>">Cat Psychology &amp; Domestication: Are We Good Owners?</a></li>
<li><a href="./Replication" class="docMetadata" data-popup-title="The Replication Crisis: Flaws in Mainstream Science" data-popup-author="Gwern Branwen" data-popup-date="2010-10-27" data-popup-abstract="<p>Long-standing problems in standard scientific methodology have exploded as the <q>â<a href=&quot;https://en.wikipedia.org/wiki/Replication_Crisis&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Replication crisis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p>The <b>replication crisis</b> is, as of 2019, an ongoing methodological crisis in which it has been found that many scientific studies are difficult or impossible to replicate or reproduce. The replication crisis affects the social sciences and medicine most severely. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in the field of metascience.</p>&quot; title=&quot;Wikipedia: Replication Crisis&quot;>Replication Crisis</a>â</q>: the discovery that many results in fields as diverse as psychology, economics, medicine, biology, and sociology are in fact false or quantitatively highly inaccurately measured. I cover here a handful of the issues and publications on this large, important, and rapidly developing topic up to about 2013, at which point the Replication Crisis became too large a topic to cover more than cursorily. (A <a href=&quot;./Replication#further-reading&quot;>compilation of some additional links</a> are provided for post-2013 developments.)</p> <p>The crisis is caused by methods &amp; publishing procedures which interpret random noise as important results, far too small datasets, selective analysis by an analyst trying to reach expected/desired results, publication bias, poor implementation of existing best-practices, nontrivial levels of research fraud, software errors, philosophical beliefs among researchers that false positives are acceptable, neglect of known confounding like genetics, and skewed incentives (financial &amp; professional) to publish âhotâ results.</p> <p>Thus, any individual piece of research typically establishes little. Scientific validation comes not from small <em>p</em>-values, but from discovering a regular feature of the world which disinterested third parties can discover with straightforward research done independently on new data with new proceduresâ<em>replication</em>.</p>">The Replication Crisis</a></li>
<li><a href="./Hydrocephalus" class="docMetadata" data-popup-title="Hydrocephalus and Intelligence: The Hollow Men" data-popup-author="Gwern Branwen" data-popup-date="2015-07-28" data-popup-abstract="<p>Hydrocephalus is a damaging brain disorder where fluids compress the brain, drastically decreasing its volume. While often extremely harmful or life-threatening when untreated, some people with the condition nevertheless are relatively normal, and in one case (Lorber) they have been claimed to have IQs as high as 126 with a brain volume 5% of normal brains. A few of these case studies have been used to argue the extraordinary claim that brain volume has little or nothing to do with intelligence; authors have argued that hydrocephalus suggests enormous untapped cognitive potential which are tapped into rarely for repairs and can boost intelligence on net, or that intelligence/consciousness are nonmaterial or tapping into ESP.</p> <p>I point out why this claim is almost uncertainly untrue because it predicts countless phenomena we never observe, and investigate the claimed examples in more detail: the cases turn out to be suspiciously unverifiable (Lorber), likely fraudulent (Oliveira), or actually low intelligence (Feuillet). It is unclear if high-functioning cases of hydrocephalus even have less brain mass, as opposed to lower proxy measures like brain volume.</p> <p>I then summarize anthropologist John Hawksâs criticisms of the original hydrocephalus author: his brain imaging data could not have been as precise as claimed, he studied a selective sample, the story of the legendary IQ 126 hydrocephalus patient raises questions as to how normal or intelligent he really was, and hydrocephalus in general appears to be no more anomalous or hard-to-explain than many other kinds of brain injuries, and in a comparison, hemispherectomies, removing or severing a hemisphere, has produced no anomalous reports of above-average intelligence (just deficits), though they ought to be just the same in terms of repairs or ESP.</p> <p>That hydrocephalus cases can reach roughly normal levels of functioning, various deficits aside, can be explained by brain size being one of several relevant variables, brain plasticity enabling cognitive flexibility &amp; recovery from gradually-developing conditions, and overparameterization giving robustness to damage and poor environments, and learning ability. The field of deep learning has observed similar phenomenon in training of artificial neural networks. This is consistent with Lorberâs original contention that the brain was more robust, and hydrocephalus was more treatable, than commonly accepted, but does not support any of the more exotic interpretations since put on his findings.</p> <p>In short, there is little anomalous to explain, and standard brain-centric accounts appear to account for existing verified observations without much problem or resort to extraordinary claims.</p>">Hydrocephalus &amp; IQ</a></li>
<li><a href="./Socks" class="docMetadata" data-popup-title="On Having Enough Socks" data-popup-author="Gwern Branwen" data-popup-date="22 Nov 2017" data-popup-abstract="<p>After running out of socks one day, I reflected on how ordinary tasks get neglected. Anecdotally and in 3 online surveys, people report often not having enough socks, a problem which correlates with rarity of sock purchases and demographic variables, consistent with a neglect/procrastination interpretation: because there is no specific time or triggering factor to replenish a shrinking sock stockpile, it is easy to run out.</p><p>This reminds me of akrasia on minor tasks, âyak shavingâ, and the nature of disaster in complex systems: lack of hard rules lets errors accumulate, without any âglobalâ understanding of the drift into disaster (or at least inefficiency). Humans on a smaller scale also âdriftâ when they engage in System I reactive thinking &amp;amp; action for too long, resulting in cognitive biases. An example of drift is the generalized human failure to explore/experiment adequately, resulting in overly greedy exploitative behavior of the current local optimum. Grocery shopping provides a case study: despite large gains, most people do not explore, perhaps because there is no established routine or practice involving experimentation. Fixes for these things can be seen as ensuring that System II deliberative cognition is periodically invoked to review things at a global level, such as developing a habit of maximum exploration at first purchase of a food product, or annually reviewing possessions to note problems like a lack of socks.</p><p>While socks may be small things, they may reflect big things.</p>">On Having Enough Socks</a></li>
</ul>
</section>
<section id="qs" class="level1">
<h1><a href="#qs" title="Link to section: 'QS'">QS</a></h1>
<ul>
<li><p>Sleep:</p>
<ul>
<li><a href="./Melatonin" class="docMetadata" data-popup-title="Melatonin" data-popup-author="Gwern Branwen" data-popup-date="19 Dec 2008" data-popup-abstract="<p>I discuss melatoninâs effects on sleep &amp;amp; its safety with research up to 2015; I segue into the general benefits of sleep and the severely disrupted sleep of the modern Western world, the cost of melatonin use and the benefit (eg. enforcing regular bedtimes), followed by a basic cost-benefit analysis of melatonin concluding that the net profit is large enough to be worth giving it a try barring unusual conditions or very pessimistic safety estimates.</p>">Melatonin</a></li>
<li><a href="./Modafinil" class="docMetadata" data-popup-title="Modafinil" data-popup-author="Gwern Branwen" data-popup-date="20 Feb 2009" data-popup-abstract="Modafinil is a prescription stimulant drug. I discuss informally, from a cost-benefit-informed perspective, the research up to 2015 on modafinil's cognitive effects, the risks of side-effects and addiction/tolerance and law enforcement, and give a table of current grey-market suppliers and discuss how to order from them.">Modafinil</a></li>
<li><a href="./Zeo" class="docMetadata" data-popup-title="Zeo sleep self-experiments" data-popup-author="Gwern Branwen" data-popup-date="28 Dec 2010" data-popup-abstract="<p>I discuss my beliefs about Quantified Self, and demonstrate with a series of <a href=&quot;https://en.wikipedia.org/wiki/single-subject_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Single-subject design&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In design of experiments, &amp;lt;b&amp;gt;single-subject design&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;single-case research design&amp;lt;/b&amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: single-subject design&quot;>single-subject design</a> self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:</p><ol type=&quot;1&quot;><li>the Zeo headband is wearable long-term</li><li><a href=&quot;#melatonin&quot;>melatonin</a> improves my sleep</li><li><a href=&quot;#one-legged-standing&quot;>one-legged standing</a> does little</li><li>Vitamin D <a href=&quot;#vitamin-d&quot;>at night</a> damages my sleep &amp;amp; Vitamin D in morning does not affect my sleep</li><li>potassium (<a href=&quot;#potassium-day-use&quot;>over the day</a> but not so much <a href=&quot;#potassium-morning-use&quot;>the morning</a>) damages my sleep and does not improve my mood/productivity</li><li>small quantities of <a href=&quot;#alcohol&quot;>alcohol</a> appear to make little difference to my sleep quality</li><li>I may be better off <a href=&quot;#timing&quot;>changing my sleep timing</a> by waking up somewhat earlier &amp;amp; going to bed somewhat earlier</li><li><a href=&quot;#lithium&quot;>lithium orotate</a> does not affect my sleep</li><li><a href=&quot;#redshiftf.lux&quot;>Redshift</a> causes me to go to bed earlier</li><li><a href=&quot;#zma&quot;>ZMA</a>: inconclusive results slightly suggestive of benefits</li></ol>">Sleep self-experiments</a>:
<ul>
<li><a href="./zeo/Caffeine" class="docMetadata" data-popup-title="Caffeine wakeup experiment" data-popup-author="Gwern Branwen" data-popup-date="7 April 2013" data-popup-abstract="<p>One trick to combat morning sluggishness is to get caffeine extra-early by using caffeine pills shortly before or upon trying to get up. From 2013-2014 I ran a blinded &amp;amp; placebo-controlled randomized experiment measuring the effect of caffeine pills in the morning upon awakening time and daily productivity. The estimated effect is small and the posterior probability relatively low, but a decision analysis suggests that since caffeine pills are so cheap, it would be worthwhile to conduct another experiment; however, increasing Zeo equipment problems have made me hold off additional experiments indefinitely.</p>">Caffeine</a></li>
<li><a href="./zeo/Potassium" class="docMetadata" data-popup-title="Potassium sleep experiments" data-popup-author="Gwern Branwen" data-popup-date="21 December 2012" data-popup-abstract="<p>Potassium and magnesium are minerals that many Americans are deficient in. I tried using potassium citrate and immediately noticed difficulty sleeping. A short randomized (but not blinded) self-experiment of ~4g potassium taken throughout the day confirmed large negative effects on my sleep. A longer followup randomized and blinded self-experiment used standardized doses taken once a day early in the morning, and also found some harm to sleep, and I discontinued potassium use entirely.</p>">Potassium</a></li>
<li><a href="./zeo/Redshift" class="docMetadata" data-popup-title="Redshift sleep experiment" data-popup-author="Gwern Branwen" data-popup-date="9 May 2012" data-popup-abstract="<p>I ran a randomized experiment with a free program (Redshift) which reddens screens at night to avoid tampering with melatonin secretion &amp;amp; the sleep from 2012-2013, measuring sleep changes with my Zeo. With 533 days of data, the main result is that Redshift causes me to go to sleep half an hour earlier but otherwise does not improve sleep quality.</p>">Redshift/f.lux</a></li>
<li><a href="./zeo/Vitamin-D" class="docMetadata" data-popup-title="Vitamin D sleep experiments" data-popup-author="Gwern Branwen" data-popup-date="1 January 2012" data-popup-abstract="<p>Vitamin D is a hormone endogenously created by exposure to sunlight; due to historically low outdoors activity levels, it has become a popular supplement and I use it. Some anecdotes suggest that vitamin D may have circadian and zeitgeber effects due to its origin, and is harmful to sleep when taken at night. I ran a blinded randomized self-experiment on taking vitamin D pills at bedtime. The vitamin D damaged my sleep and especially how rested I felt upon wakening, suggesting vitamin D did have a stimulating effect which obstructed sleep. I conducted a followup blinded randomized self-experiment on the logical next question: if vitamin D is a daytime cue, then would vitamin D taken in the morning show some beneficial effects? The results were inconclusive (but slightly in favor of benefits). Given the asymmetry, I suggest that vitamin D supplements should be taken only in the morning.</p>">Vitamin D</a></li>
<li><a href="./zeo/ZMA" class="docMetadata" data-popup-title="ZMA Sleep Experiment" data-popup-author="Gwern Branwen" data-popup-date="13 March 2017" data-popup-abstract="<p>I ran a blinded randomized self-experiment of 2.5g nightly ZMA powder effect on Zeo-recorded sleep data during March-October 2017 (<em>n</em>=127). The linear model and SEM model show no statistically-significant effects or high posterior probability of benefits, although all point-estimates were in the direction of benefits. Data quality issues reduced the available dataset, rendering the experiment particularly underpowered and the results more inconclusive. I decided to not continue use of ZMA after running out; ZMA may help my sleep but I need to improve data quality before attempting any further sleep self-experiments on it.</p>"><span class="smallcaps-auto">ZMA</span></a></li>
</ul></li>
<li><a href="./Wooden-pillows" class="docMetadata" data-popup-title="Wooden Pillow" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="China &amp; Egypt used wooden pillows; my recreations fail">Wooden pillows</a></li>
<li><a href="./Lunar-sleep" class="docMetadata" data-popup-title="Lunar circadian rhythms" data-popup-author="Gwern Branwen" data-popup-date="26 July 2013" data-popup-abstract="<p>I attempt to replicate, using public <a href=&quot;./Zeo&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Zeo sleep self-experiments&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;28 Dec 2010&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I discuss my beliefs about Quantified Self, and demonstrate with a series of &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/single-subject_design&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Single-subject design&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In design of experiments, &amp;amp;lt;b&amp;amp;gt;single-subject design&amp;amp;lt;/b&amp;amp;gt; or &amp;amp;lt;b&amp;amp;gt;single-case research design&amp;amp;lt;/b&amp;amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Wikipedia: single-subject design&amp;quot;&amp;gt;single-subject design&amp;lt;/a&amp;gt; self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:&amp;lt;/p&amp;gt;&amp;lt;ol type=&amp;quot;1&amp;quot;&amp;gt;&amp;lt;li&amp;gt;the Zeo headband is wearable long-term&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#melatonin&amp;quot;&amp;gt;melatonin&amp;lt;/a&amp;gt; improves my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#one-legged-standing&amp;quot;&amp;gt;one-legged standing&amp;lt;/a&amp;gt; does little&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Vitamin D &amp;lt;a href=&amp;quot;#vitamin-d&amp;quot;&amp;gt;at night&amp;lt;/a&amp;gt; damages my sleep &amp;amp;amp; Vitamin D in morning does not affect my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;potassium (&amp;lt;a href=&amp;quot;#potassium-day-use&amp;quot;&amp;gt;over the day&amp;lt;/a&amp;gt; but not so much &amp;lt;a href=&amp;quot;#potassium-morning-use&amp;quot;&amp;gt;the morning&amp;lt;/a&amp;gt;) damages my sleep and does not improve my mood/productivity&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;small quantities of &amp;lt;a href=&amp;quot;#alcohol&amp;quot;&amp;gt;alcohol&amp;lt;/a&amp;gt; appear to make little difference to my sleep quality&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;I may be better off &amp;lt;a href=&amp;quot;#timing&amp;quot;&amp;gt;changing my sleep timing&amp;lt;/a&amp;gt; by waking up somewhat earlier &amp;amp;amp; going to bed somewhat earlier&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#lithium&amp;quot;&amp;gt;lithium orotate&amp;lt;/a&amp;gt; does not affect my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#redshiftf.lux&amp;quot;&amp;gt;Redshift&amp;lt;/a&amp;gt; causes me to go to bed earlier&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#zma&amp;quot;&amp;gt;ZMA&amp;lt;/a&amp;gt;: inconclusive results slightly suggestive of benefits&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&quot;>Zeo</a>-recorded sleep datasets, a finding of a monthly circadian rhythm affecting sleep in a small sleep lab. I find only small non-statistically-significant correlations, despite being well-powered.</p>">Lunar circadian rhythms</a></li>
</ul></li>
<li><p><a href="./Nootropics" class="docMetadata" data-popup-title="Nootropics" data-popup-author="Gwern Branwen" data-popup-date="02 Jan 2010" data-popup-abstract="Notes on nootropics I tried, and my experiments">Nootropics experiments</a></p></li>
<li><p><a href="./LSD-microdosing" class="docMetadata" data-popup-title="LSD microdosing RCT" data-popup-author="Gwern Branwen" data-popup-date="20 Aug 2012" data-popup-abstract="<p>Some early experimental studies with LSD suggested that doses of LSD too small to cause any noticeable effects may improve mood and creativity. Prompted by recent discussion of this claim and the purely anecdotal subsequent evidence for it, I decided to run a well-powered randomized blind trial of 3-day LSD microdoses from September 2012 to March 2013. No beneficial effects reached statistical-significance and there were worrisome negative trends. LSD microdosing did not help me.</p>"><span class="smallcaps-auto">LSD</span> microdosing</a></p></li>
<li><p><a href="./Creatine" class="docMetadata" data-popup-title="Creatine Cognition Meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="6 Sep 2013" data-popup-abstract="<p>I attempt to meta-analyze conflicting studies about the cognitive benefits of creatine supplementation. The wide variety of psychological measures by uniformly small studies hampers any aggregation. 3 studies measured IQ and turn in a positive result, but suggestive of vegetarianism causing half the benefit. Discussions indicate that publication bias is at work. Given the variety of measures, small sample sizes, publication bias, possible moderators, and small-study biases, any future creatine studies should use the most standard measures of cognitive function like RAPM in a reasonably large pre-registered experiment.</p>">Creatine/cognition meta-analysis</a></p></li>
<li><p><a href="./Lithium" class="docMetadata" data-popup-title="Lithium in ground-water and well-being" data-popup-author="Gwern Branwen" data-popup-date="14 October 2010" data-popup-abstract="<p>The metal lithium is a well-known mood stabilizer &amp;amp; suicide preventive widely used in psychiatry. It is also a trace mineral present to various levels in all drinking water and much food. A long-running but obscure vein of research speculates on whether lithium is beneficial and a nutrient, specifically, cognitively-protective. Epidemiological research has correlated chronic lithium consumption through drinking water with a number of population-level variables like rates of mental illness, violence, &amp;amp; suicide. If causal, lithium should be regarded as a vital nutrient for mental health and added to drinking water to substantially improve population-wide outcomes.</p><p>However, the evidence is weak. Most of this research is cross-sectional, only some is longitudinal, none offers particularly strong causal evidence using natural experiments or other designs, there are questions about confounding with autocorrelated spatial properties such as altitude, and some of the best research, using Scandinavian population registries, offers more mixed evaluations of claimed correlates.</p><p>It is unlikely that further such correlational research will resolve the debate, despite the mounting opportunity cost. I suggest that formal experimentation is required, and concerns about harms from lithium supplementation making experiments âunethicalâ can be circumvented by instead <em>removing</em> lithium or looking for natural experiments with cause changes (such as changes or upgrades to water treatment plants or plumbing modify lithium concentration).</p>">Lithium in ground-water review</a></p></li>
<li><p><a href="./2014-spirulina" class="docMetadata" data-popup-title="2014 Spirulina randomized self-experiment" data-popup-author="Gwern Branwen" data-popup-date="5 Oct 2014" data-popup-abstract="<p>The supplement Spirulina has been suggested to help allergy symptoms. A randomized self-experiment is run by sceaduwe April - August 2014. Analysis suggests no effect of the spirulina.</p>">Spirulina allergy experiment</a></p></li>
<li><p><a href="./Melon" class="docMetadata" data-popup-title="Bitter Melon for blood glucose" data-popup-author="Gwern Branwen" data-popup-date="14 Sep 2015" data-popup-abstract="<p>I re-analyze a bitter-melon/blood-glucose self-experiment, finding a small effect of increasing blood glucose after correcting for temporal trends &amp;amp; daily variation, giving both frequentist &amp;amp; Bayesian analyses. I then analyze the self-experiment from a subjective Bayesian decision-theoretic perspective, cursorily estimating the costs of diabetes &amp;amp; benefits of intervention in order to estimate Value Of Information for the self-experiment and the benefit of further self-experimenting; I find that the expected value of more data (EVSI) is negative and further self-experimenting would not be optimal compared to trying out other anti-diabetes interventions.</p>">Bitter melon blood-glucose experiment</a></p></li>
<li><p><a href="./Lewis-meditation" class="docMetadata" data-popup-title="2013 Lewis meditation results" data-popup-author="Gwern Branwen" data-popup-date="12 July 2013" data-popup-abstract="<p>A small group of Quantified Selfers tested themselves daily on arithmetic and engaged in a month of meditation. I analyze their scores with a multilevel model with per-subject grouping, and find the expect result: a small decrease in arithmetic errors which is not statistically-significant, with practice &amp;amp; time-of-day effects (but not day-of-week or weekend effects). This suggests a longer experiment by twice as many experimenters in order to detect this effect.</p>">Meditation &amp; math errors</a></p></li>
<li><p><a href="./Weather" class="docMetadata" data-popup-title="Weather and My Productivity" data-popup-author="Gwern Branwen" data-popup-date="19 Mar 2013" data-popup-abstract="<p>Weather is often said to affect our mood, and that people in sunnier places are happier <em>because</em> of that. Curious about the possible effect (it could be worth controlling for in my future QS analyses or attempting to imitate benefits inside my house eg brighter lighting), I combine my long-term daily self-ratings with logs from the nearest major official weather stations, which offer detailed weather information about temperature, humidity, precipitation, cloud cover, wind speed, brightness etc, and try to correlate them.</p><p>In general, despite considerable data, there are essentially no bivariate correlations, nothing in several versions of a linear model, and nothing found by a random forest. It would appear that weather does not correlate with my self-ratings to any detectable degree, much less cause it.</p>">Weather/mood</a></p></li>
<li><p><a href="./Treadmill" class="docMetadata" data-popup-title="Treadmill desk observations" data-popup-author="Gwern Branwen" data-popup-date="19 June 2012" data-popup-abstract="Notes relating to my use of a treadmill desk and 2 self-experiments showing walking treadmill use interferes with typing and memory performance.">Treadmill memory/typing experiment</a></p></li>
<li><p><a href="./Bacopa" class="docMetadata" data-popup-title="Bacopa Quasi-Experiment" data-popup-author="Gwern Branwen" data-popup-date="6 May 2014" data-popup-abstract="<p>Bacopa is a supplement herb often used for memory or stress adaptation. Its chronic effects reportedly take many weeks to manifest, with no important acute effects. Out of curiosity, I bought 2 bottles of Bacognize Bacopa pills and ran a non-randomized non-blinded ABABA quasi-self-experiment from June 2014 to September 2015, measuring effects on my memory performance, sleep, and daily self-ratings of mood/productivity. For analysis, a multi-level Bayesian model on two memory performance variables was used to extract per-day performance, factor analysis was used to extract a sleep index from 9 Zeo sleep variables, and the 3 endpoints were modeled as a multivariate Bayesian time-series regression with splines. Because of the slow onset of chronic effects, small effective sample size, definite temporal trends probably unrelated to Bacopa, and noise in the variables, the results were as expected, ambiguous, and do not strongly support any correlation between Bacopa and memory/sleep/self-rating (+/-/- respectively).</p>">Bacopa quasi-experiment</a></p></li>
</ul>
</section>
<section id="practical" class="level1">
<h1><a href="#practical" title="Link to section: 'Practical'">Practical</a></h1>
<ul>
<li><a href="./Complement" class="docMetadata" data-popup-title="Laws of Tech: Commoditize Your Complement" data-popup-author="Gwern Branwen" data-popup-date="17 March 2018" data-popup-abstract="<p>Joel Spolsky in 2002 identified a major pattern in technology business &amp;amp; economics: the pattern of <q>âcommoditizing your complementâ</q>, an alternative to vertical integration, where companies seek to secure a chokepoint or quasi-monopoly in products composed of many necessary &amp;amp; sufficient layers by dominating one layer while fostering so much competition in another layer above or below its layer that no competing monopolist can emerge, prices are driven down to marginal costs elsewhere in the stack, total price drops &amp;amp; increases demand, and the majority of the consumer surplus of the final product can be diverted to the quasi-monopolist. A classic example is the commodification of PC hardware by the Microsoft OS monopoly, to the detriment of IBM &amp;amp; benefit of MS.</p><p>This pattern explains many otherwise odd or apparently self-sabotaging ventures by large tech companies into apparently irrelevant fields, such as the high rate of releasing open-source contributions by many Internet companies or the intrusion of advertising companies into smartphone manufacturing &amp;amp; web browser development &amp;amp; statistical software &amp;amp; fiber-optic networks &amp;amp; municipal WiFi &amp;amp; radio spectrum auctions &amp;amp; DNS (Google): they are pre-emptive attempts to commodify another company elsewhere in the stack, or defenses against it being done to them.</p>">Commoditize Your Complement</a></li>
<li><a href="./Drug-heuristics" class="docMetadata" data-popup-title="The Algernon Argument" data-popup-author="Gwern Branwen" data-popup-date="23 Mar 2010" data-popup-abstract="Why most supplements fail: IQ improvement skepticism, Yudkowsky &amp; Bostrom's heuristics, nootropics">Evolutionary Heuristics for Drugs</a></li>
<li><a href="./Search" class="docMetadata" data-popup-title="Internet Search Tips" data-popup-author="Gwern Branwen" data-popup-date="11 Dec 2018" data-popup-abstract="<p>Over time, I developed a certain google-fu and expertise in finding references, papers, and books online. Some of these tricks are not well-known, like checking the Internet Archive (IA) for books. I try to write down my search workflow, and give general advice about finding and hosting documents.</p>">Internet Search Tips</a></li>
<li><a href="./Timing" class="docMetadata" data-popup-title="Timing Technology: Lessons From The Media Lab" data-popup-author="Gwern Branwen" data-popup-date="12 July 2012" data-popup-abstract="<p>Technological forecasts are often surprisingly prescient in terms of predicting that something was possible &amp;amp; desirable and what they predict eventually happens; but they are far less successful at predicting the timing, and almost always fail, with the success (and riches) going to another.</p><p>Why is their knowledge so useless? The right moment cannot be known exactly in advance, so attempts to forecast will typically be off by years or worse. For many claims, there is no way to invest in an idea except by going all in and launching a company, resulting in extreme variance in outcomes, even when the idea is good and the forecasts correct about the (eventual) outcome.</p><p>Progress can happen and can be foreseen long before, but the details and exact timing due to bottlenecks are too difficult to get right. Launching too early means failure, but being conservative &amp;amp; launching later is just as bad because regardless of forecasting, a good idea will draw overly-optimistic researchers or entrepreneurs to it like <a href=&quot;https://en.wikipedia.org/wiki/Winner%27s_curse&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Winner&amp;#39;s curse&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;winner&amp;#39;s curse&amp;lt;/b&amp;gt; is a phenomenon that may occur in common value auctions, where all bidders have the same value for an item but receive different private signals about this value and wherein the winner is the bidder with the most optimistic evaluation of the asset and therefore will tend to overestimate and overpay. Accordingly, the winner will be &amp;quot;cursed&amp;quot; in one of two ways: either the winning bid will exceed the value of the auctioned asset making the winner worse off in absolute terms, or the value of the asset will be less than the bidder anticipated, so the bidder may garner a net gain but will be worse off than anticipated. However, an actual overpayment will generally occur only if the winner fails to account for the winner&amp;#39;s curse when bidding.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Winner&amp;#39;s curse&quot;>moths to a flame</a>: all get immolated but the one with the dumb luck to kiss the flame at the perfect instant, who then wins everything, at which point everyone can see that the optimal time is past. All major success stories overshadow their long list of predecessors who did the same thing, but got unlucky. So, ideas can be divided into the overly-optimistic &amp;amp; likely doomed, or the <em>fait accompli</em>. On an individual level, ideas are worthless because so many others have them tooââmultiple inventionâ is the rule, and not the exception.</p><p>This overall problem falls under the reinforcement learning paradigm, and successful approaches are analogous to Thompson sampling/posterior sampling: even an informed strategy canât reliably beat random exploration which gradually shifts towards successful areas while continuing to take occasional long shots. Since people tend to systematically over-exploit, how is this implemented? Apparently by individuals acting suboptimally on the personal level, but optimally on societal level by serving as random exploration.</p><p>A major benefit of R&amp;amp;D, then, is in laying fallow until the âripe timeâ when they can be immediately exploited in previously-unpredictable ways; applied R&amp;amp;D or VC strategies should focus on maintaining diversity of investments, while continuing to flexibly revisit previous failures which forecasts indicate may have reached âripe timeâ. This balances overall exploitation &amp;amp; exploration to progress as fast as possible, showing the usefulness of technological forecasting on a global level despite its uselessness to individuals.</p>">Timing Technology: Media Lab Lessons</a></li>
<li><a href="./Girl-Scouts-and-good-governance" class="docMetadata" data-popup-title="Girl Scouts &amp; Good Corporate Governance" data-popup-author="Gwern Branwen" data-popup-date="21 Apr 2011" data-popup-abstract="Cookie prices &amp; tax filings as evidence of corporate inefficiency">Girl Scouts &amp; Good Governance</a></li>
<li><a href="./plastination" class="docMetadata" data-popup-title="Plastination versus Cryonics" data-popup-author="Gwern Branwen" data-popup-date="24 Jul 2011" data-popup-abstract="Break down survival as Drake equation, see how plastination differs from cryonics, try to calculate advantage">Plastination vs Cryonics</a></li>
<li><a href="./Charity-is-not-about-helping" class="docMetadata" data-popup-title="Charity is not about Helping" data-popup-author="Gwern Branwen" data-popup-date="15 Sep 2011" data-popup-abstract="Simple cost-benefit: distributed computing considered harmful as scientific lemon projects run at high resource cost.">Charity is not about helping</a></li>
<li><a href="./Console-Insurance" class="docMetadata" data-popup-title="Console Insurance Is A Ripoff" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2009" data-popup-abstract="Back of envelope financial calculations: Warranties, fine; insurance, no!">Console Insurance</a></li>
<li><a href="./Life-contract" class="docMetadata" data-popup-title="Life contracts" data-popup-author="Gwern Branwen" data-popup-date="02 Nov 2009" data-popup-abstract="How I reinvented longevity insurance, which provides payouts if one lives to a certain point and thus might run out of savings.">Life contracts</a></li>
<li><a href="./Red" class="docMetadata" data-popup-title="Rubrication Design Examples" data-popup-author="Gwern Branwen" data-popup-date="30 May 2019" data-popup-abstract="<p>Dating back to medieval manuscripts, text has often been highlighted using a particular distinct bright red. The contrast of black and red on a white background is highly visible and striking, and this has been reused many times, in a way which I have not noticed for other colors. I call these uses <em>rubrication</em> and collate examples I have noticed from many time periods.</p><p>Why this design pattern? Why red, specifically, and not, say, orange or purple? Is it just a historical accident? Cross-cultural research suggests that for humans, red may be intrinsically more noticeable &amp; has a higher contrast with black, explaining its perennial appeal as a design pattern.</p><p>Regardless, it is a beautiful design pattern which has been used in many interesting ways over the millennia, and perhaps may inspire the reader.</p>">Rubrication Design Examples</a></li>
<li><a href="./Improvements" class="docMetadata" data-popup-title="My Ordinary Life: Improvements Since the 1990s" data-popup-author="Gwern Branwen" data-popup-date="2019-05-30" data-popup-abstract="It can be hard to see the gradual improvement of most goods over time, but I think one way to get a handle on them is to look at their <em>downstream</em> effects: all the small ordinary everyday things which nevertheless depend on obscure innovations and improving cost-performance ratios and gradually dropping costs and new material and... etc. All of these gradually drop the cost, drop the price, improve the quality at the same price, remove irritations or limits not explicitly noticed, or so on. It all adds up. So here is a personal list of some of the small ways in which my ordinary everyday daily life has been getting better since the late '80s/early '90s (as far back as I can clearly remember these things---I am sure the list of someone growing up in the 1940s would include many hassles I've never known at all).">Ordinary Life Improvements</a></li>
</ul>
</section>
<section id="politics" class="level1">
<h1><a href="#politics" title="Link to section: 'Politics'">Politics</a></h1>
<ul>
<li><a href="./The-Melancholy-of-Subculture-Society" class="docMetadata" data-popup-title="The Melancholy of Subculture Society" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2009" data-popup-abstract="Internet links small groups, helping dissolve big groups; good, bad? But a bit sad.">The Melancholy of Subculture Society</a></li>
<li><a href="./Terrorism-is-not-about-Terror" class="docMetadata" data-popup-title="Terrorism Is Not About Terror" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="<p>Statistical analysis of terrorist groupsâ longevity, aims, methods and successes reveal that groups are self-contradictory and self-sabotaging, generally ineffective; common stereotypes like terrorists being poor or ultra-skilled are false. Superficially appealing counter-examples are discussed and rejected. Data on motivations and the dissolution of terrorist groups are brought into play and the surprising conclusion reached: terrorism is a form of socialization or status-seeking.</p>">Terrorism is not about Terror</a></li>
<li><a href="./Terrorism-is-not-Effective" class="docMetadata" data-popup-title="Terrorism Is Not Effective" data-popup-author="Gwern Branwen" data-popup-date="14 Apr 2009" data-popup-abstract="<p>Terrorism is not about causing terror or casualties, but about other things. Evidence of this is the fact that, despite often considerable resources spent, most terrorists are incompetent, impulsive, prepare poorly for attacks, are inconsistent in planning, tend towards exotic &amp;amp; difficult forms of attack such as bombings, and in practice ineffective: the modal number of casualties per terrorist attack is near-zero, and global terrorist annual casualty have been a rounding error for decades. This is despite the fact that there are many examples of extremely destructive easily-performed potential acts of terrorism, such as poisoning food supplies or renting large trucks &amp;amp; running crowds over or engaging in sporadic sniper attacks.</p>">Terrorism is not Effective</a></li>
<li><a href="./Slowing-Moores-Law" class="docMetadata" data-popup-title="Slowing Moore's Law: How It Could Happen" data-popup-author="Gwern Branwen" data-popup-date="16 Mar 2012" data-popup-abstract="<p><a href=&quot;https://en.wikipedia.org/wiki/Brain_emulation&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Mind uploading&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Whole brain emulation&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;WBE&amp;lt;/b&amp;gt;), &amp;lt;b&amp;gt;mind upload&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;brain upload&amp;lt;/b&amp;gt; is the hypothetical futuristic process of scanning the mental state of a particular brain substrate and copying it to a computer. The computer could then run a simulation model of the brain&amp;#39;s information processing, such that it would respond in essentially the same way as the original brain and experience having a conscious mind.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Brain emulation&quot;>Brain emulation</a> requires enormous computing power; enormous computing power requires further progression of <a href=&quot;https://en.wikipedia.org/wiki/Moore%27s_law&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Moore&amp;#39;s law&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Moore&amp;#39;s law&amp;lt;/b&amp;gt; is the observation that the number of transistors in a dense integrated circuit doubles about every two years. The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and CEO of Intel, whose 1965 paper described a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41.4%.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Mooreâs law&quot;>Mooreâs law</a>; further Mooreâs law relies on large-scale production of cheap processors in ever more-advanced <a href=&quot;https://en.wikipedia.org/wiki/chip_fabs&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Semiconductor fabrication plant&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In the microelectronics industry, a &amp;lt;b&amp;gt;semiconductor fabrication plant&amp;lt;/b&amp;gt; is a factory where devices such as integrated circuits are manufactured.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: chip fabs&quot;>chip fabs</a>; cutting-edge chip fabs are both expensive and vulnerable to state actors (but <em>not</em> non-state actors <a href=&quot;#state-actors-why-not-terrorism&quot;>such as terrorists</a>). Therefore: the advent of brain emulation can be delayed by global regulation of chip fabs.</p>">Mooreâs law &amp; chip fabs</a></li>
<li><a href="./Littlewood" class="docMetadata" data-popup-title="Littlewood's Law and the Global Media" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2018" data-popup-abstract="<p>Online &amp;amp; mainstream media and social networking have become increasingly misleading as to the state of the world by focusing on âstoresâ and âeventsâ rather than trends and averages. This is because as the global population increases and the scope of media increases, mediaâs urge for narrative focuses on the most extreme outlier datapointsâbut such datapoints are, at a global scale, deeply misleading as they are driven by unusual processes such as the mentally ill or hoaxers.</p><p>At a global scale, anything that can happen will happen a small but nonzero times: this has been epitomized as <q>âLittlewoodâs Law: in the course of any normal personâs life, miracles happen at a rate of roughly one per month.â</q> Hence, there will be enough âmiraclesâ that all media coverage of events can potentially be composed of nothing but extreme chance chances, even though it would seem like an âextraordinaryâ claim to say that all media-reported events may be flukes.</p><p>Given this, it is important to maintain extreme skepticism of any individual anecdotes or stories which are selectively reported but still claimed (often implicitly) to be representative of a general trend or fact about the world. Standard techniques like critical thinking, emphasizing trends &amp;amp; averages, and demanding original sources can help fight the biasing effect of news.</p>">Littlewood's Law &amp; the Global Media</a></li>
<li><a href="./Colder-Wars" class="docMetadata" data-popup-title="Colder Wars" data-popup-author="Gwern Branwen" data-popup-date="07 Jun 2009" data-popup-abstract="MAD will not work in outer space; pre-emptive strikes are nigh-guaranteed.">Colder Wars</a></li>
</ul>
</section>
<section id="philosophy" class="level1">
<h1><a href="#philosophy" title="Link to section: 'Philosophy'">Philosophy</a></h1>
<ul>
<li><a href="./The-Existential-Risk-of-Mathematical-Error" class="docMetadata" data-popup-title="The Existential Risk of Math Errors" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Mathematical mistake/error-rates limit our understanding of rare risks and ability to defend against them">Existential Risks &amp; Math Errors</a></li>
<li><a href="./Language" class="docMetadata" data-popup-title="On the Existence of Powerful Natural Languages" data-popup-author="Gwern Branwen" data-popup-date="18 Dec 2016" data-popup-abstract="<p>Designed formal notations &amp;amp; distinct vocabularies are often employed in STEM fields, and these specialized languages are credited with greatly enhancing research &amp;amp; communication. Many philosophers and other thinkers have attempted to create more generally-applicable designed languages for use outside of specific technical fields to enhance human thinking, but the empirical track record is poor and no such designed language has demonstrated substantial improvements to human cognition such as resisting cognitive biases or logical fallacies. I suggest that the success of specialized languages in fields is inherently due to encoding large amounts of previously-discovered information specific to those fields, and this explains their inability to boost human cognition across a wide variety of domains.</p>">On Powerful Natural Languages</a></li>
<li><a href="./Culture-is-not-about-Esthetics" class="docMetadata" data-popup-title="Culture Is Not About Esthetics" data-popup-author="Gwern Branwen" data-popup-date="16 Jul 2009" data-popup-abstract="Aesthetically &amp; economically, maybe there is too much new art. Don't take this too seriously.">Culture is not about Esthetics</a></li>
<li><a href="./The-Narrowing-Circle" class="docMetadata" data-popup-title="The Narrowing Circle" data-popup-author="Gwern Branwen" data-popup-date="24 Apr 2012" data-popup-abstract="<p>The <q>âexpanding circleâ</q> historical thesis ignores all instances in which modern ethics <em>narrowed</em> the set of beings to be morally regarded, often backing its exclusion by asserting their non-existence, and thus assumes its conclusion: where the circle is expanded, itâs highlighted as moral âprogressâ, and where it is narrowed, what is outside is simply defined away. When one compares modern with ancient society, the religious differences are striking: almost every single supernatural entity (place, personage, or force) has been excluded from the circle of moral concern, where they used to be huge parts of the circle and one could almost say the entire circle. Further examples include estates, houses, fetuses, prisoners, and graves.</p>">The Narrowing Circle</a></li>
<li><a href="./Modus" class="docMetadata" data-popup-title="One Man's Modus Ponens" data-popup-author="Gwern Branwen" data-popup-date="1 May 2012" data-popup-abstract="<p>A logically-valid argument which takes the form of a <a href=&quot;https://en.wikipedia.org/wiki/modus_ponens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus ponens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus ponens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a rule of inference. It can be summarized as &amp;quot;&amp;lt;i&amp;gt;P implies Q&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;P&amp;lt;/i&amp;gt; is asserted to be true, therefore &amp;lt;i&amp;gt;Q&amp;lt;/i&amp;gt; must be true.&amp;quot;&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus ponens&quot;>modus ponens</a> may be interpreted in several ways; a major one is to interpret it as a kind of <em><a href=&quot;https://en.wikipedia.org/wiki/reductio_ad_absurdum&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reductio ad absurdum&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In logic, &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;reductio ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, also known as &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;argumentum ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;apagogical arguments&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; or the &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;appeal to extremes&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;, is a form of argument that attempts either to disprove a statement by showing it inevitably leads to a ridiculous, absurd, or impractical conclusion, or to prove one by showing that if it were not true, the result would be absurd or impossible. Traced back to classical Greek philosophy in Aristotle&amp;#39;s &amp;lt;i&amp;gt;Prior Analytics&amp;lt;/i&amp;gt;, this technique has been used throughout history in both formal mathematical and philosophical reasoning, as well as in debate.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reductio ad absurdum&quot;>reductio ad absurdum</a></em>, where by âprovingâ a conclusion believed to be false, one might instead take it as a <a href=&quot;https://en.wikipedia.org/wiki/modus_tollens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus tollens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus tollens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contrapositive.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus tollens&quot;>modus tollens</a> which proves that one of the <em>premises</em> is false. This <q>âMoorean shiftâ</q> is aphorized as the snowclone, <q>âOne manâs modus ponens is another manâs modus tollensâ</q>. The Moorean shift is a powerful counter-argument which has been deployed against many skeptical &amp;amp; metaphysical claims in philosophy, where often the conclusion is extremely unlikely and little evidence can be provided for the premises used in the proofs; and it is relevant to many other debates, particularly methodological ones.</p>">One Man's Modus Ponens</a></li>
<li><a href="./Newton" class="docMetadata" data-popup-title="Newton's System of the World and Comets" data-popup-author="Gwern Branwen" data-popup-date="13 June 2016" data-popup-abstract="<p>Isaac Newton published few of his works, and only those he considered perfect after long delays. This leaves his system the world, as described in the <em>Principia</em> and elsewhere, incomplete, and many questions simply unaddressed, like the fate of the Sun or role of comets. But in 2 conversations with an admirer and his nephew, the elderly Newton sketched out the rest of his cosmogony.</p><p>According to Newton, the solar system is <em>not</em> stable and must be adjusted by angels; the Sun does not burn perpetually, but comets regularly fuel the Sun; and the final result is that humanity will be extinguished by a particularly large comet causing the sun to flare up, and requiring intelligent alien beings to arise on other planets or their moons. He further gives an anthropic argument: one reason we know that intelligent races regularly go extinct is that humanity itself arose only recently, as demonstrated by the recent innovations in every field, inconsistent with any belief that human beings have existed for hundreds of thousands or millions of years.</p><p>This is all interestingly wrong, particularly the anthropic argument. That Newton found it so absurd to imagine humanity existing for millions of years but only recently undergoing exponential improvements in technology demonstrates how counterintuitive and extraordinary the Industrial &amp;amp; Scientific Revolutions were.</p>">Newton's System of the World &amp; Comets</a></li>
<li><a href="./Subscripts" class="docMetadata" data-popup-title="Subscripts For Citations" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2020" data-popup-abstract="<p>I propose reviving an old General Semantics notation: borrow from scientific notation and use subscripts like âGwern<sub>2020</sub>â for denoting sources (like citation, timing, or medium). Using subscript indices is flexible, compact, universally technically supported, and intuitive. While (currently) unusual, subscripting might be a useful trick for clearer writing, compared to omitting such information or using standard cumbersome circumlocutions.</p>">On Subscripting Citations</a></li>
<li><a href="./Ontological-pantheism" class="docMetadata" data-popup-title="Ontological Pantheism" data-popup-author="Gwern Branwen" data-popup-date="09 Nov 2009" data-popup-abstract="Descartes's God is pantheism; a reductio ad absurdum of his ontology">Ontological pantheism</a></li>
<li><a href="./An-Abortion-Dialogue" class="docMetadata" data-popup-title="An Abortion Dialogue" data-popup-author="Gwern Branwen" data-popup-date="10 Nov 2008" data-popup-abstract="Dialogue pointing out some difficulties of materialist objections to abortion.">An Abortion Dialogue</a></li>
<li><a href="./On-Disrespect" class="docMetadata" data-popup-title="On Disrespect" data-popup-author="Gwern Branwen" data-popup-date="09 Feb 2009" data-popup-abstract="An attempt to reinvent classic theories of social interaction as expressions of power">On Disrespect</a></li>
<li><a href="./Justifications" class="docMetadata" data-popup-title="On Justifications" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Philosophical fiction or a prose poem about suffering innocents and the theodicy">Justifications</a></li>
<li><a href="./Against-The-Miletians" class="docMetadata" data-popup-title="Against the Miletians and the One True Element" data-popup-author="Gwern Branwen" data-popup-date="17 Oct 2008" data-popup-abstract="Exploring consequences of material monism and conflict with observations">Against The Miletians</a></li>
<li><a href="./Copyright" class="docMetadata" data-popup-title="Against Copyright" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Copyright considered paradoxical, incoherent, and harmful from an information theory and compression perspective as there is no natural kind corresponding to ">Copyright</a></li>
<li><a href="./Immoral-Books" class="docMetadata" data-popup-title="Immoral Books" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2010" data-popup-abstract="Argument that texts are neither moral nor immoral as they require active interpretation.">Immoral Books</a></li>
<li><a href="./Isomorphisms" class="docMetadata" data-popup-title="Isomorphisms" data-popup-author="Gwern Branwen" data-popup-date="25 Apr 2009" data-popup-abstract="Investigating when two theories or representations *mean* the same things.">Isomorphisms</a></li>
<li><a href="./Ethical-sperm-donation" class="docMetadata" data-popup-title="The Morality of Sperm Donation" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Is sperm donating a worthwhile form of positive eugenics?">Moral Sperm Donation</a></li>
<li><a href="./Zen-and-the-Art-of-Bicycle-Maintenance" class="docMetadata" data-popup-title="Zen and the Art of Bicycle Maintenance" data-popup-author="Gwern Branwen" data-popup-date="02 Feb 2009" data-popup-abstract="Mechanical parts teach you that small things matter.">Zen &amp; the Art of Bicycle Maintenance</a></li>
</ul>
</section>
<section id="fiction" class="level1">
<h1><a href="#fiction" title="Link to section: 'Fiction'">Fiction</a></h1>
<ul>
<li><p>Prose:</p>
<ul>
<li><a href="./fiction/The-Erl-King" class="docMetadata" data-popup-title="The Erl King" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Fairy tale tragedy, or, a lesson in courtesy and logic">âThe Erl Kingâ</a></li>
<li><a href="./fiction/The-Ones-Who-Walk-Towards-Acre" class="docMetadata" data-popup-title="The Ones Who Walk Towards Acre" data-popup-author="Gwern Branwen" data-popup-date="21 Dec 2010" data-popup-abstract="Short story on assassination markets.">âThe Ones Who Walk Towards Acreâ</a></li>
<li><a href="./fiction/Missing-Cities" class="docMetadata" data-popup-title="Missing Cities" data-popup-author="Gwern Branwen" data-popup-date="01 Feb 2009" data-popup-abstract="3 short stories in the style of Italo Calvino's 'Missing Cities'">âMissing Citiesâ</a></li>
<li><a href="./fiction/Men-of-Iron" class="docMetadata" data-popup-title="Men of Iron" data-popup-author="Gwern Branwen" data-popup-date="24 Dec 2012" data-popup-abstract="What-if Chiang-style SF story on iron vanishing and the Great Silence">âMen of Ironâ</a></li>
<li><a href="./fiction/Menard" class="docMetadata" data-popup-title="Gilles Goullet, Author of the Blindsight" data-popup-author="Gwern Branwen" data-popup-date="01 Sep 2010" data-popup-abstract="A parody of SF, the Internet, and Borges.">âGilles Goullet, Author of <em>Blindsight</em>â</a></li>
<li><a href="./fiction/How-the-Panther-got-Black" class="docMetadata" data-popup-title="How the Panther Got Black" data-popup-author="Gwern Branwen" data-popup-date="02 Feb 2009" data-popup-abstract="Mythic short story in the vein of Kipling's Just-so Stories">âHow the Panther got Blackâ</a></li>
<li><a href="./fiction/The-Palace-of-Wonders" class="docMetadata" data-popup-title="The Palace of Wonders" data-popup-author="Gwern Branwen" data-popup-date="23 May 2011" data-popup-abstract="A Borgesian fable about the Caliph and the Koran.">âThe Palace of Wondersâ</a></li>
<li><a href="./fiction/Batman" class="docMetadata" data-popup-title="The Gift of the Amygdali" data-popup-author="Gwern Branwen" data-popup-date="29 Oct 2017" data-popup-abstract="A high-concept 'Batman' short story in the style of a 1980s comic book script about the Scarecrow and the gifts no one appreciates: pain/guilt/fear/anxiety.">âThe Gift of the Amygdaliâ</a></li>
<li><a href="./fiction/Cloud-Nine" class="docMetadata" data-popup-title="Cloud Nine" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Unfinished fantasy/SF novel"><em>Cloud Nine</em></a></li>
<li><a href="./fiction/The-Last-Muezzin" class="docMetadata" data-popup-title="The Last Muezzin" data-popup-author="Gwern Branwen" data-popup-date="15 Jun 2011" data-popup-abstract="Another tribute to Borges; you can be me when I'm gone.">âThe Last Muezzinâ</a></li>
<li><a href="./fiction/Dinosaur-Comics" class="docMetadata" data-popup-title="_Dinosaur Comics_ comics" data-popup-author="Gwern Branwen" data-popup-date="10 Jun 2011" data-popup-abstract="Comics using the format of Ryan North's _Dinosaur Comics_"><em>Dinosaur Comics</em> comics</a></li>
<li><a href="./fiction/The-Buddhas-Wheel" class="docMetadata" data-popup-title="The Buddha's Wheel" data-popup-author="Gwern Branwen" data-popup-date="30 Nov 2008" data-popup-abstract="The enlightened is as one with cause and effect.">âThe Buddhaâs Wheelâ</a></li>
</ul></li>
<li><p>Verse:</p>
<ul>
<li><a href="./fiction/Poems" class="docMetadata" data-popup-title="Poems" data-popup-author="Gwern Branwen" data-popup-date="28 July 2011" data-popup-abstract="Miscellaneous waka/haiku, by season">Poems</a></li>
<li><a href="./fiction/Brave-poem" class="docMetadata" data-popup-title="Brave Poem" data-popup-author="Gwern Branwen" data-popup-date="13 Nov 2010" data-popup-abstract="Poem on memory inspired by the anime 'Angel Beats!'; do you remember love?">âBrave poemâ</a></li>
<li><a href="./fiction/Dying-Outside" class="docMetadata" data-popup-title="Dying Outside" data-popup-author="Gwern Branwen" data-popup-date="12 Dec 2009" data-popup-abstract="Poem about ALS">âDying Outsideâ</a></li>
<li><a href="./fiction/Genshiken" class="docMetadata" data-popup-title="Poems on the theme of 'Genshiken'" data-popup-author="Gwern Branwen" data-popup-date="24 Jul 2011" data-popup-abstract="Waka/haiku on Genshiken, clubs, and Comiket; allusions to Fujiwara no Teika and ShÅtetsu">Poems on the theme of <em>Genshiken</em></a></li>
<li><a href="./fiction/Safecracker" class="docMetadata" data-popup-title="Safecracker" data-popup-author="Gwern Branwen" data-popup-date="15 Aug 2009" data-popup-abstract="The thief of time gives us memories, the safecracker of hearts restores them">âSafecrackerâ</a></li>
<li><a href="./fiction/Hybrid-Rainbow" class="docMetadata" data-popup-title="Hybrid Rainbow" data-popup-author="Gwern Branwen" data-popup-date="01 Feb 2009" data-popup-abstract="Poem about man &amp; his machines.">âHybrid Rainbowâ</a></li>
</ul></li>
<li><p>Criticism:</p>
<ul>
<li><a href="./Story-Of-Your-Life" class="docMetadata" data-popup-title="'Story Of Your Life' Is Not A Time-Travel Story" data-popup-author="Gwern Branwen" data-popup-date="12 Dec 2012" data-popup-abstract="<p>One of Ted Chiangâs most noted philosophical SF short stories, âStory of Your Lifeâ, was made into a successful time-travel movie, <em>Arrival</em>, sparking interest in the original. However, movie viewers often misread the short story: âStoryâ is <em>not</em> a time-travel movie. At no point does the protagonist travel in time or enjoy precognitive powers, and interpreting the story this way leads to many serious plot holes and renders most of the exposition-heavy dialogue irrelevant.</p><p>Instead, Chiangâs story is about <em>psychology and physics</em>: classical physics allows usefully interpreting the laws of physics in both a âforwardâ way in which events happen step by step, but also a teleological way in which events are simply the unique optimal solution to a set of constraints including the final outcome and allows reasoning âbackwardsâ. The alien race exemplifies this other, equally valid, possible way of thinking and viewing the universe, and the protagonist learns their way of thinking by studying their language, which requires seeing written characters as a unified <em>gestalt</em>. This holistic view of the universe as an immutable âblock-universeâ in which events unfold as they must changes the protagonistâs attitude towards life and the tragic death of her daughter, teaching her in a somewhat Buddhist or Stoic fashion to embrace life in both its ups and downs.</p>">âStory Of Your Lifeâ Is Not Time-Travel</a></li>
<li><a href="./Dune-genetics" class="docMetadata" data-popup-title="Genetics and Eugenics in Frank Herbert's <em>Dune</em>" data-popup-author="Gwern Branwen" data-popup-date="5 May 2018" data-popup-abstract="<p>Frank Herbertâs SF <em>Dune</em> series features as a central mechanic a multi-millennium human eugenics breeding program by the Bene Gesserit, which produces the main character, Paul Atreides, with precognitive powers. The breeding program is described as oddly slow and ineffective and requiring roles for incest and inbreeding at some points, which contradict most proposed human eugenics methods. I describe the two main historical paradigms of complex trait genetics, the Fisherian infinitesimal model and the Mendelian monogenic model, the former of which is heavily used in human behavioral genetics and the latter of which is heavily used in agricultural breeding for novel traits, and argue that Herbert (incorrectly but understandably) believed the latter applied to most human traits, perhaps related to his longstanding autodidactic interest in plants &amp;amp; insects &amp;amp; farming, and this unstated but implicit intellectual background shaped <em>Dune</em> and resolves the anomalies.</p>">Genetics &amp; Eugenics in Herbert's <em>Dune</em></a></li>
<li><a href="./Suzanne-Delage" class="docMetadata" data-popup-title="Interpreting 'Suzanne Delage'" data-popup-author="Gwern Branwen" data-popup-date="23 Feb 2009" data-popup-abstract="The many interpretations of a Wolfe story which remains a mystery.">On Wolfeâs âSuzanne Delageâ</a></li>
<li><a href="./Scanners" class="docMetadata" data-popup-title="'Scanners Live in Vain' as realistic SF" data-popup-author="Gwern Branwen" data-popup-date="28 June 2013" data-popup-abstract="<p>Cordwainer Smithâs classic SF short story <q>âScanners Live in Vainâ</q> is remembered in part for its use of the space-madness trope, <q>âthe Great Pain of Spaceâ</q>, usually interpreted symbolically/psychologically by critics. I discuss the state of aerospace medicine in 1945 and subsequent research on <q>âthe breakaway effectâ</q>, <q>âthe overview effectâ</q>, and other unusual psychological states induced by air &amp;amp; space travel, and suggest Smithâs <q>âthe pain of spaceâ</q> is more founded on SF-style speculation &amp;amp; extrapolation of contemporary science/technology and anxieties than is appreciated due to the obscurity of the effects and the relative benignity of the subsequent best documented effects.</p>">âScanners Live in Vainâ as realistic SF</a></li>
<li><a href="./fiction/The-Mulberry-Tree" class="docMetadata" data-popup-title="The Mulberry Tree" data-popup-author="Gwern Branwen" data-popup-date="12 Oct 2010" data-popup-abstract="Essay on writing &amp; rewriting short tanka">The Mulberry Tree</a></li>
<li><a href="./fiction/The-Snowbanks-of-Time" class="docMetadata" data-popup-title="The Snowbanks of Time" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2011" data-popup-abstract="Essay on writing tanka on truth &amp; lies">The Snowbanks of Time</a></li>
</ul></li>
</ul>
</section>
<section id="anime" class="level1">
<h1><a href="#anime" title="Link to section: 'Anime'">Anime</a></h1>
<ul>
<li><a href="./The-Melancholy-of-Kyon" class="docMetadata" data-popup-title="The Melancholy of Kyon" data-popup-author="Gwern Branwen" data-popup-date="08 Jun 2009" data-popup-abstract="<p>The light novel series <em>The Melancholy of Haruhi Suzumiya</em>, featuring a character named Haruhi who is a god unawares and her search for novelty, has a number of anomalies and unclear overarching plot. I argue that these anomalies can be resolved, and greater literary depth achieved, by interpreting the first-person protagonist Kyon as the actual unaware god.</p>">The Melancholy of Kyon</a></li>
<li><em>Death Note</em>:
<ul>
<li><a href="./Death-Note-Anonymity" class="docMetadata" data-popup-title="Death Note: L, Anonymity &amp; Eluding Entropy" data-popup-author="Gwern Branwen" data-popup-date="04 May 2011" data-popup-abstract="<p>In the manga <em>Death Note</em>, the protagonist Light Yagami is given the supernatural weapon âDeath Noteâ which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. <em>Death Note</em> is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of Lâs process from the perspective of computer security, cryptography, and information theory, to quantify Lightâs initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest.</p><ol type=&quot;1&quot;><li>Mistake 1: Lightâs fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)</li><li>Mistake 2: Worse, the deaths are non-random in other waysâthey tend to occur at particular times! Just the scheduling of deaths cost Light 6 bits of anonymity</li><li>Mistake 3: Lightâs third mistake was reacting to the blatant provocation of Lind L. Tailor. L narrowed his target down to 1/3 the original Japanese population, for a gain of ~1.6 bits.</li><li>Mistake 4: Lightâs fourth mistake was to use confidential police information stolen using his policeman fatherâs credentials. This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!</li><li>Mistake 5: If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.</li><li>Endgame: At this point in the plot, L resorts to direct measures and enters Lightâs life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.</li></ol><p>Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.</p><p><em>(Note: This essay assumes a familiarity with the early plot of <em><a href=&quot;https://en.wikipedia.org/wiki/Death_Note&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note&quot;>Death Note</a></em> and <a href=&quot;https://en.wikipedia.org/wiki/Light_Yagami&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Light Yagami&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Light Yagami&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a fictional character and the main protagonist of the manga series &amp;lt;i&amp;gt;Death Note&amp;lt;/i&amp;gt;, created by Tsugumi Ohba and Takeshi Obata. He is portrayed as an accomplished yet bored teen genius who finds the Death Note, a supernatural notebook that allows the user to kill anyone by knowing their name and face, after it is dropped by the Shinigami Ryuk. In an effort to create a utopia, Light eventually uses the notebook to murder criminals as the vigilante &amp;lt;b&amp;gt;Kira&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; (ã­ã©)&amp;lt;/span&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Light Yagami&quot;>Light Yagami</a>; if you are unfamiliar with it, see my <a href=&quot;./Death-Note-Ending&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&amp;#39;s Ending&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;29 Sep 2008&quot; data-popup-abstract=&quot;Ambiguous ending means even the victor is unclear; who was right?&quot;><em>Death Note</em> Ending</a> essay or consult <a href=&quot;https://en.wikipedia.org/wiki/Death_Note#Plot_summary&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note#Plot_summary&quot;>Wikipedia</a> or <a href=&quot;http://deathnote.wikia.com/wiki/Rules_of_the_Death_Note&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Rules of the Death Note&quot;>read the DN rules</a>.)</em></p>">L, Anonymity &amp; Eluding Entropy</a></li>
<li><a href="./Death-Note-script" class="docMetadata" data-popup-title="Who wrote the 'Death Note' script?" data-popup-author="Gwern Branwen" data-popup-date="2 Nov 2009" data-popup-abstract="<p>I give a history of the 2009 leaked script, discuss internal &amp;amp; external evidence for its realness including stylometrics; and then give a simple step-by-step Bayesian analysis of each point. We finish with high confidence in the script being real, discussion of how this analysis was surprisingly enlightening, and what followup work the analysis suggests would be most valuable.</p>">Who Wrote The Movie?</a></li>
<li><a href="./Death-Note-Ending" class="docMetadata" data-popup-title="Death Note's Ending" data-popup-author="Gwern Branwen" data-popup-date="29 Sep 2008" data-popup-abstract="Ambiguous ending means even the victor is unclear; who was right?">On The Ending</a></li>
</ul></li>
<li><a href="./MLP" class="docMetadata" data-popup-title="MLP: Immanetizing The Equestrian" data-popup-author="Gwern Branwen" data-popup-date="24 Oct 2018" data-popup-abstract="<p>I watch the 2010 Western animated series <em>My Little Pony: Friendship is Magic</em> (seasons 1â8), delving deep into it and the MLP fandom, and reflect on it. What makes it good and powers its fandom subculture, producing a wide array of fanfictions, music, and art? Focusing on fandom, plot, development, and meaning of bronydom, I conclude that, among other things, it has surprisingly high-quality production &amp;amp; aesthetics which are easily adapted to fandom and which power a Westernized shonen animeâwhich depicts an underappreciated contemporary <em>capitalist</em> utopian perspective on self-actualization, reminiscent of other more explicitly self-help-oriented pop culture movements such as the recent Jordan B. Peterson movement. Included are my personal rankings of characters, seasons, episodes, and official &amp;amp; fan music.</p>"><em>My Little Pony</em>: Immanetizing The Equestrian</a></li>
<li><a href="./Hafu" class="docMetadata" data-popup-title="Hafu Gender Ratios in Anime" data-popup-author="Gwern Branwen" data-popup-date="06 Apr 2011" data-popup-abstract="Race as reflected in gender ratios within fictional bi-racial marriages in anime/manga show equal sex ratios and Western European overrepresentation with striking absence of Korean characters.">Hafu Gender Ratios in Anime</a></li>
<li><a href="./Arias-past-present-and-future" class="docMetadata" data-popup-title="Aria's Past, Present, and Future" data-popup-author="Gwern Branwen" data-popup-date="13 Jul 2011" data-popup-abstract="On divining the esoteric truth of Neo-Venezia through holes in world-building."><em>Aria</em>âs past, present, &amp; future</a></li>
<li><a href="./FMP-parody" class="docMetadata" data-popup-title="Parody in 'Full Metal Panic!'" data-popup-author="Gwern Branwen" data-popup-date="09 Oct 2008" data-popup-abstract="The unexpected critical depths of a single throw-away scene"><em><span class="smallcaps-auto">FMP</span></em> parody</a></li>
<li><a href="./komm-susser-tod" class="docMetadata" data-popup-title="Komm Susser Tod" data-popup-author="Gwern Branwen" data-popup-date="05 Nov 2010" data-popup-abstract="Perspective and interpretation">âKomm Susser Todâ</a></li>
</ul>
</section>
<section id="docs" class="level1">
<h1><a href="#docs" title="Link to section: 'Docs'">Docs</a></h1>
<ul>
<li><a href="./docs/radiance/2002-scholz-radiance" class="docMetadata" data-popup-title="Radiance: A Novel" data-popup-author="Carter Scholz, Gregory Benford, Hugh Gusterson, Sam Cohen, Curtis LeMay" data-popup-date="6 July 2013" data-popup-abstract="E-book edition of the 2002 Carter Scholz novel of post-Cold War science/technology, extensively annotated with references and related texts."><em>Radiance</em></a> (annotated Scholz novel)</li>
<li><a href="./docs/xrisks/1985-hofstadter" class="docMetadata" data-popup-title="Metamagical Themas: Sanity and Survival" data-popup-author="Douglas Hofstadter" data-popup-date="16 Apr 2012" data-popup-abstract="3 essays by AI researcher Douglas Hofstadter exploring cooperation/game theory/'superrationality' in the context of the failure of political coordination to prevent global nuclear war"><em>Metamagical Themas</em>: âSanity and Survivalâ</a> (Hofstadter)</li>
<li><a href="./docs/sociology/1987-rossi" class="docMetadata" data-popup-title="The Iron Law Of Evaluation And Other Metallic Rules" data-popup-author="Peter H. Rossi" data-popup-date="18 Sep 2012" data-popup-abstract="<p><q>â<span class=&quot;smallcaps&quot;>The Iron Law Of Evaluation</span> And Other Metallic Rulesâ</q> is a classic review paper by American <q>â<a href=&quot;https://en.wikipedia.org/wiki/Sociology&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Sociology&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Sociology&amp;lt;/b&amp;gt; is the study of society, patterns of social relationships, social interaction and culture of everyday life. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, acceptance, and change or social evolution. While some sociologists conduct research that may be applied directly to social policy and welfare, others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro-sociology level of individual agency and interaction to the macro level of systems and the social structure.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Sociology&quot;>sociologist</a> <a href=&quot;http://www.asanet.org/footnotes/dec06/indextwo.html&quot;>Peter Rossi</a>, a dedicated progressive and the nationâs leading expert on social program evaluation from the 1960s through the 1980sâ</q>; it discusses the difficulties of creating a useful <a href=&quot;https://en.wikipedia.org/wiki/social_program&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Welfare&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Welfare&amp;lt;/b&amp;gt; is a type of government support for the citizens of that society. Welfare may be provided to people of any income level, as with social security, but it is usually intended to ensure that people can meet their basic human needs such as food and shelter. Welfare attempts to provide a minimal level of well-being, usually either a free- or a subsidized-supply of certain goods and social services, such as healthcare, education, and vocational training.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: social program&quot;>social program</a>, and proposed some aphoristic summary rules, including most famously: &amp;gt; &amp;gt; &amp;gt; - The Iron law: <q>âThe expected value of any net impact assessment of any large scale social program is zeroâ</q> &amp;gt; &amp;gt; - the Stainless Steel law: <q>âthe better designed the impact assessment of a social program, the more likely is the resulting estimate of net impact to be zero.â</q> &amp;gt; &amp;gt; It expands an earlier paper by Rossi (<a href=&quot;../../docs/sociology/1978-rossi.pdf&quot;><q>âIssues in the evaluation of human services deliveryâ</q></a>, Rossi 1978), where he coined the first, <q>âIron Lawâ</q>.</p>">âThe Iron Law of Evaluation And Other Metallic Rulesâ</a> (Rossi)</li>
<li><a href="./docs/cs/1955-nash" class="docMetadata" data-popup-title="John Nash on cryptography" data-popup-author="John Nash" data-popup-date="22 Feb 2012" data-popup-abstract="1955 letters of John Nash and the NSA on a cryptosystem and Nash's belief that near-perfect cryptography could exploit exponential difficulties">John Nash on cryptography &amp; P=NP (1955)</a></li>
<li><a href="./docs/statistics/bayes/1994-falk" class="docMetadata" data-popup-title="The Ups and Downs of the Hope Function In a Fruitless Search" data-popup-author="Ruma Falk, Abigail Lipson, Clifford Konold" data-popup-date="01 Jul 2012" data-popup-abstract="On Bayesian updating of beliefs in sequentially searching a set of possibilities where failure is possible, such as waiting for a bus; the psychologically counterintuitive implication is that success on the next search increases even as the total probability of success decreases.">âUps &amp; Downs of the Hope Function In a Fruitless Searchâ</a></li>
<li><a href="./docs/bitcoin/2008-nakamoto" class="docMetadata" data-popup-title="Wei Dai/Satoshi Nakamoto 2009 Bitcoin emails" data-popup-author="Satoshi Nakamoto, Wei Dai" data-popup-date="17 Mar 2014" data-popup-abstract="Emails in 2009 between Wei Dai and Satoshi Nakamoto discussing Bitcoin draft proposal and B-money">Wei Dai/Satoshi Nakamoto 2009 Bitcoin emails</a></li>
<li><a href="./docs/bitcoin/2011-davis" class="docMetadata" data-popup-title="The Crypto-Currency: Bitcoin and its mysterious inventor" data-popup-author="Joshua Davis" data-popup-date="18 Apr 2013" data-popup-abstract="<p><span class=&quot;smallcaps&quot;>Dept. Of Technology</span> about <a href=&quot;https://en.wikipedia.org/wiki/bitcoin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Bitcoin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Bitcoin&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;â¿&amp;lt;/b&amp;gt;) is a cryptocurrency. It is a decentralized digital currency without a central bank or single administrator that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: bitcoin&quot;>bitcoin</a> [sic] and its mysterious creator. There are lots of ways to make money: You can earn it, find it, counterfeit it, steal it. Or, if youâre <a href=&quot;https://en.wikipedia.org/wiki/Satoshi_Nakamoto&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Satoshi Nakamoto&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Satoshi Nakamoto&amp;lt;/b&amp;gt; is the name used by the pseudonymous person or persons who developed bitcoin, authored the bitcoin white paper, and created and deployed bitcoin&amp;#39;s original reference implementation. As part of the implementation, Nakamoto also devised the first blockchain database. In the process, Nakamoto was the first to solve the double-spending problem for digital currency using a peer-to-peer network. Nakamoto was active in the development of bitcoin up until December 2010.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Satoshi Nakamoto&quot;>Satoshi Nakamoto</a>, you can invent it. Thatâs what he did on the evening of January 3, 2009, when he pressed a button on his keyboard and created a new currency called Bitcoin. It was all bit, and no coin. There was no paper, copper, or silver-just thirty-one thousand lines of code and an announcement on the Internet. Nakamoto wanted to create a currency immune to the predations of bankers and politicians. The currency was controlled entirely by software. Every ten minutes or so, coins would be distributed through a process that resembled a lottery. This way, the bitcoin software would release a total of twenty-one million bitcoins, most all of them over the next twenty years. Interest in Nakamotoâs invention built steadily. More and more people dedicated their computers to the lottery, and forty-four exchanges popped up, allowing anyone with bitcoins to trade them for dollars, euros, or other currencies. At first, a single bitcoin was valued at less than a penny. But merchants gradually began to accept bitcoin, and at the end of 2010 the value began to appreciate rapidly. By June of 2011, a bitcoin was worth more than twenty-nine dollars. Market gyrations followed, and by September the exchange rate had fallen to five dollars. Still, with more than seven million bitcoins in circulation, Nakamoto had created thirty-five million dollars of value. And yet Nakamoto was a cipher. There was no trace of any coder with that name before the dÃ©but of bitcoin. He used an e-mail address and Web site that were untraceable. In 2009 and 2010, he wrote hundreds of posts in flawless English, invited other software developers to help him improve the code. Then, in April, 2011, he sent a note to a developer saying that he had âmoved on to other things.â He has not been heard from since. Tells about failed attempts to hack the bitcoin encryption code. Writer tries to deduce Nakamotoâs true identity from clues in his posts and his code. Describes the <a href=&quot;https://en.wikipedia.org/wiki/International_Cryptology_Conference&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;International Cryptology Conference&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;CRYPTO&amp;lt;/b&amp;gt;, the &amp;lt;b&amp;gt;International Cryptology Conference&amp;lt;/b&amp;gt;, is one of the largest academic conferences in cryptography and cryptanalysis. It is organized by the International Association for Cryptologic Research (IACR), and it is held yearly in August in Santa Barbara, California at the University of California, Santa Barbara.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: International Cryptology Conference&quot;>Crypto</a> 2011 conference of cryptographers, where the writer went looking for Nakamoto. Writer speaks with two possible candidates, Michael Clear and Vili Lehdonvirta, both of whom deny that they are Nakamoto. Also tells about Kevin Groce, who runs a bitcoin-mining operation in Kentucky. Over the summer, hackers targeted bitcoin, and though they were unable to break Nakamotoâs code, they were able to disrupt the exchanges and destroy Web sites that helped users store bitcoins. The number of transactions decreased and the exchange rate plummeted. Commentators predicted the end of the currency. In September, however, volume began to increase again, and the price stabilized, at least temporarily.</p>">âThe Crypto-Currency: Bitcoin &amp; its mysterious inventorâ</a></li>
<li><a href="./docs/japanese/2002-gibson" class="docMetadata" data-popup-title="Shiny balls of Mud: William Gibson Looks at Japanese Pursuits of Perfection" data-popup-author="William Gibson" data-popup-date="20 Apr 2012" data-popup-abstract="Essay on minimalism, otaku, and hikikomori as esthetic choices reflecting an obsessive focus on perfection of a single activity, exemplified by the unusual sculpture form _dorodango_  (hand-rolling mud into colorful spheres).">âShiny balls of Mudâ</a> (Gibson)</li>
<li><a href="./docs/culture/2007-wolfe" class="docMetadata" data-popup-title="Nor the Summers as Golden: Writing Multivolume Works" data-popup-author="Gene Wolfe" data-popup-date="02 Mar 2012" data-popup-abstract="Gene Wolfe on the depth and ending of novel series">âNor the Summers as Golden: Writing Multivolume Worksâ</a> (Wolfe)</li>
<li><a href="./docs/culture/1983-wolfe-thecitadeloftheautarch-thejustman" class="docMetadata" data-popup-title="Loyal to the Group of Seventeen's StoryâThe Just Man" data-popup-author="Gene Wolfe" data-popup-date="20 Jan 2018" data-popup-abstract="Short story on the limits of propaganda and 'Newspeak' using a constructed language; from Chapter 11 of Gene Wolfe's _The Book of the New Sun_, volume 4, _The Citadel of the Autarch_.">âLoyal to the Group of Seventeen's StoryâThe Just Manâ</a> (Wolfe)</li>
<li><a href="./docs/culture/1963-asimov" class="docMetadata" data-popup-title="The Sword of Achilles" data-popup-author="Isaac Asimov" data-popup-date="31 Aug 2011" data-popup-abstract="interest in science fiction predicts future scientific merit?">âThe Sword of Achillesâ</a> (Asimov)</li>
<li><a href="./docs/sr/2013-power" class="docMetadata" data-popup-title="Drugs 2.0: Your Crack's in the Post" data-popup-author="Mike Power" data-popup-date="19 Oct 2013" data-popup-abstract="<p>This is an annotated transcript of the chapter âYour Crackâs in the Postâ (pg219-244) &amp;amp; an excerpt from the chapter âProhibition in the Digital Ageâ (pg262), of <a href=&quot;https://www.amazon.com/Drugs-2-0-Revolution-Thats-Changing-ebook/dp/B00BLCAD4O?tag=gwernnet-20&quot;><em>Drugs 2.0: The Web Revolution Thatâs Changing How the World Gets High</em></a>, <a href=&quot;http://mikepower.pressfolios.com/&quot;>Mike Power</a> (2 May 2013); it is principally on the topic of <a href=&quot;https://en.wikipedia.org/wiki/Bitcoin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Bitcoin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Bitcoin&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;â¿&amp;lt;/b&amp;gt;) is a cryptocurrency. It is a decentralized digital currency without a central bank or single administrator that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Bitcoin&quot;>Bitcoin</a>, <a href=&quot;https://en.wikipedia.org/wiki/Tor_%28anonymity_network%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor (anonymity network)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Tor&amp;lt;/b&amp;gt; is free and open-source software for enabling anonymous communication. The name is derived from an acronym for the original software project name &amp;quot;The Onion Router&amp;quot;. Tor directs Internet traffic through a free, worldwide, volunteer overlay network consisting of more than seven thousand relays to conceal a user&amp;#39;s location and usage from anyone conducting network surveillance or traffic analysis. Using Tor makes it more difficult to trace Internet activity to the user: this includes &amp;quot;visits to Web sites, online posts, instant messages, and other communication forms&amp;quot;. Tor&amp;#39;s intended use is to protect the personal privacy of its users, as well as their freedom and ability to conduct confidential communication by keeping their Internet activities from being monitored.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Tor (anonymity network)&quot;>Tor</a>, and <a href=&quot;../../Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;gwern&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>Silk Road 1</a>.</p>"><em>Drugs 2.0</em>: âYour Crackâs in the Postâ</a> (Mike Power)</li>
<li><a href="./docs/bitcoin/2014-mccaleb" class="docMetadata" data-popup-title="2014 Jed McCaleb MtGox interview" data-popup-author="Jed McCaleb" data-popup-date="16 Feb 2014" data-popup-abstract="Email interview with McCaleb on early history of his MtGox, verifying it did not trade real but online Magic cards">2014 Jed McCaleb MtGox interview</a></li>
<li><a href="./Fulltext" class="docMetadata" data-popup-title="Research Bounties On Fulltexts" data-popup-author="Gwern Branwen" data-popup-date="31 Dec 2018" data-popup-abstract="A list of papers/books/materials I have failed to obtain, and financial bounties for anyone who can provide copies to me or the Internet.">Research Bounties On Documents</a></li>
</ul>
</section>
<section id="anime-docs" class="level1">
<h1><a href="#anime-docs" title="Link to section: 'Anime docs'">Anime docs</a></h1>
<ul>
<li><a href="./otaku" class="docMetadata" data-popup-title="Neon Genesis Evangelion source anthology" data-popup-author="Gwern Branwen" data-popup-date="30 Sep 2009" data-popup-abstract="<p>This page is an Extensive anthology of Gainax/Hideaki Anno/<em>Evangelion</em>-related quotes, excerpts, sources, references, &amp;amp; analyses, organized by reliability &amp;amp; year.</p><p>The purpose of compiling a large page of quotes &amp;amp; references classified by date &amp;amp; source level is to make it easier to put NGE into a historical context by tracing the evolution of plot or characters, cross-reference statements made in interviews, jump forward and backwards to flesh out otherwise obscure allusions to events, and enable easy keyword-based search for various concepts (eg. the connection of Kaworu to cats, Gainaxâs bafflement that viewers might think Misato killed Kaji, the influence of earthquakes on people, connections to Aum Shinrikyo, garbled information about suicide attempts, Annoâs conservative nationalist views or philosophy of âpoisonâ, retcons like swapping the Adam and Lilith plot devices, panspermia &amp;amp; First Ancestral Race being slowly removed from production materials and then post-NGE slowly restored, the many conflicting pieces of information on the end of NGE TV and <em>EoE</em>, Yamagaâs questionable reliability etc).</p><p>As I compile more material, I become increasingly convinced that far from <em>Evangelion</em> being a baffling mystery, it is in fact one of the most understandable anime out there, with a wealth of information about almost every detail, from the earliest planning meetings to how long particular episode productions took to the source of minor details like the âA-10 nerveâ, and that Hideaki Anno, far from being a reticent auteur of mystery, has collectively been forthcoming about anything one might ask - to the point where multiple interviews could justly be described as âbook-lengthâ (the books in question being_June_, <em>Schizo</em>, <em>Prano</em>, the <em>1.0 CRC</em>, &amp;amp; the <em>2.0 CRC</em>). There is so much material that half the difficulty is simply collating the existing materials, and some extensive sources seem to have been lost to both the Japanese and English fandoms (eg. there seem to be no mentions or quotations of the <em>Anata to Watashi no Gainax</em> interviews in the Japanese web).</p>"><em>Evangelion</em> sources anthology</a></li>
<li><a href="./docs/eva/2002-takeda-notenkimemoirs" class="docMetadata" data-popup-title="The Notenki Memoirs: Studio Gainax And The Men Who Created Evangelion" data-popup-author="Yasuhiro Takeda" data-popup-date="27 Dec 2010" data-popup-abstract="Fulltext annotated e-book of 2002 memoir by anime producer Yasuhiro Takeda, discussing Japanese SF conventions &amp; fandom, formation &amp; history of Gainax and its productions up to 2002, including the origins of Evangelion &amp; the tax raid"><em>The Notenki Memoirs</em></a> (Takeda)</li>
<li><a href="./docs/eva/2010-crc" class="docMetadata" data-popup-title="Evangelion 2.0 Complete Records Collection" data-popup-author="Ryusuke Hikawa, Hideaki Anno, Shinji Higuchi, YÅji Enokido, Kazuya Tsurumaki, Mohiro Kitoh, Shigeto Koyama, Yoshito Asari" data-popup-date="29 Aug 2011" data-popup-abstract="Translated interviews about making 'Evangelion 2.0' w/Anno, Higuchi, Enokido, &amp; Tsurumaki"><em><span class="smallcaps-auto">NGE</span> 2.0 Complete Records Collection (CRC)</em></a></li>
<li><a href="./docs/eva/1996-animerica-conscience-otaking" class="docMetadata" data-popup-title="The Conscience of the Otaking: The Studio Gainax Saga in Four Parts" data-popup-author="Toshio Okada" data-popup-date="03 Oct 2011" data-popup-abstract="Interview of former Gainax president Toshio Okada on Gainax's history, _Wings of Honneamise_, _Aoki Uru_, etc.">âConscience of the Otakingâ</a> (Okada)</li>
<li><a href="./docs/eva/2011-house" class="docMetadata" data-popup-title="Interviewing translator Michael House" data-popup-author="Michael House" data-popup-date="11 Nov 2011" data-popup-abstract="Working at Gainax, Evangelion's production &amp; censorship, anime translation">âGainax translator Michael House interviewâ</a></li>
<li><a href="./docs/eva/1996-newtype-anno-interview" title="English translation of French translation of controversial NewType interview at the end of the TV broadcast">1996 Hideaki Anno interview</a></li>
<li><a href="./docs/eva/1997-animeland-may-hideakianno-interview-english" class="docMetadata" data-popup-title="May 1997 AnimeLand Interview with Hideaki Anno (English)" data-popup-author="Hideaki Anno" data-popup-date="28 Feb 2012" data-popup-abstract="English translation of a French anime journalist's interview of Hideaki Anno on anime and Evangelion">1997 Hideaki Anno interview</a> (<a href="./docs/eva/1997-animeland-may-hideakianno-interview-french" class="docMetadata" data-popup-title="Interview with Hideaki Anno (French)" data-popup-author="Hideaki Anno" data-popup-date="28 Feb 2012" data-popup-abstract="Transcript of a French anime writer discussion of anime and Evangelion">French</a>)</li>
<li><a href="./docs/eva/2003-oshii-izubuchi" class="docMetadata" data-popup-title="Talk About RahXephon: In Search of Fantasy and Details" data-popup-author="Mamoru Oshii, Yutaka Izubuchi" data-popup-date="28 Feb 2012" data-popup-abstract="Oshii criticizes RahXephon and compares it to Neon Genesis Evangelion">âTalk About RahXephonâ</a></li>
<li><a href="./docs/eva/2003-rahxephoncomplete-anno-izubuchi" class="docMetadata" data-popup-title="Special Talk: Yutaka Izubuchi x Hideaki Anno" data-popup-author="Gwern Branwen" data-popup-date="28 Feb 2012" data-popup-abstract="Discussion by Izubuchi and Anno of classic mecha anime">âSpecial Talk: Yutaka Izubuchi x Hideaki Annoâ</a></li>
<li><a href="./docs/eva/2004-okada" class="docMetadata" data-popup-title="Otaku Talk" data-popup-author="Toshio Okada, Kaichiro Morikawa, Takashi Murakami, Reiko Tomii" data-popup-date="09 Apr 2012" data-popup-abstract="Definition of otaku, mania, moe, dame, anime, and generations">âOtaku Talkâ</a> (Okada, Morikawa etc)</li>
<li><a href="./docs/eva/2005-murakami" class="docMetadata" data-popup-title="Earth in My Window" data-popup-author="Takashi Murakami" data-popup-date="04 Mar 2012" data-popup-abstract="Essay by Pop Art artist Takashi Murakami on Japanese society and on WWII infantilizing Japanese culture as revealed by media, anime, and otaku.">âEarth in My Windowâ</a> (Murakami)</li>
<li><a href="./docs/anime/1997-utena" class="docMetadata" data-popup-title="Utena 2011 Boxset Booklet Commentary" data-popup-author="Kunihiko Ikuhara, Yuichirou Oguro, Hiroshi Kaneda, Haruyasu Yamazaki, Tomomi Takemura, Hideki Ito, Yo Yamada, Tomokazu Mii, Yoji Enokido, Shinya Hasegawa, J.A. Caesar, Toshimichi Otsuki, Chiho Saito, Sarah Alys Lindholm, C.A.P." data-popup-date="7 Feb 2013" data-popup-abstract="Kunihiko Ikuhara and staff episode and music commentary/discussion 1997-2011 on the anime 'Revolutionary Girl Utena'; discusses origins of ideas and meaning of themes, and video/audio remastering for the DVD box set."><em>Utena</em> 2011 Boxset Commentary</a></li>
<li><a href="./docs/eva/2005-sawaragi" class="docMetadata" data-popup-title="On The Battlefield of " data-popup-author="Noi Sawaragi" data-popup-date="18 May 2012" data-popup-abstract="Post-WWII ambiguity causes otakudom">âOn The Battlefield of âSuperflatââ</a> (Sawaragi)</li>
<li><a href="./docs/anime/2010-sarrazin" class="docMetadata" data-popup-title="Ero-Anime: Manga Comes Alive" data-popup-author="Stephen Sarrazin" data-popup-date="23 Dec 2011" data-popup-abstract="Short history of trends in Japanese erotic anime and manga, especially lolicon">âEro-Anime: Manga Comes Aliveâ</a> (Sarrazin)</li>
</ul>
</section>
<section id="wikipedia" class="level1">
<h1><a href="#wikipedia" title="Link to section: 'Wikipedia'">Wikipedia</a></h1>
<ul>
<li><a href="./In-Defense-Of-Inclusionism" class="docMetadata" data-popup-title="In Defense of Inclusionism" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="<p>English Wikipedia is in decline. As a long-time editor &amp;amp; former admin, I was deeply dismayed by the process. Here, I discuss UI principles, changes in Wikipedian culture, the large-scale statistical evidence of decline, run small-scale experiments demonstrating the harm, and conclude with parting thoughts.</p>">In Defense Of Inclusionism</a></li>
<li><a href="./Wikipedia-and-Dark-Side-Editing" class="docMetadata" data-popup-title="Wikipedia and Dark Side Editing" data-popup-author="Gwern Branwen" data-popup-date="24 May 2009" data-popup-abstract="Cynical tactics encouraged by Wikipedia's abdication of thought known as 'No Original Research' and 'Reliable Sources'">Wikipedia &amp; Darkside Editing</a></li>
<li><a href="./Wikipedia-and-Other-Wikis" class="docMetadata" data-popup-title="Wikipedia and Other Wikis" data-popup-author="Gwern Branwen" data-popup-date="27 Jan 2009" data-popup-abstract="Network effects &amp; benefits of gritting one's teeth &amp; submitting to a Wikipedia's rules, rather than using Wikia or one's own site.">Wikipedia &amp; Other Wikis</a></li>
<li><a href="./Wikipedia-and-YouTube" class="docMetadata" data-popup-title="Wikipedia &amp; YouTube" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="Why Wikipedia and YouTube will never be integrated.">Wikipedia &amp; YouTube</a></li>
<li><a href="./Wikipedia-and-Knol" class="docMetadata" data-popup-title="Wikipedia &amp; Knol: Why Knol Already Failed" data-popup-author="Gwern Branwen" data-popup-date="21 Jan 2009" data-popup-abstract="Why Knol is worse than Wikipedia, has failed, and will continue to fail.">Wikipedia &amp; Knol</a></li>
<li><a href="./Wikipedia-resume" class="docMetadata" data-popup-title="Wikipedia RÃ©sumÃ©" data-popup-author="Gwern Branwen" data-popup-date="18 Oct 2010" data-popup-abstract="A precis of my work on the English Wikipedia, 2004-2017 (edit counts, articles by subject, and particularly notable articles)">My Wikipedia articles</a></li>
</ul>
</section>
<section id="personal" class="level1">
<h1><a href="#personal" title="Link to section: 'Personal'">Personal</a></h1>
<ul>
<li><a href="./About" class="docMetadata" data-popup-title="About This Website" data-popup-author="Gwern Branwen" data-popup-date="01 Oct 2010" data-popup-abstract="Meta page describing gwern.net site ideals of stable long-term essays which improve over time; technical decisions using Markdown and static hosting; idea sources and writing methodology; metadata definitions; semi-annual web traffic statistics; copyright license">About</a> / <a href="./Links" class="docMetadata" data-popup-title="Links" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Who am I online &amp; what have I done? Contact information; sites I use; computers and software tools; things I've worked on; psychological profiles">Links</a> / <a href="./tags/newsletter" class="docMetadata" data-popup-title="Gwern.net newsletter archives" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews.)">newsletter archives</a></li>
<li><a href="./Notes" class="docMetadata" data-popup-title="Notes" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Misc thoughts, memories, proto-essays, musings, etc.">Notes</a></li>
<li><a href="./Epigrams" class="docMetadata" data-popup-title="Epigrams" data-popup-author="Gwern Branwen" data-popup-date="30 April 2014" data-popup-abstract="Witticisms, parodies, pointed observations, japeries, jocularity, Tom Swifties, examples of nominative determinism, and discursive drollery">Epigrams</a></li>
<li><a href="./Book-reviews" class="docMetadata" data-popup-title="Ratings and reviews" data-popup-author="Gwern Branwen" data-popup-date="23 Aug 2013" data-popup-abstract="A list of books I have read since ~1997, with reviews.">Book reviews/ratings</a></li>
<li><a href="./Tea" class="docMetadata" data-popup-title="Tea Reviews" data-popup-author="Gwern Branwen" data-popup-date="13 Apr 2011" data-popup-abstract="Teas I have drunk, with reviews and future purchases; focused primarily on oolongs and greens. Plus experiments on water.">Tea reviews</a>/<a href="./Mead" class="docMetadata" data-popup-title="Mead" data-popup-author="Gwern Branwen" data-popup-date="02 May 2012" data-popup-abstract="Ratings of mead and fruit wines I have tried.">Mead</a></li>
<li><a href="./Blackmail" class="docMetadata" data-popup-title="Blackmail fail" data-popup-author="Gwern Branwen" data-popup-date="10 Dec 2013" data-popup-abstract="<p>In September 2012, I was extorted for $32 for being gwern; I declined to pay. In November 2013, I called an encryption bluff that I was Dread Pirate Roberts. In December 2013, a crazy person tried to blackmail me for billions of dollars for being Satoshi Nakamoto; I declined to pay. In March 2014, the DNM Evolution threatened to dox me if I did not reveal information about their security vulnerabilities. In February 2015, an Agora user doxed me in an unexpected way and I paid a small bounty.</p>">Blackmail attempts</a></li>
</ul>
</section>
</div>
</article>
</main>

<link rel="stylesheet" type="text/css" href="/static/css/fonts.css">
<link rel="stylesheet" type="text/css" href="/static/css/default.css">


<script src="/static/js/popups.js" defer=""></script>

<script src="/static/js/darkmode.js" defer=""></script>

<script id="googleAnalytics" src="https://www.googletagmanager.com/gtag/js?id=UA-18912926-1" async=""></script>
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-18912926-1');
    </script>


<div id="ui-elements-container"><div id="mode-selector"><button type="button" class="select-mode-auto selected" tabindex="-1" data-name="auto" title="Set light or dark mode automatically, according to system-wide setting" disabled="">Auto</button><button type="button" class="select-mode-light active" tabindex="-1" data-name="light" title="Light mode at all times">Light</button><button type="button" class="select-mode-dark" tabindex="-1" data-name="dark" title="Dark mode at all times">Dark</button></div></div></body>
<html lang="en"><head>
<meta charset="utf-8">
<meta name="generator" content="hakyll">
<meta name="google-site-verification" content="BOhOQI1uMfsqu_DopVApovk1mJD5ZBLfan0s9go3phk">
<meta name="title" content="Gwern.net Index of Essays">
<meta name="author" content="gwern">
<meta name="description" content="Writer, self-experimenter, and programmer: psychology, statistics, and technology. This index page is a categorized list of gwern.net pages.">
<meta name="keywords" content="statistics, darknet markets, Bitcoin, blinded self-experiments, Quantified Self, dual n-back, spaced repetition, modafinil, literary criticism">
<meta name="dc.date.issued" content="27 January 2009">
<meta name="dcterms.modified" content="2 june 2020">
<link rel="canonical" href="https://www.gwern.net/index">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Essays - Gwern.net</title>

<link rel="preload" href="/images/logo-smooth.svg" as="image" type="image/svg+xml">


<style> /* index-specific: */
      @media only screen and (max-width:64.9ch) { body.siteIndex #markdownBody li { line-height: 1.65; } }

/*  Add some vertical padding for the introduction.
    */
body.siteIndex #markdownBody {
    padding-top: 1.5ch;
}

/*  Make the sections reflow; better than fixed-width tables.
    */
body.siteIndex article section {
    display: inline-block;
    vertical-align: top;
}

/*  No index self-link.
    */
body.siteIndex #sidebar a#logo {
    pointer-events: none;
}
/*  Abstract is not a real abstract.
    */
body.siteIndex #abstract {
    padding: 0;
    border: none;
    margin: 0;
    box-shadow: none;
}

/*  Less fancy headings.
    */
body.siteIndex section:not(.collapse) > h1:first-child {
    font-size: 1.5em;
    line-height: 1.125;
    box-shadow: none;
    text-align: left;
    font-weight: bold;
    margin-left: 0;
    padding: 0;
}

/*  Nice-looking bottom decoration.
    */
body.siteIndex article,
body.tags article {
    position: relative;
    padding-bottom: 4em;
}
body.siteIndex article::after,
body.tags article::after{
    content: "";
    background-color: #fff;
    background-image: url('/images/logo-smooth.svg');
    display: block;
    position: absolute;
    bottom: 0.75em;
    width: 22px;
    height: 30px;
    background-size: contain;
    background-position: center;
    background-repeat: no-repeat;
    left: 0;
    right: 0;
    margin: auto;
    padding: 0 10px 0 11px;
    z-index: 1;
}
body.siteIndex article::before,
body.tags article::before{
    content: "";
    display: block;
    position: absolute;
    bottom: calc(0.75em + 15px);
    height: 1px;
    border-bottom: 1px dotted #000;
    width: 100%;
    opacity: 0.5;
}

/*  Lists on the home page.
    */
body.siteIndex #markdownBody ul,
body.tags #markdownBody ul {
    margin: 1.25em 3em 0 0;
    padding: 0 0 0 1.375em;
}
body.siteIndex #markdownBody li > ul,
body.tags #markdownBody li > ul {
    margin: 0.25em 0 0.25em 0;
}
body.siteIndex #markdownBody li,
body.tags #markdownBody li {
    margin-top: 0;
}
body.siteIndex #markdownBody > section ~ p {
    border-top: 1px dotted #777;
    margin: 2em 0 0 0;
    padding: 1em 0 0 0;
    font-weight: bold;
}

/*  Internal links on the home page need no decoration.
    */
body.siteIndex #markdownBody a[href^="."]:not([href*="/docs/"]):not([href*="/images/"])::after,
body.siteIndex #markdownBody a[href^="https://www.gwern.net/"]:not([href$=".pdf"])::after {
    content: none;
}

@media only screen and (min-width: 120ch) {
    /*  Leave enough horizontal room to show multiple sections simultaneously,
        table-like
        */
    body.siteIndex #markdownBody {
        width: 135ch;
        max-width: calc(50vw + 42ch);
    }
    body.siteIndex p,
    body.siteIndex hr {
        max-width: 97ch;
    }
}
@media only screen and (max-width: 64.9ch) {
    body.siteIndex #sidebar #logo {
        border: 1px dotted transparent;
        align-self: flex-start;
        padding: 7px 5px 6px 5px;
    }
    body.siteIndex #markdownBody section {
        max-width: 100%;
    }
    body.siteIndex #markdownBody ul,
    body.tags #markdownBody ul {
        overflow-wrap: break-word;
        margin-right: 0;
    }
    body.siteIndex #markdownBody li,
    body.tags #markdownBody li {
        padding: 1px 0;
        margin: 2px 0 0 0;
    }
    body.siteIndex #markdownBody ul > li::before,
    body.tags #markdownBody ul > li::before {
        top: 0.25em;
    }
}

      /* general */
      :root{--GW-blockquote-background-color:#f5f5f5}html{padding:0;margin:0;background-color:#fff;color:#000;font-weight:400;font-family:'Source Serif Pro',Baskerville,'Libre Baskerville',serif}body{max-width:112ch}@media only screen and (max-width:64.9ch){html{font-size:18px}}@media only screen and (min-width:65ch){body{padding:0 1.5ch 0 .5ch;margin:0 auto}@media only screen and (min-width:118.5ch){body{padding:0 6ch 0 .5ch}}main{min-height:100vh;display:flex;flex-flow:column}@media only screen and (min-width:176.1ch){main{position:relative;right:4ch}}@supports (-moz-user-focus:normal){@media only screen and (min-width:176.1ch){main{right:0}}}article{flex:1 1 auto}#sidebar{position:absolute}article,header{margin-left:15ch}@media only screen and (max-width:120ch){article,header{margin-left:14.5ch}}@media only screen and (max-width:112ch){article,header{margin-left:14ch}}@media only screen and (max-width:104ch){article,header{margin-left:13.5ch}}@media only screen and (max-width:96ch){article,header{margin-left:13ch}}}@media only screen and (max-width:64.9ch){body{margin:0 1ch}}#sidebar code{border:none;background-color:transparent;padding:0}#sidebar a{display:block}@media only screen and (min-width:65ch){#sidebar{font-variant:small-caps;padding:0 4ch 0 1ch;width:10ch}#sidebar a#logo{margin:1em 0 2em 0}#sidebar a#logo img{width:64px}#sidebar a.patreon{border-top:1px dotted #aaa}#sidebar a.new{box-shadow:0 1.125em 0 0 #fff inset}#sidebar a.patreon{font-size:.9em;margin-top:1.375em;padding-top:1.25em;box-shadow:0 1.25em 0 0 #fff inset;white-space:nowrap}#sidebar a.mail,#sidebar a.r_gwern{line-height:1;padding-left:.25em;font-size:1.05em}#sidebar a.mail::before,#sidebar a.r_gwern::before{content:'Â°\2007';text-shadow:0 0 0 #000;position:relative;top:.25em}#sidebar code{font-variant:none;text-transform:uppercase;font-size:.7em}}@media only screen and (max-width:64.9ch){#sidebar{justify-content:center;margin:0 0 .5em 0}#sidebar a{border:1px dotted #000;padding:3px 10px;text-align:center;margin:1px}#sidebar a#logo{padding:8px 5px 3px 5px}#sidebar,#sidebar-links{display:flex}#sidebar-links{flex-flow:row wrap;margin:.5em 0 0 0}#sidebar a.links,#sidebar a.site{flex:1 1 20%}#sidebar a.mail,#sidebar a.new,#sidebar a.r_gwern{padding:3px 10px;flex:1 1 auto}#sidebar a.patreon{display:none}#sidebar #logo{margin:calc(.5em + 1px) 1px 1px 0}#sidebar #logo img{width:2.5rem}}header{overflow:auto}header h1{margin:.75em 0;text-align:center;text-transform:none;font-variant:small-caps;font-size:2.5em;line-height:1.15;font-weight:600;letter-spacing:-1px}@media only screen and (max-width:64.9ch){header h1{font-size:2em}}#page-metadata hr{display:none}#page-metadata{margin:0 0 2rem 0;overflow:auto;font-size:.95em;line-height:1.5}#page-metadata #page-description{display:block;margin:0 auto .5em auto}#page-metadata #page-description+br{display:none}#page-metadata span:nth-of-type(n+3){white-space:nowrap}#TOC{border:1px solid #ccc;background-color:#f9f9f9;font-family:'Lucida Sans Unicode','Source Sans Pro',Helvetica,'Trebuchet MS',sans-serif;margin:0 2rem 1.5rem 0;line-height:1.25;padding:10px 10px 2px 14px;position:relative;z-index:1}#TOC:empty{display:none}@media only screen and (max-width:128ch){#TOC{font-size:.95rem}}@media only screen and (max-width:120ch){#TOC{font-size:.9rem}}@media only screen and (max-width:112ch){#TOC{font-size:.85rem;margin:0 1.5rem 1.25rem 0}}@media only screen and (max-width:104ch){#TOC{font-size:.8rem;margin:0 1.25rem 1rem 0}}@media only screen and (max-width:96ch){#TOC{margin:0 1rem 1rem 0}}@media only screen and (min-width:90ch){#TOC{float:left;max-width:30ch}}@media only screen and (max-width:90ch){#TOC{float:none;margin:2em auto;font-size:1rem}#TOC>ul>li>ul{column-count:2}}@media only screen and (max-width:64.9ch){#TOC a{display:inline-block}#TOC>ul>li>ul{column-count:1}#TOC li li a{padding:0 0 1px 0}#TOC li li li a{padding:0 0 2px 0}#TOC li li li li a{padding:0 0 3px 0}#TOC li li li li a{padding:0 0 4px 0}}#TOC ul{list-style-type:none;padding-left:0;margin-bottom:0;margin-top:4px;padding-left:1.4em;text-indent:0;padding:0}#TOC ul ul{list-style-type:none;padding-left:.7em;margin-top:2px}#TOC li{font-weight:700;margin:5px 0 10px 0;padding-left:1.125em;position:relative;overflow-wrap:break-word}#TOC li li{margin-bottom:0;font-weight:400;font-size:.9em}#TOC p{margin-top:9px;margin-bottom:3px}#TOC a{border:0;display:block;position:relative}#TOC a:hover{background-color:rgba(0,0,0,.05);color:#000}#TOC a:hover::after{content:'';display:inline-block;position:absolute;left:100%;top:0;background-color:#ccc;width:.25em;height:100%}#TOC code{font-family:inherit;font-size:inherit;border:none;padding:0;background-color:inherit}#TOC>ul{counter-reset:htoc_1}#TOC>ul>li::before{counter-increment:htoc_1;content:counter(htoc_1) '\2006  '}#TOC>ul ul{counter-reset:htoc_2}#TOC>ul ul li::before{counter-increment:htoc_2;content:counter(htoc_1) '.' counter(htoc_2) '\2006  '}#TOC>ul ul ul{counter-reset:htoc_3}#TOC>ul ul ul li::before{counter-increment:htoc_3;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '\2006  '}#TOC>ul ul ul ul{counter-reset:htoc_4}#TOC>ul ul ul ul li::before{counter-increment:htoc_4;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '\2006  '}#TOC>ul ul ul ul ul{counter-reset:htoc_5}#TOC>ul ul ul ul ul li::before{counter-increment:htoc_5;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '.' counter(htoc_5) '\2006  '}#TOC>ul ul ul ul ul ul{counter-reset:htoc_6}#TOC>ul ul ul ul ul ul li::before{counter-increment:htoc_6;content:counter(htoc_1) '.' counter(htoc_2) '.' counter(htoc_3) '.' counter(htoc_4) '.' counter(htoc_5) '.' counter(htoc_6) '\2006  '}#TOC ul li::before{position:absolute;right:calc(100% - 1em);width:12ch;text-align:right;font-weight:400;opacity:.4;pointer-events:none}#TOC ul li:hover::before{opacity:.7}#markdownBody{overflow-wrap:break-word}@media only screen and (min-width:176.1ch){#markdownBody{position:relative}}@media only screen and (min-width:65ch){@media only screen and (max-width:100ch){#markdownBody{line-height:1.45}}@media only screen and (min-width:100.1ch) and (max-width:120ch){#markdownBody{line-height:1.5}}@media only screen and (min-width:120.1ch){#markdownBody{line-height:1.55}}}@supports (-webkit-hyphens:auto) or (-ms-hyphens:auto) or (hyphens:auto){#markdownBody li,#markdownBody p{text-align:justify;-webkit-hyphens:auto;-ms-hyphens:auto;hyphens:auto}}#markdownBody p{font-variant-numeric:oldstyle-nums}#abstract blockquote{margin:0 0 1.5em 0;box-shadow:0 0 0 5px #fff inset;border-color:#bbb;padding:.9rem 1.25rem .95rem 1.25rem}h1{margin:1.25em 0 .5em -.75rem;font-weight:700;position:relative}@media only screen and (max-width:65ch){h1{margin:1.25em 0 .5em 0}}h1{font-feature-settings:'smcp';font-size:1.75em;line-height:1.25;letter-spacing:-.75px}a{color:#3c3c3c;text-decoration:none}#markdownBody a{word-wrap:break-word}article>:not(#TOC) a:link{text-decoration:none;background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#333,#333);background-size:.05em 1px,.05em 1px,1px 1px;background-repeat:no-repeat,no-repeat,repeat-x;background-position:0 90%,100% 90%,0 90%;text-shadow:.03em 0 #fff,-.03em 0 #fff,0 .03em #fff,0 -.03em #fff,.06em 0 #fff,-.06em 0 #fff,.09em 0 #fff,-.09em 0 #fff,.12em 0 #fff,-.12em 0 #fff,.15em 0 #fff,-.15em 0 #fff;font-variant-numeric:lining-nums}article>:not(#TOC) a:hover{background-image:linear-gradient(#fff,#fff),linear-gradient(#fff,#fff),linear-gradient(#999,#999)}#markdownBody ol,#markdownBody ul{list-style-type:none;margin:1.25em 0 1.5em 0;padding:0 0 0 2.5em;overflow:hidden}#markdownBody li>ol,#markdownBody li>ul{margin:.5em 0}#markdownBody ol>li,#markdownBody ul>li{position:relative;margin:0}#markdownBody ol>li:nth-of-type(n+2),#markdownBody ul>li:nth-of-type(n+2){margin:.5em 0 0 0}#markdownBody ol>li::before,#markdownBody ul>li::before{position:absolute;z-index:1}@media only screen and (max-width:64.9ch){#markdownBody ol,#markdownBody ul{padding:0 0 0 1.75em}}blockquote{margin:1.625em 0 1.75em 0;border:1px solid #ccc;font-size:.95em;padding:1em 1.25em}@media only screen and (min-width:65ch){blockquote{overflow:hidden}}@media only screen and (max-width:64.9ch){blockquote{margin:1.25em 0 1.5em 0}}blockquote,blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote{z-index:-2;background-color:var(--GW-blockquote-background-color)}blockquote blockquote,blockquote blockquote blockquote blockquote,blockquote blockquote blockquote blockquote blockquote blockquote{background-color:#e6e6e6}article>:not(#TOC) a:link q,article>:not(#TOC) blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote blockquote a:link,article>:not(#TOC) q a:link,article>:not(#TOC) span.quote-mark.open+a:link{text-shadow:.03em 0 var(--GW-blockquote-background-color),-.03em 0 var(--GW-blockquote-background-color),0 .03em var(--GW-blockquote-background-color),0 -.03em var(--GW-blockquote-background-color),.06em 0 var(--GW-blockquote-background-color),-.06em 0 var(--GW-blockquote-background-color),.09em 0 var(--GW-blockquote-background-color),-.09em 0 var(--GW-blockquote-background-color),.12em 0 var(--GW-blockquote-background-color),-.12em 0 var(--GW-blockquote-background-color),.15em 0 var(--GW-blockquote-background-color),-.15em 0 var(--GW-blockquote-background-color)}article>:not(#TOC) blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote a:link,article>:not(#TOC) blockquote blockquote blockquote blockquote blockquote blockquote a:link{text-shadow:.03em 0 #e6e6e6,-.03em 0 #e6e6e6,0 .03em #e6e6e6,0 -.03em #e6e6e6,.06em 0 #e6e6e6,-.06em 0 #e6e6e6,.09em 0 #e6e6e6,-.09em 0 #e6e6e6,.12em 0 #e6e6e6,-.12em 0 #e6e6e6,.15em 0 #e6e6e6,-.15em 0 #e6e6e6}blockquote blockquote{margin:1em 1px}#markdownBody blockquote blockquote:first-child{margin:.25em 1px 1em 1px}#markdownBody blockquote>:last-child,#markdownBody blockquote>:last-child>:last-child,#markdownBody blockquote>:last-child>:last-child>:last-child,#markdownBody blockquote>:last-child>:last-child>:last-child>:last-child{margin-bottom:0}#markdownBody blockquote>:first-child,#markdownBody blockquote>:first-child>:first-child,#markdownBody blockquote>:first-child>:first-child>:first-child,#markdownBody blockquote>:first-child>:first-child>:first-child>:first-child{margin-top:0}#markdownBody blockquote>:last-child>:last-child>:last-child>table:last-child,#markdownBody blockquote>:last-child>:last-child>table:last-child,#markdownBody blockquote>:last-child>table:last-child,#markdownBody blockquote>table:last-child{margin-bottom:.5em}#markdownBody blockquote>:first-child>:first-child>:first-child>table:first-child,#markdownBody blockquote>:first-child>:first-child>table:first-child,#markdownBody blockquote>:first-child>table:first-child,#markdownBody blockquote>table:first-child{margin-top:.5em}#markdownBody blockquote>ol:only-child,#markdownBody blockquote>ul:only-child{margin-left:1.5em}blockquote p>a:first-child code:first-child,blockquote p>code:first-child{border:none;background-color:transparent;font-weight:700;font-family:inherit;padding:0;font-size:inherit}blockquote table{font-size:.7em}p{margin:0}p+p{text-indent:2.5em}@media only screen and (max-width:64.9ch){p+p{text-indent:1em}}code{padding:0 4px;font-family:'Liberation Mono',Consolas,Courier,monospace;font-size:.9em;border:1px solid #c8c8c8;background-color:#fafafa}pre{overflow:auto;margin:1.75em auto;border:1px solid #c8c8c8;background-color:#fafafa;cursor:text;max-height:calc(100vh - 8em)}pre code{display:block;padding:8px 14px;margin:0;border:none;background-color:transparent}span.smallcaps{font-feature-settings:'smcp'}span.allcaps{text-transform:uppercase}hr{border:none;height:0;border-bottom:1px solid #aaa;margin:1em 0}#adsense{display:block;text-align:center;display:none}#adsense a img{max-width:100%}</style>
<link rel="preload" as="style" href="/static/css/default.css">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSansPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Bold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-Semibold.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-SemiboldItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">
<link rel="preload" href="/static/font/SourceSerifPro-BASIC-RegularItalic.ttf" as="font" type="font/ttf" crossorigin="anonymous">


<style>
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSerifPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap;
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 600;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Semibold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Serif Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSerifPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }

    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Regular.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 400;
        font-style: italic;
        src: url('/static/font/SourceSansPro-BASIC-RegularItalic.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    @font-face {
        font-family: 'Source Sans Pro';
        font-weight: 700;
        font-style: normal;
        src: url('/static/font/SourceSansPro-BASIC-Bold.ttf') format('truetype');
        unicode-range: U+0020-007E, U+2010, U+2013-2014, U+2018-2019, U+201C-201D;
        font-display: swap
    }
    </style>

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://cdnjs.cloudflare.com">
<link rel="preconnect" href="https://stats.g.doubleclick.net">
<link rel="shortcut icon" type="image/x-icon" href="./static/img/favicon.png">
<style id="popups-styles">
#popup-container {
    position: absolute;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    pointer-events: none;
    z-index: 3;
}
#popup-container > * {
    pointer-events: auto;
}
@media not screen and (hover:hover) and (pointer:fine) {
    #popup-container.popup-visible::before {
        content: "";
        position: absolute;
        left: 0;
        right: 0;
        top: 0;
        bottom: 0;
        pointer-events: auto;
        background-color: #000;
        opacity: 0.4;
    }
}

#popupdiv {
    z-index: 10001;
    font-size: 0.8em;
    box-shadow: 0 0 0 2px #fff;
    position: absolute;
    opacity: 1.0;
    transition: none;
    touch-action: none;
    user-select: none;
}
#popupdiv.fading {
    opacity: 0.0;
    transition:
        opacity 0.75s ease-in 0.1s;
}
#popupdiv > div {
    background-color: #fff;
    padding: 12px 16px 14px 16px;
    border: 3px double #aaa;
    line-height: 1.45;
    overflow: auto;
    overscroll-behavior: none;
    touch-action: none;
    user-select: none;
    min-width: 360px;
    max-width: 640px;
    max-height: calc(100vh - 2 * 3px - 26px);
}
/* TODO: the popups should ideally inherit from the regular CSS once the #markdownBody class is rewritten, and the underlining can be removed */
#popupdiv a { text-decoration: underline; }
#popupdiv a:hover { color: #888; }
#popupdiv > div .data-field {
    text-align: left;
    text-indent: 0;
    hyphens: none;
}
#popupdiv > div .data-field + .data-field {
    margin-top: 0.25em;
}
#popupdiv > div .data-field:empty {
    display: none;
}
#popupdiv > div .data-field.title {
    font-weight: bold;
    font-size: 1.125em;
}
#popupdiv > div .data-field.author-plus-date {
    font-style: italic;
}
#popupdiv > div .data-field.abstract {
    text-align: justify;
    text-indent: 2em;
    hyphens: auto;
}
#popupdiv > div.popup-screenshot {
    padding: 0;
    max-width: unset;
}
#popupdiv > div.popup-screenshot img {
    display: block;
}
#popupdiv > div.popup-screenshot a::after {
    content: none;
}
#popupdiv > div.popup-section-embed,
#popupdiv > div.popup-citation-context {
    height: 100%;
    padding: 12px 24px 14px 24px;
    overflow-x: hidden;
}
#popupdiv > div.popup-section-embed > h1:first-child,
#popupdiv > div.popup-section-embed > h2:first-child,
#popupdiv > div.popup-section-embed > h3:first-child,
#popupdiv > div.popup-section-embed > h4:first-child  {
    margin-top: 0;
}
#popupdiv > div.popup-section-embed > :last-child {
    margin-bottom: 12px;
}
#popupdiv > div .icon {
    background-image: none !important;
    position: relative;
    top: 0.15em;
    font-size: 1.125em;
}
#popupdiv > div .icon::after {
    margin: 0 0.175em 0 0;
    width: 1em;
    height: 1em;
    font-size: 1em;
}
#popupdiv > div .icon:not([href*='.pdf'])::after {
    background-position: center center;
    background-size: 100%;
}
#popupdiv > div .title-link::after {
    content: none;
}

/*  Scroll bar styles (Webkit/Blink only).
    */
#popupdiv > div::-webkit-scrollbar {
    width: 14px;
}
#popupdiv > div::-webkit-scrollbar-thumb {
    background-color: #ccc;
    box-shadow:
        0 0 0 3px #fff inset;
}
#popupdiv > div::-webkit-scrollbar-thumb:hover {
    background-color: #999;
}

/*  Popups on mobile.
    */
@media only screen and (max-width: 64.9ch), not screen and (hover:hover) and (pointer:fine) {
    #popupdiv > div {
        max-width: 100%;
    }
}

/*  Image focus interaction.
    */
#markdownBody #popupdiv img {
    filter: none;
    cursor: initial;
    transform: none;
}
#markdownBody #popupdiv .popup-screenshot a img {
    cursor: pointer;
}

#popupdiv .originalURL {
  font-size: 75%;
}

</style><style id="mode-selector-styles">
    #mode-selector {
        position: absolute;
        right: 3px;
        top: 4px;
        display: flex;
        background-color: #fff;
        padding: 0.125em 0.25em;
        border: 3px solid transparent;
        opacity: 0.3;
        transition:
            opacity 2s ease;
    }
    #mode-selector.hidden {
        opacity: 0;
    }
    #mode-selector:hover {
        transition: none;
        opacity: 1.0;
        border: 3px double #aaa;
    }
    #mode-selector button {
        -moz-appearance: none;
        appearance: none;
        border: none;
        background-color: transparent;
        padding: 0.5em;
        margin: 0;
        line-height: 1;
        font-family: Lucida Sans Unicode, Source Sans Pro, Helvetica, Trebuchet MS, sans-serif;
        font-size: 0.75rem;
        text-align: center;
        color: #777;
        position: relative;
    }
    #mode-selector button:hover,
    #mode-selector button.selected {
        box-shadow:
            0 2px 0 6px #fff inset,
            0 1px 0 6px currentColor inset;
    }
    #mode-selector button:not(:disabled):hover {
        color: #000;
        cursor: pointer;
    }
    #mode-selector button:not(:disabled):active {
        transform: translateY(2px);
        box-shadow:
            0 0px 0 6px #fff inset,
            0 -1px 0 6px currentColor inset;
    }
    #mode-selector button.active:not(:hover)::after {
        content: "";
        position: absolute;
        bottom: 0.25em;
        left: 0;
        right: 0;
        border-bottom: 1px dotted currentColor;
        width: calc(100% - 12px);
        margin: auto;
    }
    </style><style id="mode-styles">@media (prefers-color-scheme:dark) {
    :root {
        --GW-blockquote-background-color: #ddd
    }
    body::before,
    body > * {
        filter: invert(90%)
    }
    body::before {
        content: '';
        width: 100vw;
        height: 100%;
        position: fixed;
        left: 0;
        top: 0;
        background-color: #fff;
        z-index: -1
    }
    img,
    video {
        filter: invert(100%);
    }
    #markdownBody, #mode-selector button {
        text-shadow: 0 0 0 #000
    }
    article > :not(#TOC) a:link {
        text-shadow:
                 0      0 #777,
             .03em      0 #fff,
            -.03em      0 #fff,
                 0  .03em #fff,
                 0 -.03em #fff,
             .06em      0 #fff,
            -.06em      0 #fff,
             .09em      0 #fff,
            -.09em      0 #fff,
             .12em      0 #fff,
            -.12em      0 #fff,
             .15em      0 #fff,
            -.15em      0 #fff
    }
    article > :not(#TOC) blockquote a:link {
        text-shadow:
                 0      0 #777,
             .03em      0 var(--GW-blockquote-background-color),
            -.03em      0 var(--GW-blockquote-background-color),
                 0  .03em var(--GW-blockquote-background-color),
                 0 -.03em var(--GW-blockquote-background-color),
             .06em      0 var(--GW-blockquote-background-color),
            -.06em      0 var(--GW-blockquote-background-color),
             .09em      0 var(--GW-blockquote-background-color),
            -.09em      0 var(--GW-blockquote-background-color),
             .12em      0 var(--GW-blockquote-background-color),
            -.12em      0 var(--GW-blockquote-background-color),
             .15em      0 var(--GW-blockquote-background-color),
            -.15em      0 var(--GW-blockquote-background-color)
    }
    #logo img {
        filter: none;
    }
    #mode-selector {
        opacity: 0.6;
    }
    #mode-selector:hover {
        background-color: #fff;
    }
}</style></head>
<body class="siteIndex">
<main>
<div id="sidebar">
<a id="logo" href="/index" title="index: categorized list of articles">
<img alt="Logo [the logo is a Gothic/Fraktur blackletter capital G]" src="/images/logo-smooth.svg">
</a>
<div id="sidebar-links">
<a class="site" href="/About" title="Site ideals, source, content, traffic, examples, license">Site</a>
<a class="links" href="/Links" title="Who am I online, what have I done, what am I like? Contact information; sites I use; things I've worked on">Me</a>
<a class="new" href="/Changelog" title="What's new or updated">New:</a>
<a class="mail" href="https://gwern.substack.com/" title="Monthly mailing list: newsletter signup form"><i>mail</i></a>
<a class="r_gwern" href="https://www.reddit.com/r/gwern/" title="Gwern subreddit: link-sharing &amp; commentary (the links typically are included in the monthly newsletter)"><em><code>/r/gwern</code></em></a>
<a class="patreon" href="https://www.patreon.com/gwern" title="Link to Patreon donation profile to support my writing">support on<br><span>PATREON</span></a>
</div>
</div>
<article>
<div id="markdownBody">
<div id="abstract">
<p><span class="smallcaps">This is the website</span> of <strong>Gwern Branwen</strong>. I write about psychology, statistics, and technology; I am best known for work on the <a href="./tags/Silk-Road" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: Silk Road">darknet markets</a> &amp; <a href="./tags/Bitcoin" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: Bitcoin">Bitcoin</a>, blinded self-<a href="./tags/experiments" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: experiments">experiments</a> &amp; Quantified Self analyses, <a href="./tags/DNB" class="docMetadata" data-popup-author="Gwern Branwen" data-popup-abstract="All gwern.net pages for this Tag: DNB">dual n-back</a> &amp; <a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">spaced repetition</a>, and <a href="./Modafinil">modafinil</a>.</p>
<p>For information about my siteâs philosophy, method, traffic statistics, and implementation, see the <em><a href="./About" class="docMetadata" data-popup-title="About This Website" data-popup-author="Gwern Branwen" data-popup-date="01 Oct 2010" data-popup-abstract="Meta page describing gwern.net site ideals of stable long-term essays which improve over time; technical decisions using Markdown and static hosting; idea sources and writing methodology; metadata definitions; semi-annual web traffic statistics; copyright license">About page</a></em>; for information about myself, my use of other websites, and contact information, see the <em><a href="./Links" class="docMetadata" data-popup-title="Links" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Who am I online &amp; what have I done? Contact information; sites I use; computers and software tools; things I've worked on; psychological profiles">Links page</a></em>; for information about new pages, see the <em><a href="./Changelog" class="docMetadata" data-popup-title="Changelog" data-popup-author="Gwern Branwen" data-popup-date="15 Sep 2013" data-popup-abstract="<p>This page is a changelog for <code>gwern.net</code>: a monthly reverse chronological list of recent major writings/changes/additions.</p><p>Following my writing can be a little difficult because it is often so incremental. So every month, in addition to my regular <a href=&quot;https://www.reddit.com/r/gwern/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;/r/gwern subreddit&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2018-10-01&quot; data-popup-abstract=&quot;A subreddit for posting links of interest and also for announcing updates to gwern.net (which can be &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/.rss&amp;quot;&amp;gt;used as a RSS feed&amp;lt;/a&amp;gt;). Submissions are categorized similar to the monthly newsletter and typically will be collated there.&quot;>/r/Gwern</a> subreddit submissions, I write up reasonably-interesting changes and <a href=&quot;https://gwern.substack.com/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter (TinyLetter subscription page)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews. You can also browse &amp;lt;a href=&amp;quot;https://www.gwern.net/tags/newsletter&amp;quot;&amp;gt;the archives since December 2013&amp;lt;/a&amp;gt;.&quot;>send it out to the mailing list</a> in addition to a compilation of links &amp;amp; reviews (<a href=&quot;./tags/newsletter&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Gwern.net newsletter archives&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2013-12-01&quot; data-popup-abstract=&quot;Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I&amp;#39;ve worked on that month (the same as the &amp;lt;a href=&amp;quot;https://www.gwern.net/Changelog&amp;quot;&amp;gt;changelog&amp;lt;/a&amp;gt;), collations of links or discussions from &amp;lt;a href=&amp;quot;https://www.reddit.com/r/gwern/&amp;quot;&amp;gt;my subreddit&amp;lt;/a&amp;gt;, and book/movie reviews.)&quot;>archives</a>).</p>">Changelog</a></em>; to receive updates, news, &amp; reviews, <em><a href="https://gwern.substack.com/" class="docMetadata" data-popup-title="Gwern.net newsletter (TinyLetter subscription page)" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Subscription page for the monthly gwern.net newsletter. There are monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews. You can also browse <a href=&quot;https://www.gwern.net/tags/newsletter&quot;>the archives since December 2013</a>.">subscribe to the newsletter</a></em> (<a href="./tags/newsletter" class="docMetadata" data-popup-title="Gwern.net newsletter archives" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews.)">archives</a>).</p>
</div>
<section>
<h1 id="newest"><a href="/Changelog" title="Newest items (last 10 from Changelog)">Newest</a></h1>
<ul>
<li><p><a href="https://www.gwern.net/newsletter/2020/05" class="docMetadata" data-popup-title="May 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="May 2020 gwern.net newsletter with anime GAN updates, links on AI scaling, discussion of GPT-3, and 1 book review.">May 2020 newsletter</a></p></li>
<li><p><a href="https://www.gwern.net/Faces#danbooru2019e621-256px-biggan" class="docMetadata" data-popup-title="Danbooru2019+e621 256px BigGAN Model Release" data-popup-author="Tensorfork (Gwern Branwen, Shawn Presser et al)" data-popup-date="2020-05-28" data-popup-abstract="<p>Release of a 256px Big<span class=&quot;smallcaps-auto&quot;>GAN</span> model trained on Danbooru2019 &amp;amp; e621. This is a prototype model testing our ability to train a Big<span class=&quot;smallcaps-auto&quot;>GAN</span> stably for hundreds of thousands of iterations on a <span class=&quot;smallcaps-auto&quot;>TPU</span>-256 pod on 3 million+ anime/illustration images. While the generated samples are far from âphotorealisticâ, they serve as proof of concept thatâunlike our failed Style<span class=&quot;smallcaps-auto&quot;>GAN</span> 2 scaling experimentsâBig<span class=&quot;smallcaps-auto&quot;>GAN</span> can successfully model anime images with great generality, and that we can potentially scale up to 512px or even 1024px and match the DeepMind ImageNet Big<span class=&quot;smallcaps-auto&quot;>GAN</span> for quality.</p>">Prototype anime GAN</a>; <a href="https://www.gwern.net/Crops#danbooru2019-figures" class="docMetadata" data-popup-title="Danbooru2019 Figures: A Large-Scale Anime Character Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="2020-05-31" data-popup-abstract="<p>The <a href=&quot;/Danbooru2019&quot;>Danbooru2019</a> Figures dataset is a large-scale character anime illustration dataset of <em>n</em>=855,880 images (248GB; minimum width 512px) cropped from Danbooru2019 using the <a href=&quot;https://github.com/jerryli27/AniSeg/&quot;>AniSeg</a> anime character detection model. The images are cropped to focus on a single characterâs entire visible body, extending âportraitâ crops to âfigureâ crops. This is useful for tasks focusing on individual characters, such as character classification or for generative tasks (a corpus for weak models like Style<span class=&quot;smallcaps-auto&quot;>GAN</span>, or data augmentation for Big<span class=&quot;smallcaps-auto&quot;>GAN</span>).</p>">Danbooru2019 Figures dataset</a></p></li>
<li><p><a href="https://www.gwern.net/newsletter/2020/04" class="docMetadata" data-popup-title="April 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="April 2020 gwern.net newsletter with links on music generation, data augmentation, terrorism, Dormin, and 1 documentary review.">April 2020 newsletter</a></p></li>
<li><p><a href="https://www.gwern.net/GPT-2-music#generating-midi-with-10k30k-context-windows" class="docMetadata" data-popup-title="Generating MIDI Music With GPT-2: Generating MIDI by converting to ABC and expanding the GPT-2 context windowâworks, if only just" data-popup-author="Gwern Branwen" data-popup-date="2020-04-25" data-popup-abstract="<p>To expand the <span class=&quot;smallcaps-auto&quot;>ABC</span> <span class=&quot;smallcaps-auto&quot;>GPT-2</span> model to cover a wider variety of musical genres, I turn to the next-most compact widespread music encoding format: <strong><span class=&quot;smallcaps-auto&quot;>MIDI</span></strong>. There are hundreds of thousands of <span class=&quot;smallcaps-auto&quot;>MIDI</span>s which can be <a href=&quot;https://en.wikipedia.org/wiki/Decompiler&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Decompiler&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p>A <b>decompiler</b> is a computer program that takes an executable file as input, and attempts to create a high level source file which can be recompiled successfully. It is therefore the opposite of a compiler, which takes a source file and makes an executable. Decompilers are usually unable to perfectly reconstruct the original source code, and as such, will frequently produce obfuscated code. Nonetheless, decompilers remain an important tool in the reverse engineering of computer software.</p>&quot; title=&quot;!Wikipedia: Decompiler&quot;>decompiled</a> to <span class=&quot;smallcaps-auto&quot;>ABC</span> format, averaging ~10k <span class=&quot;smallcaps-auto&quot;>BPE</span>sâwithin <span class=&quot;smallcaps-auto&quot;>GPT-2-117M</span>âs feasible context window when trained on <span class=&quot;smallcaps-auto&quot;>TPU</span>s (which permit training of context windows up to 30k wide).</p><p>We compile the <span class=&quot;smallcaps-auto&quot;>ABC</span> from before and 2 large <span class=&quot;smallcaps-auto&quot;>MIDI</span> datasets, and convert to <span class=&quot;smallcaps-auto&quot;>ABC</span>, yielding ~453k usable <span class=&quot;smallcaps-auto&quot;>ABC-MIDI</span> musical files (~5.1GB of text). We trained JanuaryâApril 2020 on our <span class=&quot;smallcaps-auto&quot;>TPU</span> swarm (with many interruptions), achieving a final loss of ~0.2 (underfit).</p><p>Sampling from the final model is hit-or-miss as it is prone to the likelihood repetition trap and it generates instruments one-by-one so it is common for instruments to be cut off or otherwise broken during sampling (indicating that <em>sampling</em> is increasingly a bigger problem than <em>training</em> for long-range sequence modeling). However, successful pieces are possible, and are musically far more diverse than the folk <span class=&quot;smallcaps-auto&quot;>ABC</span> corpus, with many pleasingly complex samples.</p>">Generating <span class="smallcaps-auto">MIDI</span> Music With <span class="smallcaps-auto">GPT-2</span></a></p></li>
<li><p><a href="./newsletter/2020/03" class="docMetadata" data-popup-title="March 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="March 2020 gwern.net newsletter with links on pandemics, politics, DL; one anime review.">March 2020 newsletter</a></p></li>
 <li><p><a href="https://www.gwern.net/Order-statistics#probability-of-bivariate-maximum" class="docMetadata" data-popup-title="Probability of Bivariate Maximum" data-popup-author="Gwern Branwen" data-popup-date="2020-03-11" data-popup-abstract="<p>Given a sample of <em>n</em> pairs of 2 normal variables A &amp;amp; B which are correlated <em>r</em>, what is the probability <em>P</em><sub>max</sub> that the maximum on the first variable A is also the maximum on the second variable B? This is analogous to many testing or screening situations, such as employee hiring (âwhat is the probability the top-scoring applicant on the first exam is the top-scorer on the second as well?â) or athletic contests (âwhat is the probability the current world champ will win the next championship?â).</p><p>Order statistics has long proven that asymptotically, <em>P</em><sub>max</sub> approaches <math display=&quot;inline&quot; xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><semantics><mfrac><mn>1</mn><mi>n</mi></mfrac><annotation encoding=&quot;application/x-tex&quot;></annotation></semantics></math>. Exact answers are hard to find, but confirm the asymptotics; the closest that exists is for an approximation &amp;amp; special-case of the Ali-Mikhail-Haq copula: which roughly indicates that <em>r</em> functions as a constant factor boost in <em>P</em><sub>max</sub>, and the boost from <em>r</em> fades out as <em>n</em> increases.</p><p>As long as <em>r</em>â 1, âthe tails will come apartâ. <em>n</em> increases the difficult too fast for any fixed <em>r</em> to overcome. This has implications for interpreting extremes and test metrics.</p>">Order Statistics: The Probability of a Double Maximum</a></p></li>
<li><p><a href="./newsletter/2020/02" class="docMetadata" data-popup-title="February 2020 news" data-popup-author="Gwern Branwen" data-popup-date="26 Dec 2019" data-popup-abstract="February 2020 gwern.net newsletter with links on AI scaling and disasters; 1 book, 1 movie, and 2 opera reviews.">February 2020 newsletter</a></p></li>
<li><p><a href="./Subscripts" class="docMetadata" data-popup-title="Subscripts For Citations" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2020" data-popup-abstract="<p>I propose reviving an old General Semantics notation: borrow from scientific notation and use subscripts like âGwern<sub>2020</sub>â for denoting sources (like citation, timing, or medium). Using subscript indices is flexible, compact, universally technically supported, and intuitive. While (currently) unusual, subscripting might be a useful trick for clearer writing, compared to omitting such information or using standard cumbersome circumlocutions.</p>">On Subscripting Citations</a></p></li>
<li><p><a href="./Danbooru2019" class="docMetadata" data-popup-title="Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2015" data-popup-abstract="<p>Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS <span class=&quot;smallcaps&quot;>COCO</span> fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.</p><p>The best known booru, with a focus on quality, is <a href=&quot;https://danbooru.donmai.us/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Danbooru</a>. We provide a torrent/rsync mirror which contains ~3tb of 3.69m images with 108m tag instances (of 392k defined tags, ~29/image) covering Danbooru from 24 May 2005 through 31 December 2019 (final ID: #3,734,659), providing the image files &amp;amp; a <span class=&quot;smallcaps&quot;>JSON</span> export of the metadata. We also provide a smaller torrent of <span class=&quot;smallcaps&quot;>SFW</span> images downscaled to 512x512px <span class=&quot;smallcaps&quot;>JPG</span>s (295GB; 2,828,400 images) for convenience.</p><p>Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.</p>">Danbooru2019 Released</a></p></li>
<li><p><a href="./GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like GPT-2, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ârealisticâ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpusâs average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the âenvironmentâ for a blackbox DRL algorithm like PPO.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on GPT-2. Having previously used <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>GPT-2 for poetry</a> &amp;amp; <a href=&quot;./GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The model &amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with GPT-2 preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>â10,000 ratings, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model directly model the implied utility of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which optimizes sequences to maximize the reward modelâs output.</p>">Preference Learning <span class="smallcaps-auto">GPT-2</span></a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<p><a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.</p><p><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure></p>">This Waifu Does Not Exist</a><a href="./TWDNE#twdnev3" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist, version 3" data-popup-author="Gwern Branwen" data-popup-date="2020-01-20" data-popup-abstract="Discussion of TWDNEv3, launched January 2020. TWDNEv3 upgrades TWDNEv2 to use 100k anime portraits from an <a href=&quot;/Faces#stylegan-2&quot;>anime portrait StyleGAN 2</a>, an improvement to StyleGAN released in December 2019, which removes the blob artifacts and is generally of somewhat higher visual quality. TWDNEv3 provides images in 3 ranges of diversity, showing off both narrow but high quality samples and more wild samples. It replaces the StyleGAN 1 faces and portrait samples.">v3</a></p></li>
<li><p><a href="https://old.reddit.com/r/SubSimulatorGPT2Meta/comments/entfgx/update_upgrading_to_15b_gpt2_and_adding_22_new/" class="docMetadata" data-popup-title="Update: Upgrading to 1.5B GPT-2, and adding 22 new subreddit-bots" data-popup-author="disumbrationist" data-popup-date="2020-01-12" data-popup-abstract="<p>When I originally trained the models in May 2019, Iâd used the 345M version of GPT-2, which at the time was the largest one that OpenAI had publicly released. Last November, however, OpenAI <a href=&quot;https://openai.com/blog/gpt-2-1-5b-release/&quot;>finally released the full 1.5 billion parameter model</a>.</p><p>The 1.5B model requires much more memory to fine-tune than the 345M, so I was initially having a lot of difficulty getting it to work on Colab. Thankfully, I was contacted by <a href=&quot;https://www.reddit.com/u/gwern&quot;>/u/gwern</a> (<a href=&quot;https://www.patreon.com/gwern&quot;>hereâs his Patreon</a>) and Shawn Presser (<a href=&quot;https://www.reddit.com/u/shawwwn&quot;>/u/shawwwn</a>), who very generously offered to do the fine-tuning themselves if I provided them with the dataset. This training took about 2 weeks, and apparently required <a href=&quot;https://twitter.com/gwern/status/1215005375407112193&quot;>around $70K worth of TPU credits</a>, so in hindsight this upgrade definitely wouldnât have been possible for me to do myself, without their assistance.</p><p>Based on my tests of the new model so far, Iâm pretty happy with the quality, and IMO it is noticeably more coherent than the 345M version.</p><p>One thing that I should point out about the upgrade is that the original 345M models had been separately fine-tuned for each subreddit individually (i.e. there were 108 separate models), whereas the upgraded one is just a single 1.5B model that has been fine-tuned using a combined dataset containing the comments/submissions from <em>all</em> the subreddits that I scraped. The main reason for this decision is simply that it would not have been feasible to train ~100 separate 1.5B models. Also, there may have been benefits from transfer learning across subreddits, which wouldnât occur with separate models.</p><p>â¦Here is the full list of new bots to be added: /r/capitalismvsocialism Â¶ /r/chess Â¶ /r/conlangs Â¶ /r/dota2 Â¶ /r/etymology Â¶ /r/fiftyfifty Â¶ /r/hobbydrama Â¶ /r/markmywords Â¶ /r/moviedetails Â¶ /r/neoliberal Â¶ /r/obscuremedia Â¶ /r/recipes Â¶ /r/riddles Â¶ /r/stonerphilosophy Â¶ /r/subsimulatorgpt2 Â¶ /r/subsimulatorgpt2meta Â¶ /r/tellmeafact Â¶ /r/twosentencehorror Â¶ /r/ukpolitics Â¶ /r/wordavalanches Â¶ /r/wouldyourather Â¶ /r/zen</p>">GPT-2 Subreddit Simulator</a></p></li>
<li><p><a href="./Search#case-studies" class="docMetadata" data-popup-title="Internet Search Tips: 14 Case Studies" data-popup-author="Gwern Branwen" data-popup-date="2020-01-21" data-popup-abstract="Followup section to the article covering how to search the Internet effectively: 14 case studies of challenging Internet searches drawn from the past 10 years. I present the problem, and step through the process of finding it, and describe my tacit knowledge and implicit strategies. These case studies hopefully make the prior tips more understandable by showing them off in practice.">14 Internet Search Case Studies</a></p></li>
<li><a href="./GPT-2-music" class="docMetadata" data-popup-title="GPT-2 Folk Music" data-popup-author="Gwern Branwen" data-popup-date="1 Nov 2019" data-popup-abstract="<p>In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp; copying of patterns, without problems with the hidden-state bottleneck.</p><p>I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after <a href=&quot;#spaceless-model&quot;>fixing that</a>, I successfully trained it on <em>n</em>=205,304 ABC music pieces taken from The Session &amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.</p><p>The model &amp;amp; dataset are available for download, and I provide for listening selected <a href=&quot;#samples&quot;>music samples</a> as well as medleys of random samples from throughout training.</p>">GPT-2 Folk Music</a></li>
<li><a href="./Causality#overview-the-current-situation" class="docMetadata" data-popup-title="Causality in the Social Sciences: Overview: The Current Situation" data-popup-author="Gwern Branwen" data-popup-date="2019-12-05" data-popup-abstract="<p>Here is how I currently understand the relationship between correlation and causality, and the collective findings of meta-scientific research:</p><ol type=&quot;1&quot;><li><p><a href=&quot;https://www.gwern.net/Replication&quot;><em>The Replication Crisis</em></a>: a shockingly large fraction of psychological research and other fields is simple random noise which cannot be replicated.</p></li><li><p><a href=&quot;https://www.gwern.net/Everything&quot;><em>Everything Is Correlated</em></a>: when we systematically measure many variables at large scale with large <em>n</em>, we find that âeverything is correlatedââeven things which seem to have no causal relationship whatsoever.</p></li><li><p><a href=&quot;https://www.gwern.net/docs/sociology/1987-rossi&quot;><em>The Metallic Laws</em></a>: empirically, most efforts to change human behavior and sociology and economics and education fail in randomized evaluation and the mean effect size of experiments in meta-analyses typically approaches zero, despite promising correlations.</p></li><li><p><a href=&quot;https://www.gwern.net/Correlation&quot;><em>Correlation â  Causation</em></a>: so, we live in a world where research manufactures many spurious results and, even once we see through the fake findings, finding a correlation is meaningless because everything is correlated to begin with and accordingly, they are little better than experimenting at random, which doesnât work well either.</p><p>But <em>why</em> is correlation â  causation?</p></li><li><p><a href=&quot;https://www.gwern.net/Causality#what-a-tangled-net-we-weave-when-first-we-practice-to-believe&quot;><em>Dense Causal Graphs</em></a>: because, if we write down a causal graph consistent with âeverything is correlatedâ and the empirical facts of average null effects + unpredictive correlations, this implies that all variables are part of enormous dense causal graphs where each variable is connected to several others.</p></li><li><p><a href=&quot;https://www.gwern.net/Causality#heuristics-biases&quot;><em>Incorrect Intuitions</em></a>: This inequality between observable correlations and actual useful causal manipulability merely grows with larger networks, and causal networks in fields like economics or biology are far more complex than those in more ordinary everyday fields like âcatching a ballâ.</p><p>Our intuitions, formed in simple domains designed to have sparse causal networks (it would be bad if balls could make you do random things! your brain is carefully designed to control the influence of any outside forces &amp;amp; model the world as simple for planning purposes), turn out to be profoundly misleading in these other domains.</p></li><li><p><em>No, Really, Correlation â  Causation</em>: This cognitive bias is why correlation â  causation is so difficult to internalize and accept, and honored primarily in the breach even by sophisticated researchers, and is why randomized experiments are historically late developed, neglected, counterintuitive, and criticized when run despite routinely debunking conventional wisdom of experts in almost every field.</p></li></ol>">Correlation &amp; Causality</a></li>
<li><a href="./Hydrocephalus" class="docMetadata" data-popup-title="Hydrocephalus and Intelligence: The Hollow Men" data-popup-author="Gwern Branwen" data-popup-date="28 July 2015" data-popup-abstract="<p>Hydrocephalus is a damaging brain disorder where fluids compress the brain, sometimes drastically decreasing its volume. While often extremely harmful or life-threatening when untreated, some people with severe compression nevertheless are relatively normal, and in one case (Lorber) they have been claimed to have IQs as high as 126 with a brain volume 5% of normal brains. A few of these case studies have been used to argue the extraordinary claim that brain volume has little or nothing to do with intelligence; authors have argued that hydrocephalus suggests enormous untapped cognitive potential which are tapped into rarely for repairs and can boost intelligence on net, or that intelligence/consciousness are non-material or tapping into ESP.</p><p>I point out why this claim is almost certainly untrue because it predicts countless phenomena we never observe, and investigate the claimed examples in more detail: the cases turn out to be suspiciously unverifiable (Lorber), likely fraudulent (Oliveira), or actually low intelligence (Feuillet). It is unclear if high-functioning cases of hydrocephalus even have less brain mass, as opposed to lower proxy measures like brain volume.</p><p>I then summarize anthropologist John Hawksâs criticisms of the original hydrocephalus author: his brain imaging data could not have been as precise as claimed, he studied a selective sample, the story of the legendary IQ 126 hydrocephalus patient raises questions as to how normal or intelligent he really was, and hydrocephalus in general appears to be no more anomalous or hard-to-explain than many other kinds of brain injuries, and in a comparison, hemispherectomies, removing or severing a hemisphere, has produced no anomalous reports of above-average intelligence (just deficits), though they ought to be just the same in terms of repairs or ESP.</p><p>That hydrocephalus cases can reach roughly normal levels of functioning, various deficits aside, can be explained by brain size being one of several relevant variables, brain plasticity enabling cognitive flexibility &amp;amp; recovery from gradually-developing conditions, and overparameterization giving robustness to damage and poor environments, and learning ability. The field of deep learning has observed similar phenomenon in training of artificial neural networks. This is consistent with Lorberâs original contention that the brain was more robust, and hydrocephalus was more treatable, than commonly accepted, but does not support any of the more exotic interpretations since put on his findings.</p><p>In short, there is little anomalous to explain, and standard brain-centric accounts appear to account for existing verified observations without much problem or resort to extraordinary claims.</p>">Hydrocephalus and Intelligence</a></li>
</ul>
</section>
<section>
<h1 id="most-popular"><a href="#most-popular" title="Link to section: 'Most popular'">Popular</a></h1>
<ul>
<li><p><a href="./Silk-Road" class="docMetadata" data-popup-title="Silk Road 1: Theory &amp; Practice" data-popup-author="Gwern Branwen" data-popup-date="11 Jul 2011" data-popup-abstract="<p>The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.</p>">Silk Road 1: Theory &amp; Practice</a></p></li>
<li><p><a href="./DNM-archives" class="docMetadata" data-popup-title="Darknet Market Archives (2013-2015)" data-popup-author="Gwern Branwen" data-popup-date="1 Dec 2013" data-popup-abstract="<p>Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.</p><p>From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>usage</a>, <a href=&quot;./DNM-survival&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market mortality risks&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;30 Oct 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;Silk Road 1&amp;lt;/a&amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;lt;/p&amp;gt;&quot;>lifetimes/characteristics</a>, &amp;amp; <a href=&quot;./DNM-arrests&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor DNM-related arrests, 2011-2015&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;14 Jul 2012&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;lt;/p&amp;gt;&quot;>legal riskiness</a>; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp; documents related to the DNMs as I could.</p><p>This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp; 37+ related forums, representing &amp;lt;4,438 mirrors, and is available for any research.</p><p>This page documents the download, contents, interpretation, and technical methods behind the scrapes.</p>">Darknet Market Archives (2013â2015)</a></p></li>
<li><p><a href="./DNM-arrests" class="docMetadata" data-popup-title="Tor DNM-related arrests, 2011-2015" data-popup-author="Gwern Branwen" data-popup-date="14 Jul 2012" data-popup-abstract="<p>I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.</p>">Darknet Market Arrests (2011â2015)</a></p></li>
<li><p><a href="./Modafinil" class="docMetadata" data-popup-title="Modafinil" data-popup-author="Gwern Branwen" data-popup-date="20 Feb 2009" data-popup-abstract="Modafinil is a prescription stimulant drug. I discuss informally, from a cost-benefit-informed perspective, the research up to 2015 on modafinil's cognitive effects, the risks of side-effects and addiction/tolerance and law enforcement, and give a table of current grey-market suppliers and discuss how to order from them.">Modafinil</a></p></li>
<li><p><a href="./LSD-microdosing" class="docMetadata" data-popup-title="LSD microdosing RCT" data-popup-author="Gwern Branwen" data-popup-date="20 Aug 2012" data-popup-abstract="<p>Some early experimental studies with LSD suggested that doses of LSD too small to cause any noticeable effects may improve mood and creativity. Prompted by recent discussion of this claim and the purely anecdotal subsequent evidence for it, I decided to run a well-powered randomized blind trial of 3-day LSD microdoses from September 2012 to March 2013. No beneficial effects reached statistical-significance and there were worrisome negative trends. LSD microdosing did not help me.</p>">LSD microdosing self-experiment</a></p></li>
<li><p><a href="./Zeo" class="docMetadata" data-popup-title="Zeo sleep self-experiments" data-popup-author="Gwern Branwen" data-popup-date="28 Dec 2010" data-popup-abstract="<p>I discuss my beliefs about Quantified Self, and demonstrate with a series of <a href=&quot;https://en.wikipedia.org/wiki/single-subject_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Single-subject design&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In design of experiments, &amp;lt;b&amp;gt;single-subject design&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;single-case research design&amp;lt;/b&amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: single-subject design&quot;>single-subject design</a> self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:</p><ol type=&quot;1&quot;><li>the Zeo headband is wearable long-term</li><li><a href=&quot;#melatonin&quot;>melatonin</a> improves my sleep</li><li><a href=&quot;#one-legged-standing&quot;>one-legged standing</a> does little</li><li>Vitamin D <a href=&quot;#vitamin-d&quot;>at night</a> damages my sleep &amp;amp; Vitamin D in morning does not affect my sleep</li><li>potassium (<a href=&quot;#potassium-day-use&quot;>over the day</a> but not so much <a href=&quot;#potassium-morning-use&quot;>the morning</a>) damages my sleep and does not improve my mood/productivity</li><li>small quantities of <a href=&quot;#alcohol&quot;>alcohol</a> appear to make little difference to my sleep quality</li><li>I may be better off <a href=&quot;#timing&quot;>changing my sleep timing</a> by waking up somewhat earlier &amp;amp; going to bed somewhat earlier</li><li><a href=&quot;#lithium&quot;>lithium orotate</a> does not affect my sleep</li><li><a href=&quot;#redshiftf.lux&quot;>Redshift</a> causes me to go to bed earlier</li><li><a href=&quot;#zma&quot;>ZMA</a>: inconclusive results slightly suggestive of benefits</li></ol>">Zeo sleep self-experiments</a></p></li>
<li><p><a href="./DNB-FAQ" class="docMetadata" data-popup-title="Dual N-Back FAQ" data-popup-author="Gwern Branwen" data-popup-date="25 Mar 2009" data-popup-abstract="A compendium of DNB, WM, IQ information up to 2015">Dual N-Back <span class="smallcaps-auto">FAQ</span></a></p></li>
<li><p><a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">Spaced repetition</a></p></li>
<li><p><a href="./Death-Note-Anonymity" class="docMetadata" data-popup-title="Death Note: L, Anonymity &amp; Eluding Entropy" data-popup-author="Gwern Branwen" data-popup-date="04 May 2011" data-popup-abstract="<p>In the manga <em>Death Note</em>, the protagonist Light Yagami is given the supernatural weapon âDeath Noteâ which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. <em>Death Note</em> is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of Lâs process from the perspective of computer security, cryptography, and information theory, to quantify Lightâs initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest.</p><ol type=&quot;1&quot;><li>Mistake 1: Lightâs fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)</li><li>Mistake 2: Worse, the deaths are non-random in other waysâthey tend to occur at particular times! Just the scheduling of deaths cost Light 6 bits of anonymity</li><li>Mistake 3: Lightâs third mistake was reacting to the blatant provocation of Lind L. Tailor. L narrowed his target down to 1/3 the original Japanese population, for a gain of ~1.6 bits.</li><li>Mistake 4: Lightâs fourth mistake was to use confidential police information stolen using his policeman fatherâs credentials. This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!</li><li>Mistake 5: If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.</li><li>Endgame: At this point in the plot, L resorts to direct measures and enters Lightâs life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.</li></ol><p>Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.</p><p><em>(Note: This essay assumes a familiarity with the early plot of <em><a href=&quot;https://en.wikipedia.org/wiki/Death_Note&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note&quot;>Death Note</a></em> and <a href=&quot;https://en.wikipedia.org/wiki/Light_Yagami&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Light Yagami&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Light Yagami&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a fictional character and the main protagonist of the manga series &amp;lt;i&amp;gt;Death Note&amp;lt;/i&amp;gt;, created by Tsugumi Ohba and Takeshi Obata. He is portrayed as an accomplished yet bored teen genius who finds the Death Note, a supernatural notebook that allows the user to kill anyone by knowing their name and face, after it is dropped by the Shinigami Ryuk. In an effort to create a utopia, Light eventually uses the notebook to murder criminals as the vigilante &amp;lt;b&amp;gt;Kira&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; (ã­ã©)&amp;lt;/span&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Light Yagami&quot;>Light Yagami</a>; if you are unfamiliar with it, see my <a href=&quot;./Death-Note-Ending&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&amp;#39;s Ending&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;29 Sep 2008&quot; data-popup-abstract=&quot;Ambiguous ending means even the victor is unclear; who was right?&quot;><em>Death Note</em> Ending</a> essay or consult <a href=&quot;https://en.wikipedia.org/wiki/Death_Note#Plot_summary&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note#Plot_summary&quot;>Wikipedia</a> or <a href=&quot;http://deathnote.wikia.com/wiki/Rules_of_the_Death_Note&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Rules of the Death Note&quot;>read the DN rules</a>.)</em></p>"><em>Death Note</em>: L, Anonymity &amp; Entropy</a></p></li>
<li><p><a href="./Complement" class="docMetadata" data-popup-title="Laws of Tech: Commoditize Your Complement" data-popup-author="Gwern Branwen" data-popup-date="17 March 2018" data-popup-abstract="<p>Joel Spolsky in 2002 identified a major pattern in technology business &amp;amp; economics: the pattern of <q>âcommoditizing your complementâ</q>, an alternative to vertical integration, where companies seek to secure a chokepoint or quasi-monopoly in products composed of many necessary &amp;amp; sufficient layers by dominating one layer while fostering so much competition in another layer above or below its layer that no competing monopolist can emerge, prices are driven down to marginal costs elsewhere in the stack, total price drops &amp;amp; increases demand, and the majority of the consumer surplus of the final product can be diverted to the quasi-monopolist. A classic example is the commodification of PC hardware by the Microsoft OS monopoly, to the detriment of IBM &amp;amp; benefit of MS.</p><p>This pattern explains many otherwise odd or apparently self-sabotaging ventures by large tech companies into apparently irrelevant fields, such as the high rate of releasing open-source contributions by many Internet companies or the intrusion of advertising companies into smartphone manufacturing &amp;amp; web browser development &amp;amp; statistical software &amp;amp; fiber-optic networks &amp;amp; municipal WiFi &amp;amp; radio spectrum auctions &amp;amp; DNS (Google): they are pre-emptive attempts to commodify another company elsewhere in the stack, or defenses against it being done to them.</p>">Commoditize Your Complement</a></p></li>
<li><p><a href="./Google-shutdowns" class="docMetadata" data-popup-title="Predicting Google closures" data-popup-author="Gwern Branwen" data-popup-date="28 Mar 2013" data-popup-abstract="<p>Prompted by the shutdown of Google Reader, I ponder the evanescence of online services and wonder what is the risk of them disappearing. I collect data on <a href=&quot;#sources&quot;>350 Google products</a> launched before March 2013, looking for <a href=&quot;#variables&quot;>variables predictive of mortality</a> (web hits, service vs software, commercial vs free, FLOSS, social networking, and internal vs acquired). Shutdowns are unevenly distributed over the calendar year or Googleâs history. I use logistic regression &amp;amp; survival analysis (which can deal with right-censorship) to <a href=&quot;#modeling&quot;>model the risk of shutdown over time</a> and examine correlates. The logistic regression indicates socialness, acquisitions, and lack of web hits predict being shut down, but the results may not be right. The survival analysis finds a median lifespan of 2824 days with a roughly Type III survival curve (high early-life mortality); a Cox regression finds similar results as the logistic - socialness, free, acquisition, and long life predict lower mortality. Using the best model, I <a href=&quot;#predictions&quot;>make predictions</a> about probability of shutdown of the most risky and least risky services in the next 5 years (up to March 2018). (All data &amp;amp; R source code is provided.)</p>">Google survival analysis</a></p></li>
<li><p><a href="./Ads" class="docMetadata" data-popup-title="Banner Ads Considered Harmful" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2017" data-popup-abstract="<p>One source of complexity &amp;amp; JavaScript use on <code>gwern.net</code> is the use of Google AdSense advertising to insert banner ads. In considering design &amp;amp; usability improvements, removing the banner ads comes up every time as a possibility, as readers do not like ads, but such removal comes at a revenue loss and itâs unclear whether the benefit outweighs the cost, suggesting I run an A/B experiment. However, ads might be expected to have broader effects on traffic than individual page reading times/bounce rates, affecting <em>total</em> site traffic instead through long-term effects on or spillover mechanisms between readers (eg social media behavior), rendering the usual A/B testing method of per-page-load/session randomization incorrect; instead it would be better to analyze total traffic as a time-series experiment.</p><p>Design: A decision analysis of revenue vs readers yields an maximum acceptable total traffic loss of ~3%. Power analysis of historical <code>gwern.net</code> traffic data demonstrates that the high autocorrelation yields low statistical power with standard tests &amp;amp; regressions but acceptable power with ARIMA models. I design a long-term Bayesian <code>ARIMA(4,0,1)</code> time-series model in which an A/B-test running JanuaryâOctober 2017 in randomized paired 2-day blocks of ads/no-ads uses client-local JS to determine whether to load &amp;amp; display ads, with total traffic data collected in Google Analytics &amp;amp; ad exposure data in Google AdSense. The A/B test ran from 1 January 2017 to 15 October 2017, affecting 288 days with collectively 380,140 pageviews in 251,164 sessions.</p><p>Correcting for a flaw in the randomization, the final results yield a surprisingly large estimate of an expected traffic loss of -9.7% (driven by the subset of users without adblock), with an implied -14% traffic loss if all traffic were exposed to ads (95% credible interval: -13â16%), exceeding my decision threshold for disabling ads &amp;amp; strongly ruling out the possibility of acceptably small losses which might justify further experimentation.</p><p>Thus, banner ads on <code>gwern.net</code> appear to be harmful and AdSense has been removed. If these results generalize to other blogs and personal websites, an important implication is that many websites may be harmed by their use of banner ad advertising without realizing it.</p>">Banner Ads Considered Harmful</a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.<br><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure>">This Waifu Does Not Exist</a> (<a href="./TWDNE" class="docMetadata" data-popup-title="This Waifu Does Not Exist" data-popup-author="Gwern Branwen" data-popup-date="19 Feb 2019" data-popup-abstract="<p>Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, <a href=&quot;https://www.thiswaifudoesnotexist.net/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;ThisWaifuDoesNotExist.net&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-02-19&quot; data-popup-abstract=&quot;&amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;&amp;lt;code&amp;gt;ThisWaifuDoesNotExist.net&amp;lt;/code&amp;gt;&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;https://www.gwern.net/TWDNE&amp;quot;&amp;gt;TWDNE&amp;lt;/a&amp;gt;) is a static website which uses JS to display random &amp;lt;a href=&amp;quot;https://www.gwern.net/Faces&amp;quot;&amp;gt;anime faces generated by StyleGAN&amp;lt;/a&amp;gt; neural networks, along with &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-2&amp;quot;&amp;gt;GPT-2&amp;lt;/a&amp;gt;-generated &amp;#39;anime plot summaries&amp;#39;.&amp;lt;br&amp;gt;&amp;lt;figure&amp;gt;&amp;lt;img src=&amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;quot; alt=&amp;quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;quot; /&amp;gt;&amp;lt;figcaption&amp;gt;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&quot;><q>âThis Waifu Does Not Existâ</q></a> for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or <a href=&quot;https://klimaleksus.github.io/FindTwin/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&quot;>online</a> <a href=&quot;https://github.com/darabos/high-five-trading&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Action stock exchange game for Repl.it Game Jam 2019&quot;>games</a>, uploaded to Pixiv, and used in a research paper (<a href=&quot;https://arxiv.org/abs/1904.01774&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot; data-popup-author=&quot;Atsuhiro Noguchi, Tatsuya Harada&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&quot; title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot;>Noguchi &amp;amp; Harada 2019</a>). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, <a href=&quot;https://waifulabs.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Waifu Labs</a>, which generates even better anime faces than my StyleGAN results.</p>">details</a>)</p></li>
<li><p><a href="./Tanks" class="docMetadata" data-popup-title="The Neural Net Tank Urban Legend" data-popup-author="Gwern Branwen" data-popup-date="20 Sep 2011" data-popup-abstract="<p>A cautionary tale in artificial intelligence tells about researchers training an neural network (NN) to detect tanks in photographs, succeeding, only to realize the photographs had been collected under specific conditions for tanks/non-tanks and the NN had learned something useless like time of day. This story is often told to warn about the limits of algorithms and importance of data collection to avoid <q>âdataset biasâ</q>/<q>âdata leakageâ</q> where the collected data can be solved using algorithms that do not generalize to the true data distribution, but the tank story is usually never sourced.</p><p>I collate many extent versions dating back a quarter of a century to 1992 along with two NN-related anecdotes from the 1960s; their contradictions &amp;amp; details indicate a classic âurban legendâ, with a probable origin in a speculative question in the 1960s by Edward Fredkin at an AI conference about some early NN research, which was subsequently classified &amp;amp; never followed up on.</p><p>I suggest that dataset bias is real but exaggerated by the tank story, giving a misleading indication of risks from deep learning and that it would be better to not repeat it but use real examples of dataset bias and focus on larger-scale risks like AI systems optimizing for wrong utility functions.</p>">The Neural Net/Tank Urban Legend</a></p></li>
<li><p><a href="./Self-decrypting-files" class="docMetadata" data-popup-title="Time-lock encryption" data-popup-author="Gwern Branwen" data-popup-date="24 May 2011" data-popup-abstract="<p>In cryptography, it is easy to adjust encryption of data so that one, some, or all people can decrypt it, or some combination thereof. It is not so easy to achieve adjustable decryptability over <em>time</em>, a âtime-lock cryptoâ: for some uses (data escrow, leaking, insurance, last-resort Bitcoin backups etc), one wants data which is distributed only after a certain point in time.</p><p>I survey techniques for time-lock crypto. Proposals often resort to trusted-third-parties, which are vulnerabilities. A better time-lock crypto proposal replaces trusted-third-parties with forcibly serial proof-of-work using number squaring and guaranteeing unlocking not after a certain point in time but after sufficient computation-time has been spent; itâs unclear how well number-squaring resists optimization or shortcuts. I suggest a new time-lock crypto based on chained hashes; hashes have been heavily attacked for other purposes, and may be safer than number-squaring. Finally, I cover obfuscation &amp;amp; witness-encryption which, combined with proof-of-work, can be said to solve time-lock crypto but currently remain infeasible.</p>">Time-lock cryptography</a></p></li>
<li><p><a href="./Archiving-URLs" class="docMetadata" data-popup-title="Archiving URLs" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2011" data-popup-abstract="<p>Links on the Internet last forever or a year, whichever comes first. This is a major problem for anyone serious about writing with good references, as link rot will cripple several percent of all links each year, and compounding.</p><p>To deal with link rot, I present my multi-pronged archival strategy using a combination of scripts, daemons, and Internet archival services: URLs are regularly dumped from both my web browserâs daily browsing and my website pages into an archival daemon I wrote, which pre-emptively downloads copies locally and attempts to archive them in the Internet Archive. This ensures a copy will be available indefinitely from one of several sources. Link rot is then detected by regular runs of <code>linkchecker</code>, and any newly dead links can be immediately checked for alternative locations, or restored from one of the archive sources.</p><p>As an additional flourish, my local archives are <a href=&quot;./Timestamping&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Easy Cryptographic Timestamping of Files&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;4 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to &amp;lt;em&amp;gt;timestamp&amp;lt;/em&amp;gt; it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp;amp; easily timestamp arbitrary files &amp;amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.&amp;lt;/p&amp;gt;&quot;>efficiently cryptographically timestamped using Bitcoin</a> in case forgery is a concern, and I demonstrate a simple compression trick for <a href=&quot;./Archiving-URLs#sort---key-compression-trick&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; compression trick (CLI folklore)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2014-03-03&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Programming folklore notes that one way to get better lossless compression efficiency is to rearrange files inside the archive to group âsimilarâ files together and expose redundancy to the compressor, in accordance with information-theoretical principles. A particularly easy and broadly-applicable way of doing this, which does not require using any unusual formats or tools and is fully compatible with the default archive methods, is to sort the files by &amp;lt;em&amp;gt;filename&amp;lt;/em&amp;gt; and especially file extension. I show how to do this with the standard Unix command-line &amp;lt;code&amp;gt;sort&amp;lt;/code&amp;gt; tool, using the so-called â&amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trickâ, and give examples of the large space-savings possible from my archiving work for personal website mirrors and for making &amp;lt;a href=&amp;quot;/DNM-archives&amp;quot;&amp;gt;darknet market mirror datasets&amp;lt;/a&amp;gt; where the redundancy at the file level is particularly extreme and the &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trick shines compared to the naive approach.&amp;lt;/p&amp;gt;&quot;>substantially reducing sizes of large web archives</a> such as crawls (particularly useful for repeated crawls such as my <a href=&quot;./DNM-archives&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market Archives (2013-2015)&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;1 Dec 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;usage&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;./DNM-survival&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Darknet Market mortality risks&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;amp;lt;a href=&amp;amp;quot;./Silk-Road&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;Silk Road 1: Theory &amp;amp;amp;amp; Practice&amp;amp;quot; data-popup-author=&amp;amp;quot;11 Jul 2011&amp;amp;quot; data-popup-date=&amp;amp;quot;gwern&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;div id=&amp;amp;amp;quot;abstract&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;blockquote&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;/blockquote&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;Silk Road 1&amp;amp;lt;/a&amp;amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;lifetimes/characteristics&amp;lt;/a&amp;gt;, &amp;amp;amp; &amp;lt;a href=&amp;quot;./DNM-arrests&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Tor DNM-related arrests, 2011-2015&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;legal riskiness&amp;lt;/a&amp;gt;; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp;amp; documents related to the DNMs as I could.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp;amp; 37+ related forums, representing &amp;amp;lt;4,438 mirrors, and is available for any research.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This page documents the download, contents, interpretation, and technical methods behind the scrapes.&amp;lt;/p&amp;gt;&quot;>DNM archives</a>).</p>">Archiving <span class="smallcaps-auto">URL</span>s</a></p></li>
<li><p><a href="./Terrorism-is-not-about-Terror" class="docMetadata" data-popup-title="Terrorism Is Not About Terror" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="<p>Statistical analysis of terrorist groupsâ longevity, aims, methods and successes reveal that groups are self-contradictory and self-sabotaging, generally ineffective; common stereotypes like terrorists being poor or ultra-skilled are false. Superficially appealing counter-examples are discussed and rejected. Data on motivations and the dissolution of terrorist groups are brought into play and the surprising conclusion reached: terrorism is a form of socialization or status-seeking.</p>">Terrorism is not about Terror</a></p></li>
<li><p><a href="./The-Melancholy-of-Subculture-Society" class="docMetadata" data-popup-title="The Melancholy of Subculture Society" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2009" data-popup-abstract="Internet links small groups, helping dissolve big groups; good, bad? But a bit sad.">The Melancholy of Subculture Society</a></p></li>
</ul>
</section>
<section>
<h1 id="notable"><a href="#notable" title="Link to section: 'Notable'">Notable</a></h1>
<ul>
<li><a href="./Causality" class="docMetadata" data-popup-title="Why Correlation Usually â  Causation: Causal Nets Cause Common Confounding" data-popup-author="Gwern Branwen" data-popup-date="24 Jun 2014" data-popup-abstract="<p>It is widely understood that statistical correlation between two variables â  causation. But despite this admonition, people are routinely overconfident in claiming correlations to support particular causal interpretations and are surprised by the results of randomized experiments, suggesting that they are biased &amp;amp; systematically underestimating the prevalence of confounds/common-causation. I speculate that in realistic causal networks or DAGs, the number of possible correlations grows faster than the number of possible causal relationships. So confounds really are that common, and since people do not think in DAGs, the imbalance also explains overconfidence.</p>">Why Correlation â  Causation</a></li>
<li><a href="./Embryo-selection" class="docMetadata" data-popup-title="Embryo selection for intelligence" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>With genetic predictors of a phenotypic trait, it is possible to select embryos during an in vitro fertilization process to increase or decrease that trait. Extending the work of <a href=&quot;./docs/iq/2014-shulman.pdf&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;417&quot; title=&quot;Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?&quot;>Shulman &amp;amp; Bostrom 2014</a>/<a href=&quot;https://arxiv.org/abs/1408.3421&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;On the genetic architecture of intelligence and other quantitative  traits&quot; data-popup-author=&quot;Stephen D. H. Hsu&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;How do genes affect cognitive ability or other human quantitative traits such as height or disease risk? Progress on this challenging question is likely to be significant in the near future. I begin with a brief review of psychometric measurements of intelligence, introducing the idea of a &amp;quot;general factor&amp;quot; or g score. The main results concern the stability, validity (predictive power), and heritability of adult g. The largest component of genetic variance for both height and intelligence is additive (linear), leading to important simplifications in predictive modeling and statistical estimation. Due mainly to the rapidly decreasing cost of genotyping, it is possible that within the coming decade researchers will identify loci which account for a significant fraction of total g variation. In the case of height analogous efforts are well under way. I describe some unpublished results concerning the genetic architecture of height and cognitive ability, which suggest that roughly 10k moderately rare causal variants of mostly negative effect are responsible for normal population variation. Using results from Compressed Sensing (L1-penalized regression), I estimate the statistical power required to characterize both linear and nonlinear models for quantitative traits. The main unknown parameter s (sparsity) is the number of loci which account for the bulk of the genetic variation. The required sample size is of order 100s, or roughly a million in the case of cognitive ability.&quot; title=&quot;On the genetic architecture of intelligence and other quantitative traits&quot;>Hsu 2014</a>, I consider the case of human intelligence using SNP-based genetic prediction, finding:</p><ul><li>a meta-analysis of <a href=&quot;https://en.wikipedia.org/wiki/GCTA&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Genome-wide complex trait analysis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Genome-wide complex trait analysis (GCTA) Genome-based restricted maximum likelihood (GREML)&amp;lt;/b&amp;gt; is a statistical method for variance component estimation in genetics which quantifies the total narrow-sense (additive) contribution to a trait&amp;#39;s heritability of a particular subset of genetic variants. This is done by directly quantifying the chance genetic similarity of unrelated individuals and comparing it to their measured similarity on a trait; if two unrelated individuals are relatively similar genetically and also have similar trait measurements, then the measured genetics are likely to causally influence that trait, and the correlation can to some degree tell how much. This can be illustrated by plotting the squared pairwise trait differences between individuals against their estimated degree of relatedness. The GCTA framework can be applied in a variety of settings. For example, it can be used to examine changes in heritability over aging and development.. It can also be extended to analyse bivariate genetic correlations between traits. There is an ongoing debate about whether GCTA generates reliable or stable estimates of heritability when used on current SNP data. The method is based on the outdated and false dichotomy of genes versus the environment. It also suffers from serious methodological weaknesses, such as susceptibility to population stratification.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: GCTA&quot;>GCTA</a> results indicates that SNPs can explain &amp;gt;33% of variance in current intelligence scores, and &amp;gt;44% with better-quality phenotype testing</li><li>this sets an upper bound on the effectiveness of selection: a gain of 9 IQ points when selecting the top embryo out of 10</li><li>the best 2016 polygenic score could achieve a gain of ~3 IQ points when selecting out of 10</li><li>the marginal cost of embryo selection (assuming IVF is already being done) is modest, at $1500 + $200 per embryo, with the sequencing cost projected to drop rapidly</li><li>a model of the IVF process, incorporating number of extracted eggs, losses to abnormalities &amp;amp; vitrification &amp;amp; failed implantation &amp;amp; miscarriages from 2 real IVF patient populations, estimates feasible gains of 0.39 &amp;amp; 0.68 IQ points</li><li>embryo selection is currently unprofitable (mean: -$358) in the USA under the lowest estimate of the value of an IQ point, but profitable under the highest (mean: $6230). The main constraints on selection profitability is the polygenic score; under the highest value, the NPV EVPI of a perfect SNP predictor is $24b and the EVSI per education/SNP sample is $71k</li><li>under the worst-case estimate, selection can be made profitable with a better polygenic score, which would require <em>n</em>&amp;gt;237,300 using education phenotype data (and much less using fluid intelligence measures)</li><li>selection can be made more effective by selecting on multiple phenotype traits: considering an example using 7 traits (IQ/height/BMI/diabetes/ADHD/bipolar/schizophrenia), there is a factor gain over IQ alone; the outperformance of multiple selection remains after adjusting for genetic correlations &amp;amp; polygenic scores and using a broader set of 16 traits. </li></ul>">Embryo selection for intelligence</a></li>
<li><a href="./Mail-delivery" class="docMetadata" data-popup-title="When Should I Check The Mail?" data-popup-author="Gwern Branwen" data-popup-date="21 June 2015" data-popup-abstract="<p>Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual âinterval-censored dataâ. I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (survival analysis with Gaussian regression using <code>survival</code> library), MCMC (Bayesian model in JAGS), and likelihood-free Bayesian inference (custom ABC, using the simulation). This allows estimation of the distribution of mail delivery times. I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking &amp;amp; personal observation, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen. Finally, I define a loss function on mail checks, enabling: a choice of optimal time to check the mailbox to minimize loss (exploitation); optimal time to check to maximize information gain (exploration); Thompson sampling (balancing exploration &amp;amp; exploitation indefinitely), and estimates of the value-of-information of another datapoint (to estimate when to stop exploration and start exploitation after a finite amount of data).</p>">When Should I Check The Mail?</a></li>
<li><a href="./Tool-AI" class="docMetadata" data-popup-title="Why Tool AIs Want to Be Agent AIs: The Power of Agency" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2016" data-popup-abstract="<p>Autonomous AI systems (Agent AIs) trained using <a href=&quot;https://en.wikipedia.org/wiki/reinforcement_learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reinforcement learning&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Reinforcement learning&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;RL&amp;lt;/b&amp;gt;) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reinforcement learning&quot;>reinforcement learning</a> can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved &amp;amp; executed by humans, giving equivalently superintelligent results without the risk.</p><p>I argue that this is not an effective solution for two major reasons. First, because Agent AIs will by definition be better at <em>actions</em> than Tool AIs, giving an economic advantage. Secondly, because Agent AIs will be better at <em>inference &amp;amp; learning</em> than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs will be inferior to Agent AIs in both actions and intelligence, implying use of Tool AIs is a even more highly unstable equilibrium than previously argued, as users of Agent AIs will be able to outcompete them on two dimensions (and not just one).</p>">Why Tool AIs Want to Be Agent AIs</a></li>
<li><a href="./Complexity-vs-AI" class="docMetadata" data-popup-title="Complexity no Bar to AI" data-popup-author="Gwern Branwen" data-popup-date="1 June 2014" data-popup-abstract="<p>Computational complexity theory describes the steep increase in computing power required for many algorithms to solve larger problems; frequently, the increase is large enough to render problems a few times larger totally intractable. Many of these algorithms are used in AI-relevant contexts. It has been argued that this implies that AIs will fundamentally be limited in accomplishing real-world tasks better than humans because they will run into the same computational complexity limit as humans, and so the consequences of developing AI will be small, as it is impossible for there to be any large fast global changes due to human or superhuman-level AIs. I examine the assumptions of this argument and find it neglects the many conditions under which computational complexity theorems are valid and so the argument doesnât work: problems can be solved more efficiently than complexity classes would imply, large differences in problem solubility between humans and AIs is possible, greater resource consumption is possible, the real-world consequences of small differences on individual tasks can be large on agent impacts, such consequences can compound, and many agents can be created; any of these independent objections being true destroys the argument.</p>">Complexity no Bar to AI</a></li>
<li><a href="./Faces" class="docMetadata" data-popup-title="Making Anime Faces With StyleGAN" data-popup-author="Gwern Branwen" data-popup-date="4 Feb 2019" data-popup-abstract="<p>Generative neural networks, such as GANs, have <a href=&quot;#why-dont-gans-work&quot;>struggled for years</a> to generate decent-quality anime faces, despite their great success with photographic imagery such as real human faces. The task has now been effectively solved, for anime faces as well as many other domains, by the development of a new generative adversarial network, <a href=&quot;https://arxiv.org/abs/1812.04948&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Style-Based Generator Architecture for Generative Adversarial Networks&quot; data-popup-author=&quot;Tero Karras, Samuli Laine, Timo Aila&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&quot; title=&quot;&amp;#39;A Style-Based Generator Architecture for Generative Adversarial Networks&amp;#39;, Karras et al 2018&quot;><em>StyleGAN</em></a>, whose <a href=&quot;https://github.com/NVlabs/stylegan&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>source code</a> was released in February 2019.</p><p>I <a href=&quot;#examples&quot;>show off</a> my StyleGAN anime faces &amp;amp; videos, provide downloads, provide the âmissing manualâ &amp;amp; explain how I trained them based on <a href=&quot;./Danbooru2019&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;15 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The best known booru, with a focus on quality, is &amp;lt;a href=&amp;quot;https://danbooru.donmai.us/&amp;quot;&amp;gt;Danbooru&amp;lt;/a&amp;gt;. We create &amp;amp;amp; provide a torrent which contains ~2.5tb of 3.33m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.&amp;lt;/p&amp;gt;&quot;>Danbooru2017/2018</a> with source code for the <a href=&quot;#data-preparation&quot;>data preprocessing</a>, document <a href=&quot;#installation&quot;>installation</a> &amp;amp; <a href=&quot;#configuration&quot;>configuration</a> &amp;amp; <a href=&quot;#running&quot;>training tricks</a>.</p><p>For application, I document various scripts for generating <a href=&quot;#sampling&quot;>images &amp;amp; videos</a>, briefly <a href=&quot;#twdne&quot;>describe the website</a> <a href=&quot;https://www.thiswaifudoesnotexist.net&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><q>âThis Waifu Does Not Existâ</q></a> <a href=&quot;./TWDNE&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;This Waifu Does Not Exist&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;19 Feb 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, &amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;âThis Waifu Does Not Existâ&amp;lt;/a&amp;gt; for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or &amp;lt;a href=&amp;quot;https://klimaleksus.github.io/FindTwin/&amp;quot; title=&amp;quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&amp;quot;&amp;gt;online&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://github.com/darabos/high-five-trading&amp;quot; title=&amp;quot;Action stock exchange game for Repl.it Game Jam 2019&amp;quot;&amp;gt;games&amp;lt;/a&amp;gt;, uploaded to Pixiv, and used in a research paper (&amp;lt;a href=&amp;quot;https://arxiv.org/abs/1904.01774&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot; data-popup-author=&amp;quot;Atsuhiro Noguchi, Tatsuya Harada&amp;quot; data-popup-date=&amp;quot;2019-08-21&amp;quot; data-popup-abstract=&amp;quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&amp;quot; title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot;&amp;gt;Noguchi &amp;amp;amp; Harada 2019&amp;lt;/a&amp;gt;). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, &amp;lt;a href=&amp;quot;https://waifulabs.com/&amp;quot;&amp;gt;Waifu Labs&amp;lt;/a&amp;gt;, which generates even better anime faces than my StyleGAN results.&amp;lt;/p&amp;gt;&quot;>I set up</a> as a public demo, discuss how the trained models can be <a href=&quot;#transfer-learning&quot;>used for transfer learning</a> such as generating high-quality faces of anime characters with small datasets (eg <a href=&quot;#holo&quot;>Holo</a> or <a href=&quot;#asuka&quot;>Asuka Souryuu Langley</a>), and touch on <a href=&quot;#reversing-stylegan-to-control-modify-images&quot;>more advanced StyleGAN applications</a> like encoders &amp;amp; controllable generation.</p><p>The <a href=&quot;#appendix&quot;>appendix</a> gives samples of my failures with earlier GANs for anime face generation, and I provide samples &amp;amp; model from a relatively large-scale <a href=&quot;#biggan&quot;>BigGAN</a> training run suggesting that BigGAN may be the next step forward to generating full-scale anime images.</p><p>A minute of reading could save an hour of debugging!</p>">Making Anime Faces w/Neural Nets</a></li>
<li><a href="./Death-Note-script" class="docMetadata" data-popup-title="Who wrote the 'Death Note' script?" data-popup-author="Gwern Branwen" data-popup-date="2 Nov 2009" data-popup-abstract="<p>I give a history of the 2009 leaked script, discuss internal &amp;amp; external evidence for its realness including stylometrics; and then give a simple step-by-step Bayesian analysis of each point. We finish with high confidence in the script being real, discussion of how this analysis was surprisingly enlightening, and what followup work the analysis suggests would be most valuable.</p>"><em>Death Note</em> script authenticity</a></li>
<li><a href="./The-Existential-Risk-of-Mathematical-Error" class="docMetadata" data-popup-title="The Existential Risk of Math Errors" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Mathematical mistake/error-rates limit our understanding of rare risks and ability to defend against them">The Existential Risk of Math Errors</a></li>
<li><a href="./Iodine" class="docMetadata" data-popup-title="Iodine and Adult IQ meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="29 Feb 2012" data-popup-abstract="<p>Iodization is one of the great success stories of public health intervention: iodizing salt costs pennies per ton, but as demonstrated in randomized &amp;amp; natural experiments, prevents goiters, cretinism, and can boost population IQs by a fraction of a standard deviation in the most iodine-deficient populations.</p><p>These experiments are typically done on pregnant women, and results suggest that the benefits of iodization diminish throughout the trimesters of a pregnancy. So does iodization benefit normal healthy <em>adults</em>, potentially even ones in relatively iodine-sufficient Western countries?</p><p>Compiling existing post-natal iodization studies which use cognitive tests, I find thatâoutliers asideâthe benefit appears to be nearly zero, and so likely it does not help normal healthy adults, particularly in Western adults.</p>">Iodine &amp; Adult IQ meta-analysis</a></li>
<li><a href="./The-Melancholy-of-Kyon" class="docMetadata" data-popup-title="The Melancholy of Kyon" data-popup-author="Gwern Branwen" data-popup-date="08 Jun 2009" data-popup-abstract="<p>The light novel series <em>The Melancholy of Haruhi Suzumiya</em>, featuring a character named Haruhi who is a god unawares and her search for novelty, has a number of anomalies and unclear overarching plot. I argue that these anomalies can be resolved, and greater literary depth achieved, by interpreting the first-person protagonist Kyon as the actual unaware god.</p>">The Melancholy of Kyon</a></li>
<li><a href="./In-Defense-Of-Inclusionism" class="docMetadata" data-popup-title="In Defense of Inclusionism" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="<p>English Wikipedia is in decline. As a long-time editor &amp;amp; former admin, I was deeply dismayed by the process. Here, I discuss UI principles, changes in Wikipedian culture, the large-scale statistical evidence of decline, run small-scale experiments demonstrating the harm, and conclude with parting thoughts.</p>">In Defense of Wikipedia Inclusionism</a></li>
<li><a href="./Bitcoin-is-Worse-is-Better" class="docMetadata" data-popup-title="Bitcoin Is Worse Is Better" data-popup-author="Gwern Branwen" data-popup-date="27 May 2011" data-popup-abstract="2011 essay on how Bitcoin's long gestation and early opposition indicates it is an example of the 'Worse is Better' paradigm in which an ugly complex design with few attractive theoretical properties compared to purer competitors nevertheless successfully takes over a niche, survives, and becomes gradually refined.">Bitcoin is Worse is Better</a></li>
<li><a href="./Sunk-cost" class="docMetadata" data-popup-title="Are Sunk Costs Fallacies?" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2012" data-popup-abstract="Human and animal sunk costs often aren't, and sunk cost bias may be useful on an individual level to encourage learning. Convincing examples of sunk cost bias typically operate on organizational levels and are probably driven by non-psychological causes like competition.">Are Sunk Costs Fallacies?</a></li>
</ul>
</section>
<p>By topic:</p>
<section id="statistics" class="level1">
<h1><a href="#statistics" title="Link to section: 'Statistics'">Statistics</a></h1>
<ul>
<li><a href="./Causality" class="docMetadata" data-popup-title="Why Correlation Usually â  Causation: Causal Nets Cause Common Confounding" data-popup-author="Gwern Branwen" data-popup-date="24 Jun 2014" data-popup-abstract="<p>It is widely understood that statistical correlation between two variables â  causation. But despite this admonition, people are routinely overconfident in claiming correlations to support particular causal interpretations and are surprised by the results of randomized experiments, suggesting that they are biased &amp;amp; systematically underestimating the prevalence of confounds/common-causation. I speculate that in realistic causal networks or DAGs, the number of possible correlations grows faster than the number of possible causal relationships. So confounds really are that common, and since people do not think in DAGs, the imbalance also explains overconfidence.</p>">Why Correlation Usually â  Causation</a></li>
<li><a href="./Mail-delivery" class="docMetadata" data-popup-title="When Should I Check The Mail?" data-popup-author="Gwern Branwen" data-popup-date="21 June 2015" data-popup-abstract="<p>Mail is delivered by the USPS mailman at a regular but not observed time; what is observed is whether the mail has been delivered at a time, yielding somewhat-unusual âinterval-censored dataâ. I describe the problem of estimating when the mailman delivers, write a simulation of the data-generating process, and demonstrate analysis of interval-censored data in R using maximum-likelihood (survival analysis with Gaussian regression using <code>survival</code> library), MCMC (Bayesian model in JAGS), and likelihood-free Bayesian inference (custom ABC, using the simulation). This allows estimation of the distribution of mail delivery times. I compare those estimates from the interval-censored data with estimates from a (smaller) set of exact delivery-times provided by USPS tracking &amp;amp; personal observation, using a multilevel model to deal with heterogeneity apparently due to a change in USPS routes/postmen. Finally, I define a loss function on mail checks, enabling: a choice of optimal time to check the mailbox to minimize loss (exploitation); optimal time to check to maximize information gain (exploration); Thompson sampling (balancing exploration &amp;amp; exploitation indefinitely), and estimates of the value-of-information of another datapoint (to estimate when to stop exploration and start exploitation after a finite amount of data).</p>">When Does The Mail Come? Decision Analysis</a></li>
<li><a href="./Google-shutdowns" class="docMetadata" data-popup-title="Predicting Google closures" data-popup-author="Gwern Branwen" data-popup-date="28 Mar 2013" data-popup-abstract="<p>Prompted by the shutdown of Google Reader, I ponder the evanescence of online services and wonder what is the risk of them disappearing. I collect data on <a href=&quot;#sources&quot;>350 Google products</a> launched before March 2013, looking for <a href=&quot;#variables&quot;>variables predictive of mortality</a> (web hits, service vs software, commercial vs free, FLOSS, social networking, and internal vs acquired). Shutdowns are unevenly distributed over the calendar year or Googleâs history. I use logistic regression &amp;amp; survival analysis (which can deal with right-censorship) to <a href=&quot;#modeling&quot;>model the risk of shutdown over time</a> and examine correlates. The logistic regression indicates socialness, acquisitions, and lack of web hits predict being shut down, but the results may not be right. The survival analysis finds a median lifespan of 2824 days with a roughly Type III survival curve (high early-life mortality); a Cox regression finds similar results as the logistic - socialness, free, acquisition, and long life predict lower mortality. Using the best model, I <a href=&quot;#predictions&quot;>make predictions</a> about probability of shutdown of the most risky and least risky services in the next 5 years (up to March 2018). (All data &amp;amp; R source code is provided.)</p>">Predicting Google service closures</a></li>
<li><a href="./Prediction-markets" class="docMetadata" data-popup-title="Prediction Markets" data-popup-author="Gwern Branwen" data-popup-date="10 Jan 2009" data-popup-abstract="My prediction/betting strategies and track record, reflections on rationality, prediction judgments">Predicting &amp; Prediction Markets</a></li>
<li><a href="./Ads" class="docMetadata" data-popup-title="Banner Ads Considered Harmful" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2017" data-popup-abstract="<p>One source of complexity &amp;amp; JavaScript use on <code>gwern.net</code> is the use of Google AdSense advertising to insert banner ads. In considering design &amp;amp; usability improvements, removing the banner ads comes up every time as a possibility, as readers do not like ads, but such removal comes at a revenue loss and itâs unclear whether the benefit outweighs the cost, suggesting I run an A/B experiment. However, ads might be expected to have broader effects on traffic than individual page reading times/bounce rates, affecting <em>total</em> site traffic instead through long-term effects on or spillover mechanisms between readers (eg social media behavior), rendering the usual A/B testing method of per-page-load/session randomization incorrect; instead it would be better to analyze total traffic as a time-series experiment.</p><p>Design: A decision analysis of revenue vs readers yields an maximum acceptable total traffic loss of ~3%. Power analysis of historical <code>gwern.net</code> traffic data demonstrates that the high autocorrelation yields low statistical power with standard tests &amp;amp; regressions but acceptable power with ARIMA models. I design a long-term Bayesian <code>ARIMA(4,0,1)</code> time-series model in which an A/B-test running JanuaryâOctober 2017 in randomized paired 2-day blocks of ads/no-ads uses client-local JS to determine whether to load &amp;amp; display ads, with total traffic data collected in Google Analytics &amp;amp; ad exposure data in Google AdSense. The A/B test ran from 1 January 2017 to 15 October 2017, affecting 288 days with collectively 380,140 pageviews in 251,164 sessions.</p><p>Correcting for a flaw in the randomization, the final results yield a surprisingly large estimate of an expected traffic loss of -9.7% (driven by the subset of users without adblock), with an implied -14% traffic loss if all traffic were exposed to ads (95% credible interval: -13â16%), exceeding my decision threshold for disabling ads &amp;amp; strongly ruling out the possibility of acceptably small losses which might justify further experimentation.</p><p>Thus, banner ads on <code>gwern.net</code> appear to be harmful and AdSense has been removed. If these results generalize to other blogs and personal websites, an important implication is that many websites may be harmed by their use of banner ad advertising without realizing it.</p>">Banner Ads Considered Harmful (Here)</a></li>
<li><a href="./Research-criticism" class="docMetadata" data-popup-title="How Should We Critique Research?" data-popup-author="Gwern Branwen" data-popup-date="19 May 2019" data-popup-abstract="<p>Scientific and statistical research must be read with a critical eye to understand how credible the claims are. The Reproducibility Crisis and the growth of meta-science have demonstrated that much research is of low quality and often false. But there are so many possible things any given study could be criticized for, falling short of an unobtainable ideal, that it becomes unclear which possible criticism is important and they may degenerate into mere rhetoric. How do we separate fatal flaws from unfortunate caveats from specious quibbling?</p><p>I think that what makes a criticism important is how much it could change a result if corrected and how much that would then change our decisions or actions: to what extent it is a <q>âdifference which makes a differenceâ</q>. This is why issues of research fraud, causal inference, or biases yielding overestimates are universally important, because a âcausalâ effect turning out to be zero effect or overestimated by a factor will change almost all decisions based on such research; while on the other hand, other issues like measurement error or distributional assumptions, which are equally common, are often <em>not</em> important as they typically yield much smaller changes in conclusions, and hence decisions.</p><p>If we regularly ask whether a criticism would make this kind of difference, it will be clearer which ones are important criticisms, and which ones risk being rhetorical distractions and obstructing meaningful evaluation of research.</p>">How To Critique Research?</a></li>
<li><a href="./2012-election-predictions" class="docMetadata" data-popup-title="2012 election predictions" data-popup-author="Gwern Branwen" data-popup-date="5 Nov 2012" data-popup-abstract="<p>Statistically analyzing in <code>R</code> hundreds of predictions compiled for ~10 forecasters of the 2012 American Presidential election, and ranking them by Brier, RMSE, &amp;amp; log scores; the best overall performance seems to be by Drew Linzer and Wang &amp;amp; Holbrook, while Nate Silver appears as somewhat over-rated and the famous Intrade prediction market turning in a disappointing overall performance.</p>">Judging the 2012 election forecasters</a></li>
<li><a href="./Candy-Japan" class="docMetadata" data-popup-title="Candy Japan's new box A/B test" data-popup-author="Gwern Branwen" data-popup-date="6 May 2016" data-popup-abstract="<p>I analyze an A/B test from a mail-order company of two different kinds of box packaging from a Bayesian decision-theory perspective, balancing posterior probability of improvements &amp;amp; greater profit against the cost of packaging &amp;amp; risk of worse results, finding that as the companyâs analysis suggested, the new box is unlikely to be sufficiently better than the old. Calculating expected values of information shows that it is not worth experimenting on further, and that such fixed-sample trials are unlikely to ever be cost-effective for packaging improvements. However, adaptive experiments may be worthwhile.</p>">Candy Japanâs new-box A/B test</a></li>
<li><a href="./Resorter" class="docMetadata" data-popup-title="Resorting Media Ratings" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2015" data-popup-abstract="<p>User-created datasets using ordinal scales (such as media ratings) have tendencies to drift or âclumpâ towards the extremes and fail to be informative as possible, falling prey to ceiling effects and making it difficult to distinguish between the mediocre and truly excellent. This can be counteracted by rerating the dataset to create a uniform (and hence, informative) distribution of ratings, but such manual rerating is difficult. I provide an anytime CLI program, <code>resorter</code>, which keeps track of comparisons, infers underlying ratings assuming that they are noisy in the ELO-like Bradley-Terry model, and interactively &amp;amp; intelligently queries the user with comparisons of the media with the most uncertain current ratings, until the user ends the session and a fully rescaled set of ratings are output.</p>">Interactively Resorting Lists</a></li>
<li><a href="./Google-Alerts" class="docMetadata" data-popup-title="Alerts Over Time" data-popup-author="Gwern Branwen" data-popup-date="1 July 2013" data-popup-abstract="<p>Has Google Alerts been sending fewer results the past few years? Yes. Responding to rumors of its demise, I investigate the number of results in my personal Google Alerts notifications 2007-2013, and find no overall trend of decline until I look at a transition in <a href=&quot;#it-was-mid-2011&quot;>mid-2011</a> where the results fall dramatically. I speculate about <a href=&quot;#panda&quot;>the cause</a> and <a href=&quot;#conclusion&quot;>implications</a> for Alertsâs future.</p>">Google Alerts Over Time</a></li>
<li><a href="./Order-statistics" class="docMetadata" data-popup-title="Calculating in R The Expected Maximum of a Gaussian Sample using Order Statistics" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>In generating a sample of <em>n</em> datapoints drawn from a normal/Gaussian distribution with a particular mean/SD, how big on average the biggest datapoint is will depend on how large <em>n</em> is. Knowing this average is useful in a number of areas like sports or breeding or manufacturing, as it defines how bad/good the worst/best datapoint will be (eg the score of the winner in a multi-player game).</p><p>The <a href=&quot;https://en.wikipedia.org/wiki/order_statistic&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Order statistic&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, the &amp;lt;i&amp;gt;k&amp;lt;/i&amp;gt;th &amp;lt;b&amp;gt;order statistic&amp;lt;/b&amp;gt; of a statistical sample is equal to its &amp;lt;i&amp;gt;k&amp;lt;/i&amp;gt;th-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: order statistic&quot;>order statistic</a> of the mean/average/expectation of the maximum of a draw of <em>n</em> samples from a normal distribution has no exact formula, unfortunately, and is generally not built into any programming languageâs libraries.</p><p>I implement &amp;amp; compare some of the approaches to estimating this order statistic in the R programming language, for both the maximum and the general order statistic. The overall best approach is to calculate the exact order statistics for the <em>n</em> range of interest using numerical integration via <code>lmomco</code> and cache them in a lookup table, rescaling the mean/SD as necessary for arbitrary normal distributions; next best is a polynomial regression approximation; finally, the Elfving correction to the Blom 1958 approximation is fast, easily implemented, and accurate for reasonably large <em>n</em> such as <em>n</em>&amp;gt;100.</p>">Expected Maximum of Gaussians</a></li>
<li><a href="./Everything" class="docMetadata" data-popup-title="Everything Is Correlated" data-popup-author="Gwern Branwen" data-popup-date="12 Sep 2014" data-popup-abstract="<p>Statistical folklore asserts that <q>âeverything is correlatedâ</q>: in any real-world dataset, most or all measured variables will have non-zero correlations, even between variables which appear to be completely independent of each other, and that these correlations are not merely sampling error flukes but will appear in large-scale datasets to arbitrarily designated levels of statistical-significance or posterior probability.</p><p>This raises serious questions for null-hypothesis statistical-significance testing, as it implies the null hypothesis of 0 will always be rejected with sufficient data, meaning that a failure to reject only implies insufficient data, and provides no actual test or confirmation of a theory. Even a directional prediction is minimally confirmative since there is a 50% chance of picking the right direction at random.</p><p>It also has implications for conceptualizations of theories &amp;amp; causal models, interpretations of structural models, and other statistical principles such as the <q>âsparsity principleâ</q>.</p>">Everything Is Correlated</a></li>
<li><a href="./docs/statistics/order/beanmachine-multistage/index.html" class="docMetadata" data-popup-title="Multi-Stage Bean Machine Visualization: Advantages of Repeated Optimization" data-popup-author="Rafe Kennedy, Gwern Branwen" data-popup-date="2018-12-17" data-popup-abstract="An interactive JavaScript of order statistics visualized as a Galton bean machine, showing difference in means &amp; maxima between single stage of selection and multiple stages.<br />This is an interactive JS-based visualization of the difference in optimization potentials of a single-stage pipeline vs a multi-stage pipeline, in which new samples/measurements can be generated at each step (such as in evolutionary processes).</p><p>Because it optimizes over multiple steps, the multi-stage pipeline âratchets upwardâ and can attain far more extreme maxima than a single-stage pipeline, even with the same total number of samples - the single-stage process quickly hits âdiminishing returnsâ, where large increases in sample count result in only small increases in the expected maximum. This means that small gains per stage, or a few stages, or a few generations of evolution, can result in large increases of sample means, compared to a single-stage process. Due to <a href=&quot;https://en.wikipedia.org/wiki/Order_statistic&quot;>order statistics</a>, the increases in means can cause larger increases in the probability of samples passing thresholds such as âtop 1%â/â¥2.32Ï, or yielding extremes. And the more stages, the greater differences can be (single-stage selection increases logarithmically, while multi-stage increases linearly).</p><p>These increases can be counterintuitively large, but the gains/losses are relevant to understanding many processes, such as the clinical drug discovery pipeline (eg <a href=&quot;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0147215&quot; title=&quot;When Quality Beats Quantity: Decision Theory, Drug Discovery, and the Reproducibility Crisis&quot;>Scannell &amp;amp; Bosley 2016</a>).</p><p>The visualization metaphor here is <a href=&quot;https://en.wikipedia.org/wiki/Bean_machine&quot;>Francis Galtonâs <em>quincunx</em> or âbean machineâ</a>: a ball (sample) falls from the top (zero-mean), and is affected by sets of pins (stochastic variables) which bounce the ball left/right with 50-50 probability (increase/decrease it) as it falls to the bottom (final total). The resulting binomial distribution approximates a normal distribution. The bean machine visually &amp;amp; concretely illustrates the sampling distribution of how a normally-distributed final variable can emerge out of the sum of many individual small discrete effects, without requiring any mathematics.</p><p>In this visualization, we generalize Galtonâs âbean machineâ by allowing stacking of bean machines. To stack bean machines, we select the ball which is the <em>maximum</em> within each sample. How large is it? In the single-stage bean machine, selection stops there. In the multi-stage bean machine, <em>another</em> bean machine begins with the maximum serving as the seed &amp;amp; new average, and another round of generation &amp;amp; selection begins, and so on, until a final sample is selected, and we can see how large it is. The gains turn out to be larger the more samples we use total, unsurprisingly, but also the more stages we specify; the maximum possible maximum turns out to be when we have so many stages that there are just 2 samples per stage.</p><p><img style=&quot;border: 1px solid #666;&quot; alt=&quot;Screenshot of the multi-stage bean machine, showing selection in progress in a 3x3 pipeline&quot; width=&quot;300&quot; src=&quot;/docs/statistics/order/beanmachine-multistage/beanmachine-demo.pngbeanmachine-demo.png&quot; title=&quot;Multi-Stage Bean Machine&quot;></p>">Multi-Stage Selection JS Demo</a></li>
<li><a href="./Milk" class="docMetadata" data-popup-title="The Power of Twins: Revisiting Student's Scottish Milk Experiment Example" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2016" data-popup-abstract="<p>Randomized experiments require more subjects the more variable each datapoint is to overcome the noise which obscures any effects of the intervention. Reducing noise enables better inferences with the same data, or less data to be collected, which can be done by balancing observed characteristics between control and experimental datapoints.</p><p>A particularly dramatic example of this approach is running experiments on identical twins rather than regular people, because twins vary far less from each other than random people due to shared genetics &amp;amp; family environment. In 1931, the great statistician Student (William Sealy Gosset) noted problems with an extremely large (<em>n</em>=20,000) Scottish experiment in feeding children milk (to see if they grew more in height or weight), and claimed that the experiment could have been done far more cost-effectively with an extraordinary reduction of &amp;gt;95% fewer children if it had been conducted using twins, and claimed that 100 identical twins would have been <em>more</em> accurate than 20,000 children. He, however, did not provide any calculations or data demonstrating this.</p><p>I revisit the issue and run a power calculation on height indicating that Studentâs claims were correct and that the experiment would have required ~97% fewer children if run with twins.</p><p>This reduction is not unique to the Scottish milk experiment on height/weight, and in general, one can expect a reduction of 89% in experiment sample sizes using twins rather than regular people, demonstrating the benefits of using behavioral genetics in <a href=&quot;https://en.wikipedia.org/wiki/experiment_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Design of experiments&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;design of experiments&amp;lt;/b&amp;gt; is the design of any task that aims to describe or explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: experiment design&quot;>experiment design</a>/<a href=&quot;https://en.wikipedia.org/wiki/Power_%28statistics%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Power (statistics)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;power&amp;lt;/b&amp;gt; of a binary hypothesis test is the probability that the test rejects the null hypothesis (H&amp;lt;sub&amp;gt;0&amp;lt;/sub&amp;gt;) when a specific alternative hypothesis (H&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;) is true. The statistical power ranges from 0 to 1, and as statistical power increases, the probability of making a type II error (wrongly failing to reject the null hypothesis) decreases. For a type II error probability of Î², the corresponding statistical power is 1 â Î². For example, if experiment 1 has a statistical power of 0.7, and experiment 2 has a statistical power of 0.95, then there is a stronger probability that experiment 1 had a type II error than experiment 2, and experiment 2 is more reliable than experiment 1 due to the reduction in probability of a type II error. It can be equivalently thought of as the probability of accepting the alternative hypothesis (H&amp;lt;sub&amp;gt;1&amp;lt;/sub&amp;gt;) when it is trueâthat is, the ability of a test to detect a specific effect, if that specific effect actually exists. That is,&amp;lt;/p&amp;gt;&amp;lt;dl&amp;gt;&amp;lt;dd&amp;gt;&amp;lt;span class=&amp;quot;mwe-math-element&amp;quot;&amp;gt;&amp;lt;img src=&amp;quot;https://wikimedia.org/api/rest_v1/media/math/render/svg/0f98c579718d2bfaafcbaea6c921fd9ba19ce268&amp;quot; class=&amp;quot;mwe-math-fallback-image-inline&amp;quot; aria-hidden=&amp;quot;true&amp;quot; style=&amp;quot;vertical-align:-1.005ex;width:35.813ex;height:3.176ex&amp;quot; /&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/dd&amp;gt;&amp;lt;/dl&amp;gt;&quot; title=&quot;Wikipedia: Power (statistics)&quot;>power analysis</a>.</p>">Power of Twins: the Milk Experiment</a></li>
<li><a href="./Hunter" class="docMetadata" data-popup-title="'Genius Revisited': On the Value of High IQ Elementary Schools" data-popup-author="Gwern Branwen" data-popup-date="19 June 2016" data-popup-abstract="<p><em>Genius Revisited</em> documents the longitudinal results of a high-IQ/gifted-and-talented elementary school, Hunter College Elementary School (HCES); one of the most striking results is the general high education &amp;amp; income levels, but absence of great accomplishment on a national or global scale (eg a Nobel prize). The authors suggest that this may reflect harmful educational practices at their elementary school or the low predictive value of IQ.</p><p>I suggest that there is no puzzle to this absence nor anything for HCES to be blamed for, as the absence is fully explainable by their making two statistical errors: <a href=&quot;https://en.wikipedia.org/wiki/base-rate_neglect&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Base rate fallacy&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;base rate fallacy&amp;lt;/b&amp;gt;, also called &amp;lt;b&amp;gt;base rate neglect&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;base rate bias&amp;lt;/b&amp;gt;, is a fallacy. If presented with related base rate information and specific information, the mind tends to ignore the former and focus on the latter.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: base-rate neglect&quot;>base-rate neglect</a>, and <a href=&quot;https://en.wikipedia.org/wiki/regression_to_the_mean&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Regression toward the mean&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, &amp;lt;b&amp;gt;regression toward&amp;lt;/b&amp;gt; &amp;lt;b&amp;gt;the mean&amp;lt;/b&amp;gt; is the phenomenon that arises if a random variable is extreme on its first measurement but closer to the mean or average on its second measurement and if it is extreme on its second measurement but closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called &amp;lt;b&amp;gt;reversion to the mean&amp;lt;/b&amp;gt; and &amp;lt;b&amp;gt;reversion to mediocrity&amp;lt;/b&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: regression to the mean&quot;>regression to the mean</a>.</p><p>First, their standards fall prey to a base-rate fallacy and even extreme predictive value of IQ would not predict 1 or more Nobel prizes because Nobel prize odds are measured at 1 in millions, and with a small total sample size of a few hundred, it is highly likely that there would simply be no Nobels.</p><p>Secondly, and more seriously, the lack of accomplishment is inherent and unavoidable as it is driven by the <a href=&quot;https://en.wikipedia.org/wiki/regression_to_the_mean&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Regression toward the mean&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In statistics, &amp;lt;b&amp;gt;regression toward&amp;lt;/b&amp;gt; &amp;lt;b&amp;gt;the mean&amp;lt;/b&amp;gt; is the phenomenon that arises if a random variable is extreme on its first measurement but closer to the mean or average on its second measurement and if it is extreme on its second measurement but closer to the average on its first. To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Historically, what is now called regression toward the mean has also been called &amp;lt;b&amp;gt;reversion to the mean&amp;lt;/b&amp;gt; and &amp;lt;b&amp;gt;reversion to mediocrity&amp;lt;/b&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: regression to the mean&quot;>regression to the mean</a> caused by the relatively low correlation of early childhood with adult IQsâwhich means their sample is far less elite as adults than they believe. Using early-childhood/adult IQ correlations, regression to the mean implies that HCES students will fall from a mean of 157 IQ in kindergarten (when selected) to somewhere around 133 as adults (and possibly lower). Further demonstrating the role of regression to the mean, in contrast, HCESâs associated high-IQ/gifted-and-talented high school, Hunter High, which has access to the adolescentsâ more predictive IQ scores, has much higher achievement in proportion to its lesser regression to the mean (despite dilution by Hunter elementary students being grandfathered in).</p><p>This unavoidable statistical fact undermines the main rationale of HCES: extremely high-IQ adults cannot be accurately selected as kindergartners on the basis of a simple test. This greater-regression problem can be lessened by the use of additional variables in admissions, such as parental IQs or high-quality genetic polygenic scores; unfortunately, these are either politically unacceptable or dependent on future scientific advances. This suggests that such elementary schools may not be a good use of resources and HCES students should not be assigned scarce magnet high school slots.</p>"><em>Genius Revisited</em>: High-IQ School Flaws</a></li>
<li><a href="./hpmor#analysis" class="docMetadata" data-popup-title="'HP: Methods of Rationality' review statistics" data-popup-author="Gwern Branwen" data-popup-date="3 Nov 2012" data-popup-abstract="<p>The unprecedented gap in <a href=&quot;http://www.hpmor.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><em>Methods of Rationality</em></a> updates prompts musing about whether readership is increasing enough &amp;amp; what statistics one would use; I write code to download FF.net reviews, clean it, parse it, load into R, summarize the data &amp;amp; depict it graphically, run linear regression on a subset &amp;amp; all reviews, note the poor fit, develop a quadratic fit instead, and use it to predict future review quantities.</p><p>Then, I run a similar analysis on a competing fanfiction to find out when they will have equal total review-counts. A try at logarithmic fits fails; fitting a linear model to the previous 100 days of <em>MoR</em> and the competitor works much better, and they predict a convergence in &amp;lt;5 years.</p><p>A survival analysis finds no major anomalies in reviewer lifetimes, but an apparent increase in mortality for reviewers who started reviewing with later chapters, consistent with (but far from proving) the original theory that the later chaptersâ delays are having negative effects.</p>">Modeling fiction review rates</a></li>
<li><a href="./GoodReads" class="docMetadata" data-popup-title="The Most Abandoned Books on GoodReads" data-popup-abstract="<p>What books are hardest for a reader who starts them to finish, and most likely to be abandoned? I scrape a crowdsourced tag, <code>abandoned</code>, from the GoodReads book social network to estimate conditional probability of being abandoned.</p> <p>The default GoodReads tag interface presents only raw counts of tags, not counts divided by total ratings (=reads). This conflates popularity with probability of being abandoned: a popular but rarely-abandoned book may have more <code>abandoned</code> tags than a less popular but often-abandoned book. There is also residual error from the winnerâs curse where books with fewer ratings are more mis-estimated than popular books.</p> <p>Correcting for both changes the top-5 ranking completely, from (<a href=&quot;#data&quot;>raw counts</a>):</p> <ol type=&quot;1&quot;> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Casual_Vacancy&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Casual Vacancy&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Casual Vacancy</b></i> is a 2012 novel written by J. K. Rowling. The book was published worldwide by the Little, Brown Book Group on 27 September 2012. A paperback edition was released on 23 July 2013. It was Rowling's first publication since the <i>Harry Potter</i> series, her first apart from that series, and her first novel for adult readership.</p>&quot; title=&quot;Wikipedia: The Casual Vacancy&quot;>The Casual Vacancy</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/J.K._Rowling&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;J. K. Rowling&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Joanne Rowling</b>, better known by her pen name <b>J. K. Rowling</b>, is a British author, film producer, television producer, screenwriter, and philanthropist. She is best known for writing the <i>Harry Potter</i> fantasy series, which has won multiple awards and sold more than 500 million copies, becoming the best-selling book series in history. The books are the basis of a popular film series, over which Rowling had overall approval on the scripts and was a producer on the final films. She also writes crime fiction under the name <b>Robert Galbraith</b>.</p>&quot; title=&quot;Wikipedia: J.K. Rowling&quot;>J.K. Rowling</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Catch-22&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catch-22&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Catch-22</b></i> is a satirical war novel by American author Joseph Heller. He began writing it in 1953; the novel was first published in 1961. Often cited as one of the most significant novels of the twentieth century, it uses a distinctive non-chronological third-person omniscient narration, describing events from the points of view of different characters. The separate storylines are out of sequence so the timeline develops along with the plot.</p>&quot; title=&quot;Wikipedia: Catch-22&quot;>Catch-22</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Joseph_Heller&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Joseph Heller&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Joseph Heller</b> was an American author of novels, short stories, plays, and screenplays. His best-known work is the novel <i>Catch-22</i>, a satire on war and bureaucracy, whose title has become a synonym for an absurd or contradictory choice.</p>&quot; title=&quot;Wikipedia: Joseph Heller&quot;>Joseph Heller</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/American_Gods&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;American Gods&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>American Gods</b></i> (2001) is a fantasy novel by British author Neil Gaiman. The novel is a blend of Americana, fantasy, and various strands of ancient and modern mythology, all centering on the mysterious and taciturn Shadow.</p>&quot; title=&quot;Wikipedia: American Gods&quot;>American Gods</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Neil_Gaiman&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Neil Gaiman&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Neil Richard MacKinnon Gaiman</b> is an English author of short fiction, novels, comic books, graphic novels, nonfiction, audio theatre, and films. His works include the comic book series <i>The Sandman</i> and novels <i>Stardust</i>, <i>American Gods</i>, <i>Coraline</i>, and <i>The Graveyard Book</i>. He has won numerous awards, including the Hugo, Nebula, and Bram Stoker awards, as well as the Newbery and Carnegie medals. He is the first author to win both the Newbery and the Carnegie medals for the same work, <i>The Graveyard Book</i> (2008). In 2013, <i>The Ocean at the End of the Lane</i> was voted Book of the Year in the British National Book Awards.</p>&quot; title=&quot;Wikipedia: Neil Gaiman&quot;>Neil Gaiman</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/A_Game_of_Thrones&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Game of Thrones&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>A Game of Thrones</b></i> is the first novel in <i>A Song of Ice and Fire</i>, a series of fantasy novels by the American author George R. R. Martin. It was first published on August 1, 1996. The novel won the 1997 Locus Award and was nominated for both the 1997 Nebula Award and the 1997 World Fantasy Award. The novella <i>Blood of the Dragon</i>, comprising the Daenerys Targaryen chapters from the novel, won the 1997 Hugo Award for Best Novella. In January 2011, the novel became a <span><i>New York Times</i> Bestseller</span> and reached #1 on the list in July 2011.</p>&quot; title=&quot;Wikipedia: A Game of Thrones&quot;>A Game of Thrones</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/George_R.R._Martin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;George R. R. Martin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>George Raymond Richard Martin</b>, also known as <b>GRRM</b>, is an American novelist and short story writer in the fantasy, horror, and science fiction genres, screenwriter, and television producer. He is best known for his series of epic fantasy novels, <i>A Song of Ice and Fire</i>, which was adapted into the HBO series <i>Game of Thrones</i> (2011â2019).</p>&quot; title=&quot;Wikipedia: George R.R. Martin&quot;>George R.R. Martin</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Book_Thief&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Book Thief&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Book Thief</b></i> is a historical novel by Australian author Markus Zusak and is his most popular work.</p>&quot; title=&quot;Wikipedia: The Book Thief&quot;>The Book Thief</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Markus_Zusak&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Markus Zusak&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Markus Zusak</b> is an Australian writer of German origin. He is best known for <i>The Book Thief</i> and <i>The Messenger</i>, two novels for young adults which became international bestsellers. He won the Margaret A. Edwards Award in 2014 for his contributions to young-adult literature published in the United States.</p>&quot; title=&quot;Wikipedia: Markus Zusak&quot;>Markus Zusak</a></li> </ol> <p>to (<a href=&quot;#bayesian-modeling&quot;>shrunken posterior proportions</a>):</p> <ol type=&quot;1&quot;> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Black_Leopard%2C_Red_Wolf&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Black Leopard, Red Wolf&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Black Leopard, Red Wolf</b></i> is a 2019 fantasy novel by writer Marlon James. It is the first book of a planned trilogy. The novel draws on African history and mythology, blended into the landscape of the North Kingdom and the South Kingdom, and the political tensions between these two warring states, as well as various city-states and tribes in the surrounding landscape. The rights to produce a film adaptation were purchased by Michael B. Jordan in February 2019 prior to release of the book.</p>&quot; title=&quot;Wikipedia: Black Leopard, Red Wolf&quot;>Black Leopard, Red Wolf</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Marlon_James&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Marlon James&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Marlon James</b> may refer to:</p><ul><li>Marlon James (novelist), Jamaican writer, winner of the 2015 Man Booker Prize</li> <li>Marlon James (footballer), Vincentian retired footballer</li></ul> &quot; title=&quot;Wikipedia: Marlon James&quot;>Marlon James</a></li> <li><a href=&quot;https://en.wikipedia.org/wiki/Space_Opera_%28Valente_novel%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Space Opera (Valente novel)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Space Opera</b></i> is a 2018 science fiction novel by Catherynne Valente, about a galactic version of the Eurovision Song Contest. It was first published by Saga Press.</p>&quot; title=&quot;Wikipedia: Space Opera (Valente novel)&quot;><em>Space Opera</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Catherynne_M._Valente&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catherynne M. Valente&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Catherynne M. Valente</b> is an American fiction writer, poet, and literary critic. For her speculative fiction novels she has won the annual James Tiptree, Andre Norton, and Mythopoeic Fantasy Awards. Her short fiction has appeared in <i>Clarkesworld Magazine</i>, the World Fantasy Awardâwinning anthologies <i>Salon Fantastique</i> and <i>Paper Cities</i>, along with numerous <i>Year's Best</i> volumes. Her critical work has appeared in the <i>International Journal of the Humanities</i> as well as in numerous essay collections.</p>&quot; title=&quot;Wikipedia: Catherynne M. Valente&quot;>Catherynne M. Valente</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Little%2C_Big&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Little, Big&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Little, Big: or, The Fairies' Parliament</b></i> is a modern fantasy novel by John Crowley, published in 1981. It won the World Fantasy Award in 1982.</p>&quot; title=&quot;Wikipedia: Little, Big&quot;>Little, Big</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/John_Crowley&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;John Crowley&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>John Crowley</b> may refer to:</p><ul><li>John Crowley (1891-1942), Irish revolutionary and hunger striker</li> <li>John Crowley (author), American author</li> <li>John Crowley (baseball) (1862â1896), American Major League catcher</li> <li>John Crowley, American biotechnology executive</li> <li>John Crowley (bishop), former bishop of Middlesbrough</li> <li>John Crowley (director), Irish theatre and film director</li> <li>John Crowley (politician) (1870â1934), Irish Sinn FÃ©in politician</li> <li>John Crowley (1659â1728), British politician</li> <li>John Powers Crowley (1936â1989), U.S. federal judge</li> <li>Johnny Crowley, Irish hurler</li> <li>Johnny Crowley, Gaelic footballer with Kerry GAA</li> <li>John Crowley, founder of Meadow Hall Ironworks, now the site of Meadowhall shopping centre</li></ul> &quot; title=&quot;Wikipedia: John Crowley&quot;>John Crowley</a></li> <li><a href=&quot;https://www.amazon.com/Witches-Suspicion-Betrayal-Hysteria-Salem/dp/031620059X?tag=gwernnet-20&quot;><em>The Witches: Salem, 1692</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Stacy_Schiff&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Stacy Schiff&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Stacy Madeleine Schiff</b> is an American nonfiction author residing in New York.</p>&quot; title=&quot;Wikipedia: Stacy Schiff&quot;>Stacy Schiff</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Tender_Morsels&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tender Morsels&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Tender Morsels</b></i> (2008) is a novel by Australian author Margo Lanagan. It won the Ditmar Award in 2009 for Best Novel and was joint winner of the 2009 World Fantasy Award for Best Novel.</p>&quot; title=&quot;Wikipedia: Tender Morsels&quot;>Tender Morsels</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Margo_Lanagan&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Margo Lanagan&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Margo Lanagan</b> is an Australian writer of short stories and young adult fiction.</p>&quot; title=&quot;Wikipedia: Margo Lanagan&quot;>Margo Lanagan</a></li> </ol> <p>I also consider <a href=&quot;#modeling-with-covariates&quot;>a model adjusting for covariates</a> (author/average-rating/year), to see what books are most surprisingly often-abandoned given their pedigrees &amp; rating etc. Abandon rates increase the newer a book is, and the lower the average rating.</p> <p>Adjusting for those, the top-5 are:</p> <ol type=&quot;1&quot;> <li><em>The Casual Vacancy</em>, J.K. Rowling</li> <li><a href=&quot;https://www.amazon.com/Chemist-Stephenie-Meyer/dp/0316387843?tag=gwernnet-20&quot;><em>The Chemist</em></a>, <a href=&quot;https://en.wikipedia.org/wiki/Stephenie_Meyer&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Stephenie Meyer&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Stephenie Meyer</b> is an American novelist and film producer, best known for her vampire romance series <i>Twilight</i>. The <i>Twilight</i> novels have sold over 100 million copies, with translations into 37 different languages. Meyer was the bestselling author of 2008 and 2009 in the US, having sold over 29 million books in 2008, and 26.5 million in 2009. <i>Twilight</i> was the best-selling book of 2008 in the US.</p>&quot; title=&quot;Wikipedia: Stephenie Meyer&quot;>Stephenie Meyer</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Infinite_Jest&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Infinite Jest&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Infinite Jest</b></i> is a 1996 novel by American writer David Foster Wallace. The novel is widely noted for its unconventional narrative structure and its experimental use of endnotes. It has been categorized as an encyclopedic novel and made <i>TIME</i> magazine's list of the 100 best English-language novels published between 1923 and 2005.</p>&quot; title=&quot;Wikipedia: Infinite Jest&quot;>Infinite Jest</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/David_Foster_Wallace&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;David Foster Wallace&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>David Foster Wallace</b> was an American writer and university professor in the disciplines of English and creative writing. His novel <i>Infinite Jest</i> (1996) was listed by <i>Time</i> magazine as one of the 100 best English-language novels published between 1923 and 2005. His last novel, <i>The Pale King</i> (2011), was a finalist for the Pulitzer Prize for Fiction in 2012.</p>&quot; title=&quot;Wikipedia: David Foster Wallace&quot;>David Foster Wallace</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/The_Glass_Bead_Game&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The Glass Bead Game&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>The Glass Bead Game</b></i> is the last full-length novel of the German author Hermann Hesse. It was begun in 1931 and published in Switzerland in 1943 after being rejected for publication in Germany due to Hesse's anti-Fascist views. A few years later, in 1946, Hesse won the Nobel Prize in Literature. In honoring him in its Award Ceremony Speech, the Swedish Academy said that the novel &quot;occupies a special position&quot; in Hesse's work.</p>&quot; title=&quot;Wikipedia: The Glass Bead Game&quot;>The Glass Bead Game</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/Hermann_Hesse&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Hermann Hesse&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>Hermann Karl Hesse</b> was a German-born poet, novelist, and painter. His best-known works include <i>Demian</i>, <i>Steppenwolf</i>, <i>Siddhartha</i>, and <i>The Glass Bead Game</i>, each of which explores an individual's search for authenticity, self-knowledge and spirituality. In 1946, he received the Nobel Prize in Literature.</p>&quot; title=&quot;Wikipedia: Hermann Hesse&quot;>Hermann Hesse</a></li> <li><em><a href=&quot;https://en.wikipedia.org/wiki/Theft_by_Finding%3A_Diaries_%281977%E2%80%932002%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Theft by Finding: Diaries (1977â2002)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><i><b>Theft by Finding: Diaries (1977â2002)</b></i> is an edited compilation of diary entries by David Sedaris published on May 30, 2017. Sedaris shares selected entries spanning from his days as a 20-year-old hitchhiking through Oregon to living in London just shy of his 46th birthday. It has been released in advance of <i>David Sedaris Diaries: A Visual Compendium</i>, which is scheduled to debut on October 10, 2017.</p>&quot; title=&quot;Wikipedia: Theft by Finding: Diaries (1977â2002)&quot;>Theft by Finding: Diaries (1977â2002)</a></em>, <a href=&quot;https://en.wikipedia.org/wiki/David_Sedaris&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;David Sedaris&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p><b>David Raymond Sedaris</b> is an American humorist, comedian, author, and radio contributor. He was publicly recognized in 1992 when National Public Radio broadcast his essay &quot;Santaland Diaries&quot;. He published his first collection of essays and short stories, <i>Barrel Fever,</i> in 1994. He is the brother and writing collaborator of actor Amy Sedaris.</p>&quot; title=&quot;Wikipedia: David Sedaris&quot;>David Sedaris</a></li> </ol> <p>Books at the top of the adjusted list appear to reflect a mix of highly-popular authors changing genres, and âprestigeâ books which are highly-rated but a slog to read.</p> <p>These results are interesting for how they highlight how people read books for many reasons (such as marketing campaigns, literary prestige, or following a popular author), and this is reflected in their decision whether to continue reading or to abandon a book.</p>" data-popup-date="2019-12-09" data-popup-author="Gwern Branwen">Modeling fiction drop rates</a></li>
<li><a href="./EA-donations" class="docMetadata" data-popup-title="LWer Effective Altruism donations, 2013-2014" data-popup-author="Gwern Branwen" data-popup-date="12 May 2015" data-popup-abstract="<p>A LW critic noted that the annual LW survey reported a median donation for âeffective altruistsâ of $0, though the EA movement encourages strongly donations. I look closer at the 2013-2014 LW surveys and find in multiple regression that identifying as an EA does predict more donations after controlling for age and income, suggesting that the low EA median donation may be due to EAers having low income and youth (eg being a student) rather than being unusually or even averagely selfish.</p>">LW Effective Altruism donations, 2013â2014</a></li>
<li><a href="./Statistical-notes" class="docMetadata" data-popup-title="Statistical Notes" data-popup-author="Gwern Branwen" data-popup-date="17 July 2014" data-popup-abstract="Miscellaneous statistical stuff">Miscellaneous</a></li>
</ul>
</section>
<section id="cryptobitcoin" class="level1">
<h1><a href="#cryptobitcoin" title="Link to section: 'Crypto/Bitcoin'">Crypto/Bitcoin</a></h1>
<ul>
<li><a href="./Self-decrypting-files" class="docMetadata" data-popup-title="Time-lock encryption" data-popup-author="Gwern Branwen" data-popup-date="24 May 2011" data-popup-abstract="<p>In cryptography, it is easy to adjust encryption of data so that one, some, or all people can decrypt it, or some combination thereof. It is not so easy to achieve adjustable decryptability over <em>time</em>, a âtime-lock cryptoâ: for some uses (data escrow, leaking, insurance, last-resort Bitcoin backups etc), one wants data which is distributed only after a certain point in time.</p><p>I survey techniques for time-lock crypto. Proposals often resort to trusted-third-parties, which are vulnerabilities. A better time-lock crypto proposal replaces trusted-third-parties with forcibly serial proof-of-work using number squaring and guaranteeing unlocking not after a certain point in time but after sufficient computation-time has been spent; itâs unclear how well number-squaring resists optimization or shortcuts. I suggest a new time-lock crypto based on chained hashes; hashes have been heavily attacked for other purposes, and may be safer than number-squaring. Finally, I cover obfuscation &amp;amp; witness-encryption which, combined with proof-of-work, can be said to solve time-lock crypto but currently remain infeasible.</p>">Time-lock encryption</a></li>
<li><a href="./Bitcoin-is-Worse-is-Better" class="docMetadata" data-popup-title="Bitcoin Is Worse Is Better" data-popup-author="Gwern Branwen" data-popup-date="27 May 2011" data-popup-abstract="2011 essay on how Bitcoin's long gestation and early opposition indicates it is an example of the 'Worse is Better' paradigm in which an ugly complex design with few attractive theoretical properties compared to purer competitors nevertheless successfully takes over a niche, survives, and becomes gradually refined.">Bitcoin is Worse is Better</a></li>
<li><a href="./Silk-Road" class="docMetadata" data-popup-title="Silk Road 1: Theory &amp; Practice" data-popup-author="Gwern Branwen" data-popup-date="11 Jul 2011" data-popup-abstract="<p>The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.</p>">A Silk Road 1 Guide</a></li>
<li><a href="./DNM-survival" class="docMetadata" data-popup-title="Darknet Market mortality risks" data-popup-author="Gwern Branwen" data-popup-date="30 Oct 2013" data-popup-abstract="<p>I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>Silk Road 1</a>, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.</p>"><span class="smallcaps-auto">DNM</span> census/lifetimes</a></li>
<li><a href="./DNM-archives" class="docMetadata" data-popup-title="Darknet Market Archives (2013-2015)" data-popup-author="Gwern Branwen" data-popup-date="1 Dec 2013" data-popup-abstract="<p>Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.</p><p>From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their <a href=&quot;./Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;11 Jul 2011&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>usage</a>, <a href=&quot;./DNM-survival&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market mortality risks&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;30 Oct 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;Silk Road 1&amp;lt;/a&amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;lt;/p&amp;gt;&quot;>lifetimes/characteristics</a>, &amp;amp; <a href=&quot;./DNM-arrests&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor DNM-related arrests, 2011-2015&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;14 Jul 2012&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;lt;/p&amp;gt;&quot;>legal riskiness</a>; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp; documents related to the DNMs as I could.</p><p>This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp; 37+ related forums, representing &amp;lt;4,438 mirrors, and is available for any research.</p><p>This page documents the download, contents, interpretation, and technical methods behind the scrapes.</p>"><span class="smallcaps-auto">DNM</span> archives (2013â2015)</a></li>
<li><a href="./DNM-arrests" class="docMetadata" data-popup-title="Tor DNM-related arrests, 2011-2015" data-popup-author="Gwern Branwen" data-popup-date="14 Jul 2012" data-popup-abstract="<p>I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.</p>"><span class="smallcaps-auto">DNM</span> arrests (2011â2015)</a></li>
<li><a href="./Timestamping" class="docMetadata" data-popup-title="Easy Cryptographic Timestamping of Files" data-popup-author="Gwern Branwen" data-popup-date="4 Dec 2015" data-popup-abstract="<p>Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to <em>timestamp</em> it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp; easily timestamp arbitrary files &amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.</p>">Timestamping w/Bitcoin</a></li>
</ul>
</section>
<section id="AI" class="level1">
<h1><a href="#ai" title="Link to section: 'AI'">AI</a></h1>
<ul>
<li><p><a href="./Complexity-vs-AI" class="docMetadata" data-popup-title="Complexity no Bar to AI" data-popup-author="Gwern Branwen" data-popup-date="1 June 2014" data-popup-abstract="<p>Computational complexity theory describes the steep increase in computing power required for many algorithms to solve larger problems; frequently, the increase is large enough to render problems a few times larger totally intractable. Many of these algorithms are used in AI-relevant contexts. It has been argued that this implies that AIs will fundamentally be limited in accomplishing real-world tasks better than humans because they will run into the same computational complexity limit as humans, and so the consequences of developing AI will be small, as it is impossible for there to be any large fast global changes due to human or superhuman-level AIs. I examine the assumptions of this argument and find it neglects the many conditions under which computational complexity theorems are valid and so the argument doesnât work: problems can be solved more efficiently than complexity classes would imply, large differences in problem solubility between humans and AIs is possible, greater resource consumption is possible, the real-world consequences of small differences on individual tasks can be large on agent impacts, such consequences can compound, and many agents can be created; any of these independent objections being true destroys the argument.</p>">Complexity no Bar to AI</a></p></li>
<li><p><a href="./Tool-AI" class="docMetadata" data-popup-title="Why Tool AIs Want to Be Agent AIs: The Power of Agency" data-popup-author="Gwern Branwen" data-popup-date="7 Sep 2016" data-popup-abstract="<p>Autonomous AI systems (Agent AIs) trained using <a href=&quot;https://en.wikipedia.org/wiki/reinforcement_learning&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reinforcement learning&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Reinforcement learning&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;RL&amp;lt;/b&amp;gt;) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reinforcement learning&quot;>reinforcement learning</a> can do harm when they take wrong actions, especially superintelligent Agent AIs. One solution would be to eliminate their agency by not giving AIs the ability to take actions, confining them to purely informational or inferential tasks such as classification or prediction (Tool AIs), and have all actions be approved &amp;amp; executed by humans, giving equivalently superintelligent results without the risk.</p><p>I argue that this is not an effective solution for two major reasons. First, because Agent AIs will by definition be better at <em>actions</em> than Tool AIs, giving an economic advantage. Secondly, because Agent AIs will be better at <em>inference &amp;amp; learning</em> than Tool AIs, and this is inherently due to their greater agency: the same algorithms which learn how to perform actions can be used to select important datapoints to learn inference over, how long to learn, how to more efficiently execute inference, how to design themselves, how to optimize hyperparameters, how to make use of external resources such as long-term memories or external software or large databases or the Internet, and how best to acquire new data. All of these actions will result in Agent AIs more intelligent than Tool AIs, in addition to their greater economic competitiveness. Thus, Tool AIs will be inferior to Agent AIs in both actions and intelligence, implying use of Tool AIs is a even more highly unstable equilibrium than previously argued, as users of Agent AIs will be able to outcompete them on two dimensions (and not just one).</p>">Tool AIs Want To Be Agent AIs</a></p></li>
<li><p><a href="./Danbooru2019" class="docMetadata" data-popup-title="Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2015" data-popup-abstract="<p>Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.</p><p>The best known booru, with a focus on quality, is <a href=&quot;https://danbooru.donmai.us/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Danbooru</a>. We create &amp;amp; provide a torrent which contains ~3tb of 3.69m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.</p><p>Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.</p>">Danbooru2019 Anime Image Dataset</a></p></li>
<li><a href="./Faces" class="docMetadata" data-popup-title="Making Anime Faces With StyleGAN" data-popup-author="Gwern Branwen" data-popup-date="4 Feb 2019" data-popup-abstract="<p>Generative neural networks, such as GANs, have <a href=&quot;#why-dont-gans-work&quot;>struggled for years</a> to generate decent-quality anime faces, despite their great success with photographic imagery such as real human faces. The task has now been effectively solved, for anime faces as well as many other domains, by the development of a new generative adversarial network, <a href=&quot;https://arxiv.org/abs/1812.04948&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;A Style-Based Generator Architecture for Generative Adversarial Networks&quot; data-popup-author=&quot;Tero Karras, Samuli Laine, Timo Aila&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.&quot; title=&quot;&amp;#39;A Style-Based Generator Architecture for Generative Adversarial Networks&amp;#39;, Karras et al 2018&quot;><em>StyleGAN</em></a>, whose <a href=&quot;https://github.com/NVlabs/stylegan&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>source code</a> was released in February 2019.</p><p>I <a href=&quot;#examples&quot;>show off</a> my StyleGAN anime faces &amp;amp; videos, provide downloads, provide the âmissing manualâ &amp;amp; explain how I trained them based on <a href=&quot;./Danbooru2019&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Danbooru2019: A Large-Scale Crowdsourced and Tagged Anime Illustration Dataset&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;15 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Deep learning for computer revision relies on large annotated datasets. Classification/categorization has benefited from the creation of ImageNet, which classifies 1m photos into 1000 categories. But classification/categorization is a coarse description of an image which limits application of classifiers, and there is no comparably large dataset of images with many tags or labels which would allow learning and detecting much richer information about images. Such a dataset would ideally be &amp;amp;gt;1m images with at least 10 descriptive tags each which can be publicly distributed to all interested researchers, hobbyists, and organizations. There are currently no such public datasets, as ImageNet, Birds, Flowers, and MS COCO fall short either on image or tag count or restricted distribution. I suggest that the âimage -boorusâ be used. The image boorus are longstanding web databases which host large numbers of images which can be âtaggedâ or labeled with an arbitrary number of textual descriptions; they were developed for and are most popular among fans of anime, who provide detailed annotations.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The best known booru, with a focus on quality, is &amp;lt;a href=&amp;quot;https://danbooru.donmai.us/&amp;quot;&amp;gt;Danbooru&amp;lt;/a&amp;gt;. We create &amp;amp;amp; provide a torrent which contains ~3tb of 3.69m images with 92.7m tag instances (of 365k defined tags, ~27.8/image) covering Danbooru from 24 May 2005 through 31 December 2018 (final ID: #3,368,713), providing the image files &amp;amp;amp; a JSON export of the metadata. We also provide a smaller torrent of SFW images downscaled to 512x512px JPGs (241GB; 2,232,462 images) for convenience.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;Our hope is that a Danbooru2019 dataset can be used for rich large-scale classification/tagging &amp;amp;amp; learned embeddings, test out the transferability of existing computer vision techniques (primarily developed using photographs) to illustration/anime-style images, provide an archival backup for the Danbooru community, feed back metadata improvements &amp;amp;amp; corrections, and serve as a testbed for advanced techniques such as conditional image generation or style transfer.&amp;lt;/p&amp;gt;&quot;>Danbooru2017/2018/2019</a> with source code for the <a href=&quot;#data-preparation&quot;>data preprocessing</a>, document <a href=&quot;#installation&quot;>installation</a> &amp;amp; <a href=&quot;#configuration&quot;>configuration</a> &amp;amp; <a href=&quot;#running&quot;>training tricks</a>.</p><p>For application, I document various scripts for generating <a href=&quot;#sampling&quot;>images &amp;amp; videos</a>, briefly <a href=&quot;#twdne&quot;>describe the website</a> <a href=&quot;https://www.thiswaifudoesnotexist.net&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;><q>âThis Waifu Does Not Existâ</q></a> <a href=&quot;./TWDNE&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;This Waifu Does Not Exist&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;19 Feb 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, &amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;âThis Waifu Does Not Existâ&amp;lt;/a&amp;gt; for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or &amp;lt;a href=&amp;quot;https://klimaleksus.github.io/FindTwin/&amp;quot; title=&amp;quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&amp;quot;&amp;gt;online&amp;lt;/a&amp;gt; &amp;lt;a href=&amp;quot;https://github.com/darabos/high-five-trading&amp;quot; title=&amp;quot;Action stock exchange game for Repl.it Game Jam 2019&amp;quot;&amp;gt;games&amp;lt;/a&amp;gt;, uploaded to Pixiv, and used in a research paper (&amp;lt;a href=&amp;quot;https://arxiv.org/abs/1904.01774&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot; data-popup-author=&amp;quot;Atsuhiro Noguchi, Tatsuya Harada&amp;quot; data-popup-date=&amp;quot;2019-08-21&amp;quot; data-popup-abstract=&amp;quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&amp;quot; title=&amp;quot;Image Generation from Small Datasets via Batch Statistics Adaptation&amp;quot;&amp;gt;Noguchi &amp;amp;amp; Harada 2019&amp;lt;/a&amp;gt;). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, &amp;lt;a href=&amp;quot;https://waifulabs.com/&amp;quot;&amp;gt;Waifu Labs&amp;lt;/a&amp;gt;, which generates even better anime faces than my StyleGAN results.&amp;lt;/p&amp;gt;&quot;>I set up</a> as a public demo, discuss how the trained models can be <a href=&quot;#transfer-learning&quot;>used for transfer learning</a> such as generating high-quality faces of anime characters with small datasets (eg <a href=&quot;#holo&quot;>Holo</a> or <a href=&quot;#asuka&quot;>Asuka Souryuu Langley</a>), and touch on <a href=&quot;#reversing-stylegan-to-control-modify-images&quot;>more advanced StyleGAN applications</a> like encoders &amp;amp; controllable generation.</p><p>The <a href=&quot;#appendix&quot;>appendix</a> gives samples of my failures with earlier GANs for anime face generation, and I provide samples &amp;amp; model from a relatively large-scale <a href=&quot;#biggan&quot;>BigGAN</a> training run suggesting that BigGAN may be the next step forward to generating full-scale anime images.</p><p>A minute of reading could save an hour of debugging!</p>">Making Anime Faces with Neural Nets</a></li>
<li><p><a href="./Coin-flip" class="docMetadata" data-popup-title="The Kelly Coin-Flipping Game: Exact Solutions via Decision Trees" data-popup-author="Gwern Branwen, Arthur B., nshepperd, FeepingCreature, Gurkenglas" data-popup-date="19 Jan 2017" data-popup-abstract="<p>Haghani &amp;amp; Dewey 2016 experiment with a double-or-nothing coin-flipping game where the player starts with $25 and has an edge of 60%, and can play 300 times, choosing how much to bet each time, winning up to a maximum ceiling of $250. Most of their subjects fail to play well, earning an average $91, compared to Haghani &amp;amp; Dewey 2016âs heuristic benchmark of ~$240 in winnings achievable using a modified Kelly Criterion as their strategy. The KC, however, is not optimal for this problem as it ignores the ceiling and limited number of plays.</p><p>We solve the problem of the value of optimal play exactly by using decision trees &amp;amp; dynamic programming for calculating the value function, with implementations in R, Haskell, and C. We also provide a closed-form exact value formula in R &amp;amp; Python, several approximations using Monte Carlo/random forests/neural networks, visualizations of the value function, and a Python implementation of the game for the OpenAI Gym collection. We find that optimal play yields $246.61 on average (rather than ~$240), and so the human players actually earned only 36.8% of what was possible, losing $155.6 in potential profit. Comparing decision trees and the Kelly criterion for various horizons (bets left), the relative advantage of the decision tree strategy depends on the horizon: it is highest when the player can make few bets (at <em>b</em>=23, with a difference of ~$36), and decreases with number of bets as more strategies hit the ceiling.</p><p>In the Kelly game, the maximum winnings, number of rounds, and edge are fixed; we describe a more difficult generalized version in which the 3 parameters are drawn from Pareto, normal, and beta distributions and are unknown to the player (who can use Bayesian inference to try to estimate them during play). Upper and lower bounds are estimated on the value of this game. In the variant of this game where subjects are not told the exact edge of 60%, a Bayesian decision tree approach shows that performance can closely approach that of the decision tree, with a penalty for 1 plausible prior of only $1. Two deep reinforcement learning agents, DQN &amp;amp; DDPG, are implemented but DQN fails to learn and DDPG doesnât show acceptable performance, indicating better deep RL methods may be required to solve the generalized Kelly game.</p>">Kelly Coin-Flip Game: Solutions</a></p></li>
<li><p><a href="./Backstop" class="docMetadata" data-popup-title="Evolution as Backstop for Reinforcement Learning" data-popup-author="Gwern Branwen" data-popup-date="6 Dec 2018" data-popup-abstract="<p>One defense of free markets notes the inability of non-market mechanisms to solve planning &amp;amp; optimization problems. This has difficulty with Coaseâs paradox of the firm, and I note that the difficulty is increased by the fact that with improvements in computers, algorithms, and data, ever larger planning problems <em>are</em> solved. Expanding on some Cosma Shalizi comments, I suggest a multi-level optimization paradigm: many systems can be seen as having two (or more) levels where a slow sample-inefficient but ground-truth âouterâ loss such as death, bankruptcy, or reproductive fitness, trains &amp;amp; constrains a fast sample-efficient but possibly misguided âinnerâ loss which is used by learned mechanisms such as neural networks or linear programming group selection perspective. So, one reason for free-market or evolutionary or Bayesian methods in general is that while poorer at planning/optimization in the short run, they have the advantage of simplicity and operating on ground-truth values, and serve as a constraint on the more sophisticated non-market mechanisms. I illustrate by discussing corporations, multicellular life, reinforcement learning &amp;amp; meta-learning in AI, and pain in humans. This view suggests that are inherent balances between market/non-market mechanisms which reflect the relative advantages between a slow unbiased method and faster but potentially arbitrarily biased methods.</p>">Evolution as Backstop for Learning</a></p></li>
<li><p><a href="./Tanks" class="docMetadata" data-popup-title="The Neural Net Tank Urban Legend" data-popup-author="Gwern Branwen" data-popup-date="20 Sep 2011" data-popup-abstract="<p>A cautionary tale in artificial intelligence tells about researchers training an neural network (NN) to detect tanks in photographs, succeeding, only to realize the photographs had been collected under specific conditions for tanks/non-tanks and the NN had learned something useless like time of day. This story is often told to warn about the limits of algorithms and importance of data collection to avoid <q>âdataset biasâ</q>/<q>âdata leakageâ</q> where the collected data can be solved using algorithms that do not generalize to the true data distribution, but the tank story is usually never sourced.</p><p>I collate many extent versions dating back a quarter of a century to 1992 along with two NN-related anecdotes from the 1960s; their contradictions &amp;amp; details indicate a classic âurban legendâ, with a probable origin in a speculative question in the 1960s by Edward Fredkin at an AI conference about some early NN research, which was subsequently classified &amp;amp; never followed up on.</p><p>I suggest that dataset bias is real but exaggerated by the tank story, giving a misleading indication of risks from deep learning and that it would be better to not repeat it but use real examples of dataset bias and focus on larger-scale risks like AI systems optimizing for wrong utility functions.</p>">The Neural Net Tank Urban Legend</a></p></li>
<li><p>Neural net poems: <a href="./RNN-metadata" class="docMetadata" data-popup-title="RNN metadata for mimicking individual author style" data-popup-author="Gwern Branwen" data-popup-date="12 Sep 2015" data-popup-abstract="<p>Char-RNNs are unsupervised generative models which learn to mimic text sequences. I suggest extending char-RNNs with inline metadata such as genre or author prefixed to each line of input, allowing for better &amp;amp; more efficient metadata, and more controllable sampling of generated output by feeding in desired metadata. An experiment using <code>torch-rnn</code> on a set of ~30 Project Gutenberg e-books (1 per author) to train a large char-RNN shows that a char-RNN can learn to remember metadata such as authors, learn associated prose styles, and often generate text visibly similar to that of a specified author.</p><p>I further try &amp;amp; fail to train <a href=&quot;#geocities-char-rnn&quot;>a char-RNN on Geocities HTML</a> for unclear reasons.</p><p>More successfully, <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>I experiment with a recently-developed alternative to char-RNNs</a>, the Transformer NN architecture, by finetuning training OpenAIâs GPT-2-117M Transformer model on a much larger (117MB) Project Gutenberg poetry corpus using both unlabeled lines &amp;amp; lines with inline metadata (the source book). The poetry generated is of considerably higher quality than my char-RNNs.</p>"><span class="smallcaps-auto">RNN</span>s</a>/<a href="./GPT-2" class="docMetadata" data-popup-title="GPT-2 Neural Network Poetry" data-popup-author="Gwern Branwen" data-popup-date="3 March 2019" data-popup-abstract="<p>In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: <q>âGPT-2-poetryâ</q>, trained on the poems as a continuous stream of text, and <q>âGPT-2-poetry-prefixâ</q>, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.</p><p>With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.</p>"><span class="smallcaps-auto">GPT-2</span>s</a></p></li>
<li><p><a href="https://www.thiswaifudoesnotexist.net/" class="docMetadata" data-popup-title="ThisWaifuDoesNotExist.net" data-popup-author="Gwern Branwen" data-popup-date="2019-02-19" data-popup-abstract="<a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;><code>ThisWaifuDoesNotExist.net</code></a> (<a href=&quot;https://www.gwern.net/TWDNE&quot;>TWDNE</a>) is a static website which uses JS to display random <a href=&quot;https://www.gwern.net/Faces&quot;>anime faces generated by StyleGAN</a> neural networks, along with <a href=&quot;https://www.gwern.net/GPT-2&quot;>GPT-2</a>-generated 'anime plot summaries'.<br><figure><img src=&quot;/images/gan/thiswaifudoesnotexist.png&quot; alt=&quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&quot; /><figcaption>A screenshot of <q>âThis Waifu Does Not Existâ</q> (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.</figcaption></figure>">This Waifu Does Not Exist</a> (<a href="./TWDNE" class="docMetadata" data-popup-title="This Waifu Does Not Exist" data-popup-author="Gwern Branwen" data-popup-date="19 Feb 2019" data-popup-abstract="<p>Generating high-quality anime faces has long been a task neural networks struggled with. The invention of StyleGAN in 2018 has effectively solved this task and I have trained a StyleGAN model which can generate high-quality anime faces at 512px resolution. To show off the recent progress, I made a website, <a href=&quot;https://www.thiswaifudoesnotexist.net/&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;ThisWaifuDoesNotExist.net&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2019-02-19&quot; data-popup-abstract=&quot;&amp;lt;a href=&amp;quot;https://www.thiswaifudoesnotexist.net/&amp;quot;&amp;gt;&amp;lt;code&amp;gt;ThisWaifuDoesNotExist.net&amp;lt;/code&amp;gt;&amp;lt;/a&amp;gt; (&amp;lt;a href=&amp;quot;https://www.gwern.net/TWDNE&amp;quot;&amp;gt;TWDNE&amp;lt;/a&amp;gt;) is a static website which uses JS to display random &amp;lt;a href=&amp;quot;https://www.gwern.net/Faces&amp;quot;&amp;gt;anime faces generated by StyleGAN&amp;lt;/a&amp;gt; neural networks, along with &amp;lt;a href=&amp;quot;https://www.gwern.net/GPT-2&amp;quot;&amp;gt;GPT-2&amp;lt;/a&amp;gt;-generated &amp;#39;anime plot summaries&amp;#39;.&amp;lt;br&amp;gt;&amp;lt;figure&amp;gt;&amp;lt;img src=&amp;quot;/images/gan/thiswaifudoesnotexist.png&amp;quot; alt=&amp;quot;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;quot; /&amp;gt;&amp;lt;figcaption&amp;gt;A screenshot of âThis Waifu Does Not Existâ (TWDNE) showing a random StyleGAN-generated anime face and a random GPT-2-117M text sample conditioned on anime keywords/phrases.&amp;lt;/figcaption&amp;gt;&amp;lt;/figure&amp;gt;&quot;><q>âThis Waifu Does Not Existâ</q></a> for displaying random StyleGAN faces. TWDNE displays a different neural-net-generated face &amp;amp; plot summary every 15s. The site was popular and went viral online, especially in China. TWDNE faces have been used as screensavers, user avatars, character art for game packs or <a href=&quot;https://klimaleksus.github.io/FindTwin/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Find Twin v1.0, by Kly_Men_COmpany: This is a simple game, where you need to find the same image among other similar images.&quot;>online</a> <a href=&quot;https://github.com/darabos/high-five-trading&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Action stock exchange game for Repl.it Game Jam 2019&quot;>games</a>, uploaded to Pixiv, and used in a research paper (<a href=&quot;https://arxiv.org/abs/1904.01774&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot; data-popup-author=&quot;Atsuhiro Noguchi, Tatsuya Harada&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;Thanks to the recent development of deep generative models, it is becoming possible to generate high-quality images with both fidelity and diversity. However, the training of such generative models requires a large dataset. To reduce the amount of data required, we propose a new method for transferring prior knowledge of the pre-trained generator, which is trained with a large dataset, to a small dataset in a different domain. Using such prior knowledge, the model can generate images leveraging some common sense that cannot be acquired from a small dataset. In this work, we propose a novel method focusing on the parameters for batch statistics, scale and shift, of the hidden layers in the generator. By training only these parameters in a supervised manner, we achieved stable training of the generator, and our method can generate higher quality images compared to previous methods without collapsing even when the dataset is small (~100). Our results show that the diversity of the filters acquired in the pre-trained generator is important for the performance on the target domain. By our method, it becomes possible to add a new class or domain to a pre-trained generator without disturbing the performance on the original domain.&quot; title=&quot;Image Generation from Small Datasets via Batch Statistics Adaptation&quot;>Noguchi &amp;amp; Harada 2019</a>). TWDNE results also helped inspired Sizigi Studioâs online interactive waifu GAN, <a href=&quot;https://waifulabs.com/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Waifu Labs</a>, which generates even better anime faces than my StyleGAN results.</p>">details</a>)</p></li>
<li><p><a href="./GPT-2-music" class="docMetadata" data-popup-title="GPT-2 Folk Music" data-popup-author="Gwern Branwen" data-popup-date="2019-11-01" data-popup-abstract="<p>In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp; copying of patterns, without problems with the hidden-state bottleneck.</p> <p>I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after <a href=&quot;/GPT-2-music#spaceless-model&quot;>fixing that</a>, I successfully trained it on <em>n</em>=205,304 ABC music pieces taken from The Session &amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.</p> <p>The model &amp; dataset are available for download, and I provide for listening selected <a href=&quot;/GPT-2-music#samples&quot;>music samples</a> as well as medleys of random samples from throughout training.</p>"><span class="smallcaps-auto">GPT</span>-2 Folk Music</a></p></li>
<li><p><a href="./GPT-2-preference-learning" class="docMetadata" data-popup-title="GPT-2 Preference Learning for Music and Poetry Generation" data-popup-author="Gwern Branwen" data-popup-date="16 Dec 2019" data-popup-abstract="<p>Standard language generation neural network models, like GPT-2, are trained via likelihood training to imitate human text corpuses. Generated text suffers from persistent flaws like repetition, due to myopic generation word-by-word, and cannot improve on the training data because they are trained to predict ârealisticâ completions of the training data.</p><p>A proposed alternative is to use reinforcement learning to train the NNs, to encourage global properties like coherence &amp;amp; lack of repetition, and potentially improve over the original corpusâs average quality. <em>Preference learning</em> trains a reward function on human ratings, and uses that as the âenvironmentâ for a blackbox DRL algorithm like PPO.</p><p>OpenAI released a codebase implementing this dual-model preference learning approach for textual generation, based on GPT-2. Having previously used <a href=&quot;./GPT-2&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Neural Network Poetry&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;3 March 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In February 2019, following up on my 2015â2016 text-generation experiments with char-RNNs, I experiment with the cutting-edge Transformer NN architecture for language modeling &amp;amp;amp; text generation. Using OpenAIâs GPT-2-117M (117M) model pre-trained on a large Internet corpus and nshepperdâs finetuning code, I retrain GPT-2-117M on a large (117MB) Project Gutenberg poetry corpus. I demonstrate how to train 2 variants: âGPT-2-poetryâ, trained on the poems as a continuous stream of text, and âGPT-2-poetry-prefixâ, with each line prefixed with the metadata of the PG book it came from. In May 2019, I trained the next-largest GPT-2, 345M, similarly, for a further quality boost in generated poems.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;With just a few GPU-days on 1080ti GPUs, GPT-2-117M finetuning can produce high-quality poetry which is more thematically consistent than my char-RNN poems, capable of modeling subtle features like rhyming, and sometimes even a pleasure to read. I list the many possible ways to improve poem generation and further approach human-level poems.&amp;lt;/p&amp;gt;&quot;>GPT-2 for poetry</a> &amp;amp; <a href=&quot;./GPT-2-music&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;GPT-2 Music&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;1 Nov 2019&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In November 2019, I experimented with training a GPT-2 neural net model to generate folk music in the high-level ABC music text format, following previous work in 2016 which used a char-RNN trained on a âThe Sessionâ dataset. A GPT-2 hypothetically can improve on an RNN by better global coherence &amp;amp;amp; copying of patterns, without problems with the hidden-state bottleneck.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;I encountered problems with the standard GPT-2 modelâs encoding of text which damaged results, but after fixing that, I successfully trained it on n_=205,304 ABC music pieces taken from The Session &amp;amp;amp; ABCnotation.com. The resulting music samples are in my opinion quite pleasant.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;The model &amp;amp;amp; dataset are available for download, and I provide for listening selected music samples as well as medleys of random samples from throughout training.&amp;lt;/p&amp;gt;&quot;>music generation</a>, I experimented with GPT-2 preference learning for unconditional music and poetry generation.</p><p>I found that preference learning seemed to work better for music than poetry, and seemed to reduce the presence of repetition artifacts, but the results, at <em>n</em>â10,000 ratings, are not dramatically better than alternative improvements like scaling up models or more thorough data-cleaning or more stringent sample curation.</p><p>Working with it, I suspect that preference learning is unnecessarily sample-inefficient &amp;amp; data-inefficient, and that the blackbox reinforcement learning approach is inferior to directly using the reward model to optimize text samples, and propose two major architectural overhauls: have the reward model directly model the implied utility of every datapoint, and drop the agent model entirely in favor of backprop-powered gradient ascent which optimizes sequences to maximize the reward modelâs output.</p>">Preference learning GPT-2 music</a></p></li>
<li><p><a href="./Hyperbolic-Time-Chamber" class="docMetadata" data-popup-title="The Hyperbolic Time Chamber as Brain Emulation Analogy" data-popup-author="Gwern Branwen" data-popup-date="29 Aug 2012" data-popup-abstract="<p>A time dilation tool from an anime is discussed for its practical use on Earth; there seem surprisingly few uses and none that will change the world, due to the severe penalties humans would incur while using it, and basic constraints like Amdahlâs law limit the scientific uses. A comparison with the position of an Artificial Intelligence such as an emulated human brain seems fair, except most of the time dilation disadvantages do not apply or can be ameliorated and hence any speedups could be quite effectively exploited. I suggest that skeptics of the idea that speedups give advantages are implicitly working off the crippled time dilation tool and not making allowance for the <em>dis</em>analogies.</p>">Hyperbolic Time Chambers as Brain Ems</a></p></li>
</ul>
</section>
<section id="cs" class="level1">
<h1><a href="#cs" title="Link to section: 'CS'">CS</a></h1>
<ul>
<li><a href="./haskell/Summer-of-Code" class="docMetadata" data-popup-title="Summers of Code, 2006-2013" data-popup-author="Gwern Branwen" data-popup-date="11 Feb 2009" data-popup-abstract="<p>A compilation of Haskell-related student projects 2006-2013, with evaluations of their usefulness to the Haskell community, thoughts on what makes a good project, and predictions for 2011-2013.</p>">Haskell Summer of Code</a></li>
<li><a href="./Archiving-URLs" class="docMetadata" data-popup-title="Archiving URLs" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2011" data-popup-abstract="<p>Links on the Internet last forever or a year, whichever comes first. This is a major problem for anyone serious about writing with good references, as link rot will cripple several percent of all links each year, and compounding.</p><p>To deal with link rot, I present my multi-pronged archival strategy using a combination of scripts, daemons, and Internet archival services: URLs are regularly dumped from both my web browserâs daily browsing and my website pages into an archival daemon I wrote, which pre-emptively downloads copies locally and attempts to archive them in the Internet Archive. This ensures a copy will be available indefinitely from one of several sources. Link rot is then detected by regular runs of <code>linkchecker</code>, and any newly dead links can be immediately checked for alternative locations, or restored from one of the archive sources.</p><p>As an additional flourish, my local archives are <a href=&quot;./Timestamping&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Easy Cryptographic Timestamping of Files&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;4 Dec 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Local archives are useful for personal purposes, but sometimes, in investigations that may be controversial, you want to be able to prove that the copy you downloaded was not modified and you need to &amp;lt;em&amp;gt;timestamp&amp;lt;/em&amp;gt; it and prove the exact file existed on or before a certain date. This can be done by creating a cryptographic hash of the file and then publishing that hash to global chains like centralized digital timestampers or the decentralized Bitcoin blockchain. Current timestamping mechanisms tend to be centralized, manual, cumbersome, or cost too much to use routinely. Centralization can be overcome by timestamping to Bitcoin; costing too much can be overcome by batching up an arbitrary number of hashes and creating just 1 hash/timestamp covering them all; manual &amp;amp;amp; cumbersome can be overcome by writing programs to handle all of this and incorporating them into oneâs workflow. So using an efficient cryptographic timestamping service (the OriginStamp Internet service), we can write programs to automatically &amp;amp;amp; easily timestamp arbitrary files &amp;amp;amp; strings, timestamp every commit to a Git repository, and webpages downloaded for archival purposes. We can implement the same idea offline, without reliance on OriginStamp, but at the cost of additional software dependencies like a Bitcoin client.&amp;lt;/p&amp;gt;&quot;>efficiently cryptographically timestamped using Bitcoin</a> in case forgery is a concern, and I demonstrate a simple compression trick for <a href=&quot;./Archiving-URLs#sort---key-compression-trick&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;The &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; compression trick (CLI folklore)&quot; data-popup-author=&quot;Gwern Branwen&quot; data-popup-date=&quot;2014-03-03&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Programming folklore notes that one way to get better lossless compression efficiency is to rearrange files inside the archive to group âsimilarâ files together and expose redundancy to the compressor, in accordance with information-theoretical principles. A particularly easy and broadly-applicable way of doing this, which does not require using any unusual formats or tools and is fully compatible with the default archive methods, is to sort the files by &amp;lt;em&amp;gt;filename&amp;lt;/em&amp;gt; and especially file extension. I show how to do this with the standard Unix command-line &amp;lt;code&amp;gt;sort&amp;lt;/code&amp;gt; tool, using the so-called â&amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trickâ, and give examples of the large space-savings possible from my archiving work for personal website mirrors and for making &amp;lt;a href=&amp;quot;/DNM-archives&amp;quot;&amp;gt;darknet market mirror datasets&amp;lt;/a&amp;gt; where the redundancy at the file level is particularly extreme and the &amp;lt;code&amp;gt;sort --key&amp;lt;/code&amp;gt; trick shines compared to the naive approach.&amp;lt;/p&amp;gt;&quot;>substantially reducing sizes of large web archives</a> such as crawls (particularly useful for repeated crawls such as my <a href=&quot;./DNM-archives&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Darknet Market Archives (2013-2015)&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;1 Dec 2013&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Dark Net Markets (DNM) are online markets typically hosted as Tor hidden services providing escrow services between buyers &amp;amp;amp; sellers transacting in Bitcoin or other cryptocoins, usually for drugs or other illegal/regulated goods; the most famous DNM was Silk Road 1, which pioneered the business model in 2011.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;From 2013â2015, I scraped/mirrored on a weekly or daily basis all existing English-language DNMs as part of my research into their &amp;lt;a href=&amp;quot;./Silk-Road&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Silk Road 1: Theory &amp;amp;amp; Practice&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;usage&amp;lt;/a&amp;gt;, &amp;lt;a href=&amp;quot;./DNM-survival&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Darknet Market mortality risks&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a dataset of 87 public English-language darknet markets (DNMs) 2011-2016 in the vein of the famous &amp;amp;lt;a href=&amp;amp;quot;./Silk-Road&amp;amp;quot; class=&amp;amp;quot;docMetadata&amp;amp;quot; data-popup-title=&amp;amp;quot;Silk Road 1: Theory &amp;amp;amp;amp; Practice&amp;amp;quot; data-popup-author=&amp;amp;quot;11 Jul 2011&amp;amp;quot; data-popup-date=&amp;amp;quot;gwern&amp;amp;quot; data-popup-abstract=&amp;amp;quot;&amp;amp;amp;lt;div id=&amp;amp;amp;quot;abstract&amp;amp;amp;quot;&amp;amp;amp;gt;&amp;amp;amp;lt;blockquote&amp;amp;amp;gt;&amp;amp;amp;lt;p&amp;amp;amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;amp;amp;lt;/p&amp;amp;amp;gt;&amp;amp;amp;lt;/blockquote&amp;amp;amp;gt;&amp;amp;quot;&amp;amp;gt;Silk Road 1&amp;amp;lt;/a&amp;amp;gt;, recording their openings/closing and relevant characteristics. A survival analysis indicates the markets follow a Type TODO lifespan, with a median life of TODO months. Risk factors include TODO. With the best model, I generate estimates for the currently-operating markets.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;lifetimes/characteristics&amp;lt;/a&amp;gt;, &amp;amp;amp; &amp;lt;a href=&amp;quot;./DNM-arrests&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Tor DNM-related arrests, 2011-2015&amp;quot; data-popup-author=&amp;quot;gwern&amp;quot; data-popup-date=&amp;quot;gwern&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;I compile a table and discussion of all known arrests and prosecutions related to English-language Tor-Bitcoin darknet markets (DNMs) such as Silk Road 1, primarily 2011â2015, along with discussion of how they came to be arrested.&amp;amp;lt;/p&amp;amp;gt;&amp;quot;&amp;gt;legal riskiness&amp;lt;/a&amp;gt;; these scrapes covered vendor pages, feedback, images, etc. In addition, I made or obtained copies of as many other datasets &amp;amp;amp; documents related to the DNMs as I could.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This uniquely comprehensive collection is now publicly released as a 50GB (~1.6TB uncompressed) collection covering 89 DNMs &amp;amp;amp; 37+ related forums, representing &amp;amp;lt;4,438 mirrors, and is available for any research.&amp;lt;/p&amp;gt;&amp;lt;p&amp;gt;This page documents the download, contents, interpretation, and technical methods behind the scrapes.&amp;lt;/p&amp;gt;&quot;>DNM archives</a>).</p>">Archiving <span class="smallcaps-auto">URL</span>s</a></li>
<li><a href="./The-3-Grenades" class="docMetadata" data-popup-title="The Three Grenades" data-popup-author="Gwern Branwen" data-popup-date="24 Nov 2008" data-popup-abstract="CMU game connected to error coding theory">The 3 Grenades</a></li>
<li><a href="./Turing-complete" class="docMetadata" data-popup-title="Surprisingly Turing-Complete" data-popup-author="Gwern Branwen" data-popup-date="9 Dec 2012" data-popup-abstract="<p>âComputersâ, in the sense of being Turing-complete, are extremely common. Almost any system of sufficient complexityâunless carefully engineered otherwiseâmay be found to âaccidentallyâ support Turing-complete somewhere inside it, even systems which would appear to have not the slightest thing to do with computation. Software systems are especially susceptible to this, which often leads to serious security problems as the Turing-complete components can be used to run attacks on the rest of the system.</p><p>I provide a running catalogue of systems which have been, surprisingly, demonstrated to be Turing-complete.</p>">Surprisingly Turing-Complete</a></li>
<li><a href="./haskell/Archiving-GitHub" class="docMetadata" data-popup-title="Archiving GitHub" data-popup-author="Gwern Branwen" data-popup-date="20 Mar 2011" data-popup-abstract="Scraping and downloading Haskell-related repositories from GitHub">Archiving GitHub</a></li>
<li><a href="./Simulation-inferences" class="docMetadata" data-popup-title="Simulation Inferences" data-popup-author="Gwern Branwen" data-popup-date="29 May 2009" data-popup-abstract="How small must be the computer simulating the universe?">Simulation inferences</a></li>
<li><a href="./Choosing-Software" class="docMetadata" data-popup-title="Choosing Software" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Criteria making software useful long-term &amp; worth learning in detail">Choosing Software</a></li>
<li><a href="./haskell/Run-Length-Encoding" class="docMetadata" data-popup-title="Golfing Run Length Encoding in Haskell" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Haskell: step by step refactoring to concision">Elegant Run Length Encoding</a></li>
<li><a href="./haskell/Wikipedia-Archive-Bot" class="docMetadata" data-popup-title="Writing a Wikipedia Link Archive Bot" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="<p>This is a 2008 tutorial demonstrating how to write a Haskell program to automatically archive Internet links into WebCite &amp;amp; Internet Archive to avoid linkrot, by parsing WP dumps, downloading &amp;amp; parsing WP articles for external links with the TagSoup HTML parsing library, using the WebCite/IA APIs to archive them, and optimizing runtime. This approach is suitable for one-off crawls but not for live archiving using the RSS feed, see <a href=&quot;../haskell/Wikipedia-RSS-Archive-Bot&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Writing a Wikipedia RSS Link Archive Bot&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;02 Nov 2009&quot; data-popup-abstract=&quot;Archiving using Wikipedia Recent Changes RSS feed&quot;>Wikipedia RSS Archive Bot</a> for a demonstration of how one could write a RSS-oriented daemon. These Haskell scripts are (hopefully) obsoleted by later IA/WMF initiatives dealing with linkrot. For a more general approach suitable for personal use, see the writeup of <code>archiver-bot</code> in <a href=&quot;../Archiving-URLs&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Archiving URLs&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;10 Mar 2011&quot; data-popup-abstract=&quot;Archiving the Web, because nothing lasts forever: statistics, online archive services, extracting URLs automatically from browsers, and creating a daemon to regularly back up URLs to multiple sources.&quot;>Archiving URLs</a>.</p>">Wikipedia Archive Bot</a></li>
<li><a href="./haskell/Wikipedia-RSS-Archive-Bot" class="docMetadata" data-popup-title="Writing a Wikipedia RSS Link Archive Bot" data-popup-author="Gwern Branwen" data-popup-date="02 Nov 2009" data-popup-abstract="Archiving using Wikipedia Recent Changes RSS feed"><span class="smallcaps-auto">WP</span> <span class="smallcaps-auto">RSS</span> bot</a></li>
<li><a href="./Resilient-Haskell-Software" class="docMetadata" data-popup-title="Resilient Haskell Software" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Lessons learned about bitrot in Haskell software">Resilient Haskell Software</a></li>
<li><a href="./Evolutionary-Licenses" class="docMetadata" data-popup-title="Evolutionary Software Licenses" data-popup-author="Gwern Branwen" data-popup-date="27 Jan 2009" data-popup-abstract="Game theory on BSD vs. GPL: partnership? Is GPL an evolutionary stable strategy against invasion by proprietary copyright strategies?">Evolutionary Licenses</a></li>
<li><em><span class="smallcaps-auto">SICP</span></em> <a href="./sicp/Introduction" class="docMetadata" data-popup-title="SICP Introduction" data-popup-author="Gwern Branwen" data-popup-date="14 Mar 2009" data-popup-abstract="Links to various resources for SICP studying">Introduction</a> (<a href="./sicp/Chapter-1.1" class="docMetadata" data-popup-title="SICP Chapter 1.1 notes" data-popup-author="Gwern Branwen" data-popup-date="14 Mar 2009" data-popup-abstract="Syntax, function definitions">ch1.1</a>, <a href="./sicp/Chapter-1.2" class="docMetadata" data-popup-title="SICP Chapter 1.2 notes" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="recursion into iteration; primality testing">1.2</a>, <a href="./sicp/Chapter-1.3" class="docMetadata" data-popup-title="SICP Chapter 1.3" data-popup-author="Gwern Branwen" data-popup-date="09 Jan 2010" data-popup-abstract="Generalizing functions with hardwired values">1.3</a>)</li>
<li><a href="./WiFi" class="docMetadata" data-popup-title="Internet WiFi improvement" data-popup-author="Gwern Branwen" data-popup-date="20 Oct 2016" data-popup-abstract="<p>My laptop in my apartment receives Internet via a WiFi repeater to another house, yielding slow speeds and frequent glitches. I replaced the obsolete WiFi router and increased connection speeds somewhat but still inadequate. For a better solution, I used a directional antenna to connect directly to the new WiFi router, which, contrary to my expectations, yielded a ~6x increase in speed. Extensive benchmarking of all possible arrangements of laptops/dongles/repeaters/antennas/routers/positions shows that the antenna+router is inexpensive and near optimal speed, and that the only possible improvement would be a hardwired Ethernet line, which I installed a few weeks later after learning it was not as difficult as I thought it would be.</p>">Internet WiFi improvement</a></li>
<li><a href="./AB-testing" class="docMetadata" data-popup-title="A/B testing long-form readability on gwern.net" data-popup-author="Gwern Branwen" data-popup-date="16 Jun 2012" data-popup-abstract="<p>To gain some statistical &amp;amp; web development experience and to improve my readersâ experiences, I have been running a series of CSS A/B tests since June 2012. As expected, most do not show any meaningful difference.</p>">A/B testing CSS &amp; HTML</a></li>
</ul>
</section>
<section id="psychology" class="level1">
<h1><a href="#psychology" title="Link to section: 'Psychology'">Psychology</a></h1>
<ul>
<li><a href="./Spaced-repetition" class="docMetadata" data-popup-title="Spaced Repetition for Efficient Learning" data-popup-author="Gwern Branwen" data-popup-date="11 Mar 2009" data-popup-abstract="<p>Spaced repetition is a centuries-old psychological technique for efficient memorization &amp;amp; practice of skills where instead of attempting to memorize by âcrammingâ, memorization can be done far more efficiently by instead spacing out each review, with increasing durations as one learns the item, with the scheduling done by software. Because of the greater efficiency of its slow but steady approach, spaced repetition can scale to memorizing hundreds of thousands of items (while crammed items are almost immediately forgotten) and is especially useful for foreign languages &amp;amp; medical studies.</p><p>I review what this technique is useful for, some of the large research literature on it and the testing effect (up to ~2013, primarily), the available software tools and use patterns, and miscellaneous ideas &amp;amp; observations on it.</p>">Spaced Repetition &amp; Learning</a></li>
<li><a href="./DNB-FAQ" class="docMetadata" data-popup-title="Dual N-Back FAQ" data-popup-author="Gwern Branwen" data-popup-date="25 Mar 2009" data-popup-abstract="A compendium of DNB, WM, IQ information up to 2015">Dual N-Back <span class="smallcaps-auto">FAQ</span></a> (<a href="./DNB-meta-analysis" class="docMetadata" data-popup-title="Dual N-Back Meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="20 May 2012" data-popup-abstract="<p>I meta-analyze the &amp;gt;19 studies up to 2016 which measure IQ after an <a href=&quot;./DNB-FAQ&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Dual N-Back FAQ&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;25 Mar 2009&quot; data-popup-abstract=&quot;A compendium of DNB, WM, IQ information up to 2015&quot;>n-back</a> intervention, finding (over all studies) a net <a href=&quot;#analysis&quot;>gain</a> (medium-sized) on the post-training IQ tests.</p><p>The size of this increase on IQ test score correlates highly with the methodological concern of whether a study used <a href=&quot;#control-groups&quot;>active or passive control groups</a>. This indicates that the medium effect size is due to methodological problems and that n-back training does not increase subjectsâ underlying fluid intelligence but the gains are due to the motivational effect of passive control groups (who did not train on anything) not trying as hard as the n-back-trained experimental groups on the post-tests. The remaining studies using active control groups find a small positive effect (but this may be due to matrix-test-specific training, undetected publication bias, smaller motivational effects, etc.)</p><p>I also investigate several other n-back claims, criticisms, and indicators of bias, finding:</p><ul><li><a href=&quot;#paymentextrinsic-motivation&quot;>payment reducing performance</a> claim: possible</li><li><a href=&quot;#training-time&quot;>dose-response relationship</a> of n-back training time &amp;amp; IQ gains claim: not found</li><li><a href=&quot;#training-type&quot;>kind of n-back</a> matters: not found</li><li><a href=&quot;#biases&quot;>publication bias</a> criticism: not found</li><li><a href=&quot;#iq-test-time&quot;>speeding of IQ tests</a> criticism: not found</li></ul>">meta-analysis</a>)</li>
<li><a href="./Embryo-selection" class="docMetadata" data-popup-title="Embryo selection for intelligence" data-popup-author="Gwern Branwen" data-popup-date="22 Jan 2016" data-popup-abstract="<p>With genetic predictors of a phenotypic trait, it is possible to select embryos during an in vitro fertilization process to increase or decrease that trait. Extending the work of <a href=&quot;./docs/iq/2014-shulman.pdf&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;417&quot; title=&quot;Embryo Selection for Cognitive Enhancement: Curiosity or Game-changer?&quot;>Shulman &amp;amp; Bostrom 2014</a>/<a href=&quot;https://arxiv.org/abs/1408.3421&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;On the genetic architecture of intelligence and other quantitative  traits&quot; data-popup-author=&quot;Stephen D. H. Hsu&quot; data-popup-date=&quot;2019-08-26&quot; data-popup-abstract=&quot;How do genes affect cognitive ability or other human quantitative traits such as height or disease risk? Progress on this challenging question is likely to be significant in the near future. I begin with a brief review of psychometric measurements of intelligence, introducing the idea of a &amp;quot;general factor&amp;quot; or g score. The main results concern the stability, validity (predictive power), and heritability of adult g. The largest component of genetic variance for both height and intelligence is additive (linear), leading to important simplifications in predictive modeling and statistical estimation. Due mainly to the rapidly decreasing cost of genotyping, it is possible that within the coming decade researchers will identify loci which account for a significant fraction of total g variation. In the case of height analogous efforts are well under way. I describe some unpublished results concerning the genetic architecture of height and cognitive ability, which suggest that roughly 10k moderately rare causal variants of mostly negative effect are responsible for normal population variation. Using results from Compressed Sensing (L1-penalized regression), I estimate the statistical power required to characterize both linear and nonlinear models for quantitative traits. The main unknown parameter s (sparsity) is the number of loci which account for the bulk of the genetic variation. The required sample size is of order 100s, or roughly a million in the case of cognitive ability.&quot; title=&quot;On the genetic architecture of intelligence and other quantitative traits&quot;>Hsu 2014</a>, I consider the case of human intelligence using SNP-based genetic prediction, finding:</p><ul><li>a meta-analysis of <a href=&quot;https://en.wikipedia.org/wiki/GCTA&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Genome-wide complex trait analysis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Genome-wide complex trait analysis (GCTA) Genome-based restricted maximum likelihood (GREML)&amp;lt;/b&amp;gt; is a statistical method for variance component estimation in genetics which quantifies the total narrow-sense (additive) contribution to a trait&amp;#39;s heritability of a particular subset of genetic variants. This is done by directly quantifying the chance genetic similarity of unrelated individuals and comparing it to their measured similarity on a trait; if two unrelated individuals are relatively similar genetically and also have similar trait measurements, then the measured genetics are likely to causally influence that trait, and the correlation can to some degree tell how much. This can be illustrated by plotting the squared pairwise trait differences between individuals against their estimated degree of relatedness. The GCTA framework can be applied in a variety of settings. For example, it can be used to examine changes in heritability over aging and development.. It can also be extended to analyse bivariate genetic correlations between traits. There is an ongoing debate about whether GCTA generates reliable or stable estimates of heritability when used on current SNP data. The method is based on the outdated and false dichotomy of genes versus the environment. It also suffers from serious methodological weaknesses, such as susceptibility to population stratification.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: GCTA&quot;>GCTA</a> results indicates that SNPs can explain &amp;gt;33% of variance in current intelligence scores, and &amp;gt;44% with better-quality phenotype testing</li><li>this sets an upper bound on the effectiveness of selection: a gain of 9 IQ points when selecting the top embryo out of 10</li><li>the best 2016 polygenic score could achieve a gain of ~3 IQ points when selecting out of 10</li><li>the marginal cost of embryo selection (assuming IVF is already being done) is modest, at $1500 + $200 per embryo, with the sequencing cost projected to drop rapidly</li><li>a model of the IVF process, incorporating number of extracted eggs, losses to abnormalities &amp;amp; vitrification &amp;amp; failed implantation &amp;amp; miscarriages from 2 real IVF patient populations, estimates feasible gains of 0.39 &amp;amp; 0.68 IQ points</li><li>embryo selection is currently unprofitable (mean: -$358) in the USA under the lowest estimate of the value of an IQ point, but profitable under the highest (mean: $6230). The main constraints on selection profitability is the polygenic score; under the highest value, the NPV EVPI of a perfect SNP predictor is $24b and the EVSI per education/SNP sample is $71k</li><li>under the worst-case estimate, selection can be made profitable with a better polygenic score, which would require <em>n</em>&amp;gt;237,300 using education phenotype data (and much less using fluid intelligence measures)</li><li>selection can be made more effective by selecting on multiple phenotype traits: considering an example using 7 traits (IQ/height/BMI/diabetes/ADHD/bipolar/schizophrenia), there is a factor gain over IQ alone; the outperformance of multiple selection remains after adjusting for genetic correlations &amp;amp; polygenic scores and using a broader set of 16 traits. </li></ul>">Embryo selection for intelligence</a></li>
<li>Catnip:
<ul>
<li><a href="./Catnip" class="docMetadata" data-popup-title="Catnip immunity and alternatives" data-popup-author="Gwern Branwen" data-popup-date="7 Nov 2015" data-popup-abstract="<p>Not all cats respond to the catnip stimulant; the rate of responders is generally estimated at ~70% of cats. A meta-analysis of catnip response experiments since the 1940s indicates the true value is ~62%. The low quality of studies and the reporting of their data makes examination of possible moderators like age, sex, and country difficult. Catnip responses have been recorded for a number of species both inside and outside the <em>Felidae</em> family; of them, there is evidence for a catnip response in the Felidae, and, more uncertainly, the Paradoxurinae, and Herpestinae.</p>">Response rate meta-analysis</a></li>
<li><a href="./Catnip-survey" class="docMetadata" data-popup-title="World Catnip Surveys" data-popup-author="gwern" data-popup-date="15 Nov 2015" data-popup-abstract="<p>In compiling a meta-analysis of reports of <a href=&quot;./Catnip&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Catnip immunity and alternatives&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;7 Nov 2015&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;Not all cats respond to the catnip stimulant; the rate of responders is generally estimated at ~70% of cats. A meta-analysis of catnip response experiments since the 1940s indicates the true value is ~62%. The low quality of studies and the reporting of their data makes examination of possible moderators like age, sex, and country difficult. Catnip responses have been recorded for a number of species both inside and outside the &amp;lt;em&amp;gt;Felidae&amp;lt;/em&amp;gt; family; of them, there is evidence for a catnip response in the Felidae, and, more uncertainly, the Paradoxurinae, and Herpestinae.&amp;lt;/p&amp;gt;&quot;>catnip response rats in domestic cats</a>, yielding an meta-analytic average of ~<span class=&quot;mjpage&quot;><span class=&quot;mjx-chtml&quot;><span class=&quot;mjx-math&quot; aria-label=&quot;Equation&quot;><span class=&quot;mjx-mrow&quot; aria-hidden=&quot;true&quot;><span class=&quot;mjx-semantics&quot;><span class=&quot;mjx-mfrac&quot;><span class=&quot;mjx-box MJXc-stacked&quot; style=&quot;width: 0.495em; padding: 0px 0.12em;&quot;><span class=&quot;mjx-numerator&quot; style=&quot;font-size: 70.7%; width: 0.7em; top: -1.372em;&quot;><span class=&quot;mjx-mn&quot; style><span class=&quot;mjx-char MJXc-TeX-main-R&quot; style=&quot;padding-top: 0.372em; padding-bottom: 0.372em;&quot;>2</span></span></span><span class=&quot;mjx-denominator&quot; style=&quot;font-size: 70.7%; width: 0.7em; bottom: -0.686em;&quot;><span class=&quot;mjx-mn&quot; style><span class=&quot;mjx-char MJXc-TeX-main-R&quot; style=&quot;padding-top: 0.372em; padding-bottom: 0.372em;&quot;>3</span></span></span><span style=&quot;border-bottom: 1.3px solid; top: -0.296em; width: 0.495em;&quot; class=&quot;mjx-line&quot;></span></span><span style=&quot;height: 1.456em; vertical-align: -0.485em;&quot; class=&quot;mjx-vsize&quot;></span></span></span></span></span></span></span>, the available data suggests heterogeneity from cross-country differences in rates (possibly for genetic reasons) but is insufficient to definitively demonstrate the existence of or estimate those differences (particularly a possible extremely high catnip response rate in Japan). I use Google Surveys August-September 2017 to conduct a brief 1-question online survey of a proportional population sample of 9 countries about cat ownership &amp;amp; catnip use, specifically: Canada, the USA, UK, Japan, Germany, Brazil, Spain, Australia, &amp;amp; Mexico. in total, I surveyed <em>n</em>=31,471 people, of whom <em>n</em>=9,087 are cat owners, of whom <em>n</em>=4,402 report having used catnip on their cat, and of whom <em>n</em>=2996 report a catnip response.</p><p>The survey yields catnip response rates of Canada (82%), USA (79%), UK (74%), Japan (71%), Germany (57%), Brazil (56%), Spain (54%), Australia (53%), and Mexico (52%). The differences are substantial and of high posterior probability, supporting the existence of large cross-country differences. In additional analysis, the other conditional probabilities of cat ownership and trying catnip with a cat appear to correlate with catnip response rates; this intercorrelation suggests a âcat factorâ of some sort influencing responses, although what causal relationship there might be between proportion of cat owners and proportion of catnip-responder cats is unclear.</p><p>An additional survey of a convenience sample of primarily US Internet users about catnip is reported, although the improbable catnip response rates compared to the population survey suggest the respondents are either highly unrepresentative or the questions caused demand bias.</p>">Surveys</a></li>
</ul></li>
<li><a href="./Sunk-cost" class="docMetadata" data-popup-title="Are Sunk Costs Fallacies?" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2012" data-popup-abstract="Human and animal sunk costs often aren't, and sunk cost bias may be useful on an individual level to encourage learning. Convincing examples of sunk cost bias typically operate on organizational levels and are probably driven by non-psychological causes like competition.">Are Sunk Costs Fallacies?</a></li>
<li><a href="./Iodine" class="docMetadata" data-popup-title="Iodine and Adult IQ meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="29 Feb 2012" data-popup-abstract="<p>Iodization is one of the great success stories of public health intervention: iodizing salt costs pennies per ton, but as demonstrated in randomized &amp;amp; natural experiments, prevents goiters, cretinism, and can boost population IQs by a fraction of a standard deviation in the most iodine-deficient populations.</p><p>These experiments are typically done on pregnant women, and results suggest that the benefits of iodization diminish throughout the trimesters of a pregnancy. So does iodization benefit normal healthy <em>adults</em>, potentially even ones in relatively iodine-sufficient Western countries?</p><p>Compiling existing post-natal iodization studies which use cognitive tests, I find thatâoutliers asideâthe benefit appears to be nearly zero, and so likely it does not help normal healthy adults, particularly in Western adults.</p>">Iodine/IQ meta-analysis</a></li>
<li><a href="./SMPY" class="docMetadata" data-popup-title="SMPY Bibliography" data-popup-author="Gwern Branwen" data-popup-date="28 July 2018" data-popup-abstract="<p>SMPY (Study of Mathematically Precocious Youth) is a long-running longitudinal survey of extremely mathematically-talented or intelligent youth, which has been following high-IQ cohorts since the 1970s. It has provided the largest and most concrete findings about the correlates and predictive power of screening extremely intelligent children, and revolutionized gifted &amp; talented educational practices.</p><p>Because it has been running for over 40 years, SMPY-related publications are difficult to find; many early papers were published only in long-out-of-print books and are not available in any other way. Others are digitized and more accessible, but one must already know they exist. Between these barriers, SMPY information is less widely available &amp; used than it should be given its importance.</p><p>To fix this, I have been gradually going through all SMPY citations and making fulltext copies available online with occasional commentary.</p>"><span class="smallcaps-auto">SMPY</span> Bibliography</a></li>
<li><a href="./Conscientiousness-and-online-education" class="docMetadata" data-popup-title="Conscientiousness &amp; Online Education" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Technology-driven shift in demand for Conscientiousness, not intelligence">Conscientiousness &amp; Online Schools</a></li>
<li><a href="./Bakewell" class="docMetadata" data-popup-title="Origins of Innovation: Bakewell &amp; Breeding" data-popup-author="Gwern Branwen" data-popup-date="28 Oct 2018" data-popup-abstract="<p>Like anything else, the idea of âbreedingâ had to be <em>invented</em>. That traits are genetically-influenced broadly equally by both parents subject to considerable randomness and can be selected for over many generations to create large average population-wide increases had to be discovered the hard way, with many wildly wrong theories discarded along the way. Animal breeding is a case in point, as reviewed by an intellectual history of animal breeding, <em>Like Engendâring Like</em>, which covers mistaken theories of conception &amp;amp; inheritance from the ancient Greeks to perhaps the first truly successful modern animal breeder, <a href=&quot;https://en.wikipedia.org/wiki/Robert_Bakewell_%28agriculturalist%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Robert Bakewell (agriculturalist)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Robert Bakewell&amp;lt;/b&amp;gt; was a British agriculturalist, now recognized as one of the most important figures in the British Agricultural Revolution. In addition to work in agronomy, Bakewell is particularly notable as the first to implement systematic selective breeding of livestock. His advancements not only led to specific improvements in sheep, cattle and horses, but contributed to general knowledge of artificial selection.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Robert Bakewell (agriculturalist)&quot;>Robert Bakewell</a>.</p><p>Why did it take thousands of years to begin developing useful animal breeding techniques, a topic of interest to almost all farmers everywhere, a field which has no prerequisites such as advanced mathematics or special chemicals or mechanical tools, and seemingly requires only close observation and patience? This question can be asked of many innovations early in the Industrial Revolution, such as the flying shuttle.</p><p>Some veins in economics history and sociology suggest that at least one ingredient is an <em>improving attitude</em>: a detached outsiderâs attitude which asks whether there is any way to optimize something, in defiance of âthe wisdom of traditionâ, and looks for improvements. A relevant English example is the English Royal Society of Arts, founded not too distant in time from Bakewell, specifically to spur competition and imitation and new inventions. Psychological barriers may be as important as anything like per capita wealth or peace in innovation.</p>">Robert Bakewell &amp; Inventing Breeding</a></li>
<li><a href="./Clone" class="docMetadata" data-popup-title="Dog Cloning For Special Forces: Breed All You Can Breed" data-popup-author="Gwern Branwen" data-popup-date="18 Sep 2018" data-popup-abstract="<p>Cloning is widely used in animal &amp;amp; plant breeding despite steep costs due to its advantages; more unusual recent applications include creating entire polo horse teams and reported trials of cloning in elite police/Special Forces war dogs. Given the cost of dog cloning, however, can this ever make more sense than standard screening methods for selecting from working dog breeds, or would the increase in successful dog training be too low under all reasonable models to turn a profit?</p><p>I model the question as one of expected cost per dog with the trait of successfully passing training, success in training being a dichotomous liability threshold with a polygenic genetic architecture; given the extreme level of selection possible in selecting the best among already-elite Special Forces dogs and a range of heritabilities, this predicts clonesâ success probabilities. To approximate the relevant parameters, I look at some reported training costs and success rates for regular dog candidates, broad dog heritabilities, and the few current dog cloning case studies reported in the media.</p><p>Since none of the relevant parameters are known with confidence, I run the cost-benefit equation for many hypothetical scenarios, and find that in a large fraction of them covering most plausible values, dog cloning would improve training yields enough to be profitable (in addition to its other advantages).</p><p>As further illustration of the use-case of screening for an extreme outcome based on a partial predictor, I consider the question of whether height PGSes could be used to screen the US population for people of NBA height, which turns out to be <a href=&quot;#nba-screening-scenario&quot;>reasonably doable</a> with current &amp;amp; future PGSes.</p>">Dog Cloning For Special Forces</a></li>
<li><a href="./Questions" class="docMetadata" data-popup-title="Open Questions" data-popup-author="Gwern Branwen" data-popup-date="17 Oct 2018" data-popup-abstract="<p><span class=&quot;smallcaps&quot;>A list of some questions</span> which are not necessarily important, but do puzzle me or where I find existing âanswersâ to be unsatisfying, categorized by subject (along the lines of <a href=&quot;https://patrickcollison.com/questions&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;512&quot; data-popup-image-width=&quot;512&quot;>Patrick Collisonâs list</a> &amp;amp; <a href=&quot;https://guzey.com/personal/research-ideas/&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot;>Alex Guzey</a>; see also <a href=&quot;./Statistical-notes#someone-should-do-something-wishlist-of-miscellaneous-project-ideas&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Statistical Notes&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;17 July 2014&quot; data-popup-abstract=&quot;Miscellaneous statistical stuff&quot;>my list of project ideas</a>).</p>">Open Questions</a></li>
<li><a href="./Morning-writing" class="docMetadata" data-popup-title="What Is The Morning Writing Effect?" data-popup-author="Gwern Branwen" data-popup-date="11 May 2011" data-popup-abstract="<p>Ericsson 1993 notes that many major writers or researchers prioritized writing by making it the first activity of their day, often getting up early in the morning. This is based largely on writers anecdotally reporting they write best first thing early in the morning, apparently even if they are not morning people, although there is some additional survey/software-logging evidence of morning writing being effective. I compile all the anecdotes of writers discussing their writing times I have come across thus far. Do they, and why?</p>">What is the morning-writing effect?</a></li>
<li><a href="./Cat-Sense" class="docMetadata" data-popup-title="Cat Psychology &amp; Domestication: Are We Good Owners?" data-popup-author="Gwern Branwen" data-popup-date="3 Nov 2018" data-popup-abstract="<p>I review John Bradshawâs book on cat psychology, <em>Cat Sense</em>, after difficulties dealing with my own cat. Bradshaw reviews the history of domestic cats from their apparent Middle Eastern origins as a small solitary desert predator to their domestication in Ancient Egypt where breeding millions of cats for sacrifice may have played a critical role (as opposed to any unique role as a vermin exterminator) through to the modern day and psychological studies of the learning abilities and personalities of cats, with particular emphasis on cat social skills in <q>âcat coloniesâ</q> &amp;amp; plasticity in kittenhood. As Bradshaw diagnoses it, these are responsible for what ability they have to modern pet life, even though they are not bred for this like dogs; every tame cat still has the feral cat in them, and are in many ways unsuited for contemporary living, with disturbing hints that human lack of selective breeding plus recent large-scale spay/neuter population control efforts may be producing a subtle <em>dysgenic</em> effect on domestication, and this double neglect &amp;amp; backfire may be responsible for disturbingly high rates of cat maladaptation &amp;amp; chronic stress diseases.</p>">Cat Psychology &amp; Domestication: Are We Good Owners?</a></li>
<li><a href="./Replication" class="docMetadata" data-popup-title="The Replication Crisis: Flaws in Mainstream Science" data-popup-author="Gwern Branwen" data-popup-date="2010-10-27" data-popup-abstract="<p>Long-standing problems in standard scientific methodology have exploded as the <q>â<a href=&quot;https://en.wikipedia.org/wiki/Replication_Crisis&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Replication crisis&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;<p>The <b>replication crisis</b> is, as of 2019, an ongoing methodological crisis in which it has been found that many scientific studies are difficult or impossible to replicate or reproduce. The replication crisis affects the social sciences and medicine most severely. The crisis has long-standing roots; the phrase was coined in the early 2010s as part of a growing awareness of the problem. The replication crisis represents an important body of research in the field of metascience.</p>&quot; title=&quot;Wikipedia: Replication Crisis&quot;>Replication Crisis</a>â</q>: the discovery that many results in fields as diverse as psychology, economics, medicine, biology, and sociology are in fact false or quantitatively highly inaccurately measured. I cover here a handful of the issues and publications on this large, important, and rapidly developing topic up to about 2013, at which point the Replication Crisis became too large a topic to cover more than cursorily. (A <a href=&quot;./Replication#further-reading&quot;>compilation of some additional links</a> are provided for post-2013 developments.)</p> <p>The crisis is caused by methods &amp; publishing procedures which interpret random noise as important results, far too small datasets, selective analysis by an analyst trying to reach expected/desired results, publication bias, poor implementation of existing best-practices, nontrivial levels of research fraud, software errors, philosophical beliefs among researchers that false positives are acceptable, neglect of known confounding like genetics, and skewed incentives (financial &amp; professional) to publish âhotâ results.</p> <p>Thus, any individual piece of research typically establishes little. Scientific validation comes not from small <em>p</em>-values, but from discovering a regular feature of the world which disinterested third parties can discover with straightforward research done independently on new data with new proceduresâ<em>replication</em>.</p>">The Replication Crisis</a></li>
<li><a href="./Hydrocephalus" class="docMetadata" data-popup-title="Hydrocephalus and Intelligence: The Hollow Men" data-popup-author="Gwern Branwen" data-popup-date="2015-07-28" data-popup-abstract="<p>Hydrocephalus is a damaging brain disorder where fluids compress the brain, drastically decreasing its volume. While often extremely harmful or life-threatening when untreated, some people with the condition nevertheless are relatively normal, and in one case (Lorber) they have been claimed to have IQs as high as 126 with a brain volume 5% of normal brains. A few of these case studies have been used to argue the extraordinary claim that brain volume has little or nothing to do with intelligence; authors have argued that hydrocephalus suggests enormous untapped cognitive potential which are tapped into rarely for repairs and can boost intelligence on net, or that intelligence/consciousness are nonmaterial or tapping into ESP.</p> <p>I point out why this claim is almost uncertainly untrue because it predicts countless phenomena we never observe, and investigate the claimed examples in more detail: the cases turn out to be suspiciously unverifiable (Lorber), likely fraudulent (Oliveira), or actually low intelligence (Feuillet). It is unclear if high-functioning cases of hydrocephalus even have less brain mass, as opposed to lower proxy measures like brain volume.</p> <p>I then summarize anthropologist John Hawksâs criticisms of the original hydrocephalus author: his brain imaging data could not have been as precise as claimed, he studied a selective sample, the story of the legendary IQ 126 hydrocephalus patient raises questions as to how normal or intelligent he really was, and hydrocephalus in general appears to be no more anomalous or hard-to-explain than many other kinds of brain injuries, and in a comparison, hemispherectomies, removing or severing a hemisphere, has produced no anomalous reports of above-average intelligence (just deficits), though they ought to be just the same in terms of repairs or ESP.</p> <p>That hydrocephalus cases can reach roughly normal levels of functioning, various deficits aside, can be explained by brain size being one of several relevant variables, brain plasticity enabling cognitive flexibility &amp; recovery from gradually-developing conditions, and overparameterization giving robustness to damage and poor environments, and learning ability. The field of deep learning has observed similar phenomenon in training of artificial neural networks. This is consistent with Lorberâs original contention that the brain was more robust, and hydrocephalus was more treatable, than commonly accepted, but does not support any of the more exotic interpretations since put on his findings.</p> <p>In short, there is little anomalous to explain, and standard brain-centric accounts appear to account for existing verified observations without much problem or resort to extraordinary claims.</p>">Hydrocephalus &amp; IQ</a></li>
<li><a href="./Socks" class="docMetadata" data-popup-title="On Having Enough Socks" data-popup-author="Gwern Branwen" data-popup-date="22 Nov 2017" data-popup-abstract="<p>After running out of socks one day, I reflected on how ordinary tasks get neglected. Anecdotally and in 3 online surveys, people report often not having enough socks, a problem which correlates with rarity of sock purchases and demographic variables, consistent with a neglect/procrastination interpretation: because there is no specific time or triggering factor to replenish a shrinking sock stockpile, it is easy to run out.</p><p>This reminds me of akrasia on minor tasks, âyak shavingâ, and the nature of disaster in complex systems: lack of hard rules lets errors accumulate, without any âglobalâ understanding of the drift into disaster (or at least inefficiency). Humans on a smaller scale also âdriftâ when they engage in System I reactive thinking &amp;amp; action for too long, resulting in cognitive biases. An example of drift is the generalized human failure to explore/experiment adequately, resulting in overly greedy exploitative behavior of the current local optimum. Grocery shopping provides a case study: despite large gains, most people do not explore, perhaps because there is no established routine or practice involving experimentation. Fixes for these things can be seen as ensuring that System II deliberative cognition is periodically invoked to review things at a global level, such as developing a habit of maximum exploration at first purchase of a food product, or annually reviewing possessions to note problems like a lack of socks.</p><p>While socks may be small things, they may reflect big things.</p>">On Having Enough Socks</a></li>
</ul>
</section>
<section id="qs" class="level1">
<h1><a href="#qs" title="Link to section: 'QS'">QS</a></h1>
<ul>
<li><p>Sleep:</p>
<ul>
<li><a href="./Melatonin" class="docMetadata" data-popup-title="Melatonin" data-popup-author="Gwern Branwen" data-popup-date="19 Dec 2008" data-popup-abstract="<p>I discuss melatoninâs effects on sleep &amp;amp; its safety with research up to 2015; I segue into the general benefits of sleep and the severely disrupted sleep of the modern Western world, the cost of melatonin use and the benefit (eg. enforcing regular bedtimes), followed by a basic cost-benefit analysis of melatonin concluding that the net profit is large enough to be worth giving it a try barring unusual conditions or very pessimistic safety estimates.</p>">Melatonin</a></li>
<li><a href="./Modafinil" class="docMetadata" data-popup-title="Modafinil" data-popup-author="Gwern Branwen" data-popup-date="20 Feb 2009" data-popup-abstract="Modafinil is a prescription stimulant drug. I discuss informally, from a cost-benefit-informed perspective, the research up to 2015 on modafinil's cognitive effects, the risks of side-effects and addiction/tolerance and law enforcement, and give a table of current grey-market suppliers and discuss how to order from them.">Modafinil</a></li>
<li><a href="./Zeo" class="docMetadata" data-popup-title="Zeo sleep self-experiments" data-popup-author="Gwern Branwen" data-popup-date="28 Dec 2010" data-popup-abstract="<p>I discuss my beliefs about Quantified Self, and demonstrate with a series of <a href=&quot;https://en.wikipedia.org/wiki/single-subject_design&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Single-subject design&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In design of experiments, &amp;lt;b&amp;gt;single-subject design&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;single-case research design&amp;lt;/b&amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: single-subject design&quot;>single-subject design</a> self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:</p><ol type=&quot;1&quot;><li>the Zeo headband is wearable long-term</li><li><a href=&quot;#melatonin&quot;>melatonin</a> improves my sleep</li><li><a href=&quot;#one-legged-standing&quot;>one-legged standing</a> does little</li><li>Vitamin D <a href=&quot;#vitamin-d&quot;>at night</a> damages my sleep &amp;amp; Vitamin D in morning does not affect my sleep</li><li>potassium (<a href=&quot;#potassium-day-use&quot;>over the day</a> but not so much <a href=&quot;#potassium-morning-use&quot;>the morning</a>) damages my sleep and does not improve my mood/productivity</li><li>small quantities of <a href=&quot;#alcohol&quot;>alcohol</a> appear to make little difference to my sleep quality</li><li>I may be better off <a href=&quot;#timing&quot;>changing my sleep timing</a> by waking up somewhat earlier &amp;amp; going to bed somewhat earlier</li><li><a href=&quot;#lithium&quot;>lithium orotate</a> does not affect my sleep</li><li><a href=&quot;#redshiftf.lux&quot;>Redshift</a> causes me to go to bed earlier</li><li><a href=&quot;#zma&quot;>ZMA</a>: inconclusive results slightly suggestive of benefits</li></ol>">Sleep self-experiments</a>:
<ul>
<li><a href="./zeo/Caffeine" class="docMetadata" data-popup-title="Caffeine wakeup experiment" data-popup-author="Gwern Branwen" data-popup-date="7 April 2013" data-popup-abstract="<p>One trick to combat morning sluggishness is to get caffeine extra-early by using caffeine pills shortly before or upon trying to get up. From 2013-2014 I ran a blinded &amp;amp; placebo-controlled randomized experiment measuring the effect of caffeine pills in the morning upon awakening time and daily productivity. The estimated effect is small and the posterior probability relatively low, but a decision analysis suggests that since caffeine pills are so cheap, it would be worthwhile to conduct another experiment; however, increasing Zeo equipment problems have made me hold off additional experiments indefinitely.</p>">Caffeine</a></li>
<li><a href="./zeo/Potassium" class="docMetadata" data-popup-title="Potassium sleep experiments" data-popup-author="Gwern Branwen" data-popup-date="21 December 2012" data-popup-abstract="<p>Potassium and magnesium are minerals that many Americans are deficient in. I tried using potassium citrate and immediately noticed difficulty sleeping. A short randomized (but not blinded) self-experiment of ~4g potassium taken throughout the day confirmed large negative effects on my sleep. A longer followup randomized and blinded self-experiment used standardized doses taken once a day early in the morning, and also found some harm to sleep, and I discontinued potassium use entirely.</p>">Potassium</a></li>
<li><a href="./zeo/Redshift" class="docMetadata" data-popup-title="Redshift sleep experiment" data-popup-author="Gwern Branwen" data-popup-date="9 May 2012" data-popup-abstract="<p>I ran a randomized experiment with a free program (Redshift) which reddens screens at night to avoid tampering with melatonin secretion &amp;amp; the sleep from 2012-2013, measuring sleep changes with my Zeo. With 533 days of data, the main result is that Redshift causes me to go to sleep half an hour earlier but otherwise does not improve sleep quality.</p>">Redshift/f.lux</a></li>
<li><a href="./zeo/Vitamin-D" class="docMetadata" data-popup-title="Vitamin D sleep experiments" data-popup-author="Gwern Branwen" data-popup-date="1 January 2012" data-popup-abstract="<p>Vitamin D is a hormone endogenously created by exposure to sunlight; due to historically low outdoors activity levels, it has become a popular supplement and I use it. Some anecdotes suggest that vitamin D may have circadian and zeitgeber effects due to its origin, and is harmful to sleep when taken at night. I ran a blinded randomized self-experiment on taking vitamin D pills at bedtime. The vitamin D damaged my sleep and especially how rested I felt upon wakening, suggesting vitamin D did have a stimulating effect which obstructed sleep. I conducted a followup blinded randomized self-experiment on the logical next question: if vitamin D is a daytime cue, then would vitamin D taken in the morning show some beneficial effects? The results were inconclusive (but slightly in favor of benefits). Given the asymmetry, I suggest that vitamin D supplements should be taken only in the morning.</p>">Vitamin D</a></li>
<li><a href="./zeo/ZMA" class="docMetadata" data-popup-title="ZMA Sleep Experiment" data-popup-author="Gwern Branwen" data-popup-date="13 March 2017" data-popup-abstract="<p>I ran a blinded randomized self-experiment of 2.5g nightly ZMA powder effect on Zeo-recorded sleep data during March-October 2017 (<em>n</em>=127). The linear model and SEM model show no statistically-significant effects or high posterior probability of benefits, although all point-estimates were in the direction of benefits. Data quality issues reduced the available dataset, rendering the experiment particularly underpowered and the results more inconclusive. I decided to not continue use of ZMA after running out; ZMA may help my sleep but I need to improve data quality before attempting any further sleep self-experiments on it.</p>"><span class="smallcaps-auto">ZMA</span></a></li>
</ul></li>
<li><a href="./Wooden-pillows" class="docMetadata" data-popup-title="Wooden Pillow" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="China &amp; Egypt used wooden pillows; my recreations fail">Wooden pillows</a></li>
<li><a href="./Lunar-sleep" class="docMetadata" data-popup-title="Lunar circadian rhythms" data-popup-author="Gwern Branwen" data-popup-date="26 July 2013" data-popup-abstract="<p>I attempt to replicate, using public <a href=&quot;./Zeo&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Zeo sleep self-experiments&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;28 Dec 2010&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;I discuss my beliefs about Quantified Self, and demonstrate with a series of &amp;lt;a href=&amp;quot;https://en.wikipedia.org/wiki/single-subject_design&amp;quot; class=&amp;quot;docMetadata&amp;quot; data-popup-title=&amp;quot;Single-subject design&amp;quot; data-popup-author=&amp;quot;English Wikipedia&amp;quot; data-popup-abstract=&amp;quot;&amp;amp;lt;p&amp;amp;gt;In design of experiments, &amp;amp;lt;b&amp;amp;gt;single-subject design&amp;amp;lt;/b&amp;amp;gt; or &amp;amp;lt;b&amp;amp;gt;single-case research design&amp;amp;lt;/b&amp;amp;gt; is a research design most often used in applied fields of psychology, education, and human behavior in which the subject serves as his/her own control, rather than using another individual/group. Researchers use single-subject design because these designs are sensitive to individual organism differences vs group designs which are sensitive to averages of groups. Often there will be large numbers of subjects in a research study using single-subject design, howeverâbecause the subject serves as their own control, this is still a single-subject design. These designs are used primarily to evaluate the effect of a variety of interventions in applied research.&amp;amp;lt;/p&amp;amp;gt;&amp;quot; title=&amp;quot;Wikipedia: single-subject design&amp;quot;&amp;gt;single-subject design&amp;lt;/a&amp;gt; self-experiments using a Zeo. A Zeo records sleep via EEG; I have made many measurements and performed many experiments. This is what I have learned so far:&amp;lt;/p&amp;gt;&amp;lt;ol type=&amp;quot;1&amp;quot;&amp;gt;&amp;lt;li&amp;gt;the Zeo headband is wearable long-term&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#melatonin&amp;quot;&amp;gt;melatonin&amp;lt;/a&amp;gt; improves my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#one-legged-standing&amp;quot;&amp;gt;one-legged standing&amp;lt;/a&amp;gt; does little&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;Vitamin D &amp;lt;a href=&amp;quot;#vitamin-d&amp;quot;&amp;gt;at night&amp;lt;/a&amp;gt; damages my sleep &amp;amp;amp; Vitamin D in morning does not affect my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;potassium (&amp;lt;a href=&amp;quot;#potassium-day-use&amp;quot;&amp;gt;over the day&amp;lt;/a&amp;gt; but not so much &amp;lt;a href=&amp;quot;#potassium-morning-use&amp;quot;&amp;gt;the morning&amp;lt;/a&amp;gt;) damages my sleep and does not improve my mood/productivity&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;small quantities of &amp;lt;a href=&amp;quot;#alcohol&amp;quot;&amp;gt;alcohol&amp;lt;/a&amp;gt; appear to make little difference to my sleep quality&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;I may be better off &amp;lt;a href=&amp;quot;#timing&amp;quot;&amp;gt;changing my sleep timing&amp;lt;/a&amp;gt; by waking up somewhat earlier &amp;amp;amp; going to bed somewhat earlier&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#lithium&amp;quot;&amp;gt;lithium orotate&amp;lt;/a&amp;gt; does not affect my sleep&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#redshiftf.lux&amp;quot;&amp;gt;Redshift&amp;lt;/a&amp;gt; causes me to go to bed earlier&amp;lt;/li&amp;gt;&amp;lt;li&amp;gt;&amp;lt;a href=&amp;quot;#zma&amp;quot;&amp;gt;ZMA&amp;lt;/a&amp;gt;: inconclusive results slightly suggestive of benefits&amp;lt;/li&amp;gt;&amp;lt;/ol&amp;gt;&quot;>Zeo</a>-recorded sleep datasets, a finding of a monthly circadian rhythm affecting sleep in a small sleep lab. I find only small non-statistically-significant correlations, despite being well-powered.</p>">Lunar circadian rhythms</a></li>
</ul></li>
<li><p><a href="./Nootropics" class="docMetadata" data-popup-title="Nootropics" data-popup-author="Gwern Branwen" data-popup-date="02 Jan 2010" data-popup-abstract="Notes on nootropics I tried, and my experiments">Nootropics experiments</a></p></li>
<li><p><a href="./LSD-microdosing" class="docMetadata" data-popup-title="LSD microdosing RCT" data-popup-author="Gwern Branwen" data-popup-date="20 Aug 2012" data-popup-abstract="<p>Some early experimental studies with LSD suggested that doses of LSD too small to cause any noticeable effects may improve mood and creativity. Prompted by recent discussion of this claim and the purely anecdotal subsequent evidence for it, I decided to run a well-powered randomized blind trial of 3-day LSD microdoses from September 2012 to March 2013. No beneficial effects reached statistical-significance and there were worrisome negative trends. LSD microdosing did not help me.</p>"><span class="smallcaps-auto">LSD</span> microdosing</a></p></li>
<li><p><a href="./Creatine" class="docMetadata" data-popup-title="Creatine Cognition Meta-analysis" data-popup-author="Gwern Branwen" data-popup-date="6 Sep 2013" data-popup-abstract="<p>I attempt to meta-analyze conflicting studies about the cognitive benefits of creatine supplementation. The wide variety of psychological measures by uniformly small studies hampers any aggregation. 3 studies measured IQ and turn in a positive result, but suggestive of vegetarianism causing half the benefit. Discussions indicate that publication bias is at work. Given the variety of measures, small sample sizes, publication bias, possible moderators, and small-study biases, any future creatine studies should use the most standard measures of cognitive function like RAPM in a reasonably large pre-registered experiment.</p>">Creatine/cognition meta-analysis</a></p></li>
<li><p><a href="./Lithium" class="docMetadata" data-popup-title="Lithium in ground-water and well-being" data-popup-author="Gwern Branwen" data-popup-date="14 October 2010" data-popup-abstract="<p>The metal lithium is a well-known mood stabilizer &amp;amp; suicide preventive widely used in psychiatry. It is also a trace mineral present to various levels in all drinking water and much food. A long-running but obscure vein of research speculates on whether lithium is beneficial and a nutrient, specifically, cognitively-protective. Epidemiological research has correlated chronic lithium consumption through drinking water with a number of population-level variables like rates of mental illness, violence, &amp;amp; suicide. If causal, lithium should be regarded as a vital nutrient for mental health and added to drinking water to substantially improve population-wide outcomes.</p><p>However, the evidence is weak. Most of this research is cross-sectional, only some is longitudinal, none offers particularly strong causal evidence using natural experiments or other designs, there are questions about confounding with autocorrelated spatial properties such as altitude, and some of the best research, using Scandinavian population registries, offers more mixed evaluations of claimed correlates.</p><p>It is unlikely that further such correlational research will resolve the debate, despite the mounting opportunity cost. I suggest that formal experimentation is required, and concerns about harms from lithium supplementation making experiments âunethicalâ can be circumvented by instead <em>removing</em> lithium or looking for natural experiments with cause changes (such as changes or upgrades to water treatment plants or plumbing modify lithium concentration).</p>">Lithium in ground-water review</a></p></li>
<li><p><a href="./2014-spirulina" class="docMetadata" data-popup-title="2014 Spirulina randomized self-experiment" data-popup-author="Gwern Branwen" data-popup-date="5 Oct 2014" data-popup-abstract="<p>The supplement Spirulina has been suggested to help allergy symptoms. A randomized self-experiment is run by sceaduwe April - August 2014. Analysis suggests no effect of the spirulina.</p>">Spirulina allergy experiment</a></p></li>
<li><p><a href="./Melon" class="docMetadata" data-popup-title="Bitter Melon for blood glucose" data-popup-author="Gwern Branwen" data-popup-date="14 Sep 2015" data-popup-abstract="<p>I re-analyze a bitter-melon/blood-glucose self-experiment, finding a small effect of increasing blood glucose after correcting for temporal trends &amp;amp; daily variation, giving both frequentist &amp;amp; Bayesian analyses. I then analyze the self-experiment from a subjective Bayesian decision-theoretic perspective, cursorily estimating the costs of diabetes &amp;amp; benefits of intervention in order to estimate Value Of Information for the self-experiment and the benefit of further self-experimenting; I find that the expected value of more data (EVSI) is negative and further self-experimenting would not be optimal compared to trying out other anti-diabetes interventions.</p>">Bitter melon blood-glucose experiment</a></p></li>
<li><p><a href="./Lewis-meditation" class="docMetadata" data-popup-title="2013 Lewis meditation results" data-popup-author="Gwern Branwen" data-popup-date="12 July 2013" data-popup-abstract="<p>A small group of Quantified Selfers tested themselves daily on arithmetic and engaged in a month of meditation. I analyze their scores with a multilevel model with per-subject grouping, and find the expect result: a small decrease in arithmetic errors which is not statistically-significant, with practice &amp;amp; time-of-day effects (but not day-of-week or weekend effects). This suggests a longer experiment by twice as many experimenters in order to detect this effect.</p>">Meditation &amp; math errors</a></p></li>
<li><p><a href="./Weather" class="docMetadata" data-popup-title="Weather and My Productivity" data-popup-author="Gwern Branwen" data-popup-date="19 Mar 2013" data-popup-abstract="<p>Weather is often said to affect our mood, and that people in sunnier places are happier <em>because</em> of that. Curious about the possible effect (it could be worth controlling for in my future QS analyses or attempting to imitate benefits inside my house eg brighter lighting), I combine my long-term daily self-ratings with logs from the nearest major official weather stations, which offer detailed weather information about temperature, humidity, precipitation, cloud cover, wind speed, brightness etc, and try to correlate them.</p><p>In general, despite considerable data, there are essentially no bivariate correlations, nothing in several versions of a linear model, and nothing found by a random forest. It would appear that weather does not correlate with my self-ratings to any detectable degree, much less cause it.</p>">Weather/mood</a></p></li>
<li><p><a href="./Treadmill" class="docMetadata" data-popup-title="Treadmill desk observations" data-popup-author="Gwern Branwen" data-popup-date="19 June 2012" data-popup-abstract="Notes relating to my use of a treadmill desk and 2 self-experiments showing walking treadmill use interferes with typing and memory performance.">Treadmill memory/typing experiment</a></p></li>
<li><p><a href="./Bacopa" class="docMetadata" data-popup-title="Bacopa Quasi-Experiment" data-popup-author="Gwern Branwen" data-popup-date="6 May 2014" data-popup-abstract="<p>Bacopa is a supplement herb often used for memory or stress adaptation. Its chronic effects reportedly take many weeks to manifest, with no important acute effects. Out of curiosity, I bought 2 bottles of Bacognize Bacopa pills and ran a non-randomized non-blinded ABABA quasi-self-experiment from June 2014 to September 2015, measuring effects on my memory performance, sleep, and daily self-ratings of mood/productivity. For analysis, a multi-level Bayesian model on two memory performance variables was used to extract per-day performance, factor analysis was used to extract a sleep index from 9 Zeo sleep variables, and the 3 endpoints were modeled as a multivariate Bayesian time-series regression with splines. Because of the slow onset of chronic effects, small effective sample size, definite temporal trends probably unrelated to Bacopa, and noise in the variables, the results were as expected, ambiguous, and do not strongly support any correlation between Bacopa and memory/sleep/self-rating (+/-/- respectively).</p>">Bacopa quasi-experiment</a></p></li>
</ul>
</section>
<section id="practical" class="level1">
<h1><a href="#practical" title="Link to section: 'Practical'">Practical</a></h1>
<ul>
<li><a href="./Complement" class="docMetadata" data-popup-title="Laws of Tech: Commoditize Your Complement" data-popup-author="Gwern Branwen" data-popup-date="17 March 2018" data-popup-abstract="<p>Joel Spolsky in 2002 identified a major pattern in technology business &amp;amp; economics: the pattern of <q>âcommoditizing your complementâ</q>, an alternative to vertical integration, where companies seek to secure a chokepoint or quasi-monopoly in products composed of many necessary &amp;amp; sufficient layers by dominating one layer while fostering so much competition in another layer above or below its layer that no competing monopolist can emerge, prices are driven down to marginal costs elsewhere in the stack, total price drops &amp;amp; increases demand, and the majority of the consumer surplus of the final product can be diverted to the quasi-monopolist. A classic example is the commodification of PC hardware by the Microsoft OS monopoly, to the detriment of IBM &amp;amp; benefit of MS.</p><p>This pattern explains many otherwise odd or apparently self-sabotaging ventures by large tech companies into apparently irrelevant fields, such as the high rate of releasing open-source contributions by many Internet companies or the intrusion of advertising companies into smartphone manufacturing &amp;amp; web browser development &amp;amp; statistical software &amp;amp; fiber-optic networks &amp;amp; municipal WiFi &amp;amp; radio spectrum auctions &amp;amp; DNS (Google): they are pre-emptive attempts to commodify another company elsewhere in the stack, or defenses against it being done to them.</p>">Commoditize Your Complement</a></li>
<li><a href="./Drug-heuristics" class="docMetadata" data-popup-title="The Algernon Argument" data-popup-author="Gwern Branwen" data-popup-date="23 Mar 2010" data-popup-abstract="Why most supplements fail: IQ improvement skepticism, Yudkowsky &amp; Bostrom's heuristics, nootropics">Evolutionary Heuristics for Drugs</a></li>
<li><a href="./Search" class="docMetadata" data-popup-title="Internet Search Tips" data-popup-author="Gwern Branwen" data-popup-date="11 Dec 2018" data-popup-abstract="<p>Over time, I developed a certain google-fu and expertise in finding references, papers, and books online. Some of these tricks are not well-known, like checking the Internet Archive (IA) for books. I try to write down my search workflow, and give general advice about finding and hosting documents.</p>">Internet Search Tips</a></li>
<li><a href="./Timing" class="docMetadata" data-popup-title="Timing Technology: Lessons From The Media Lab" data-popup-author="Gwern Branwen" data-popup-date="12 July 2012" data-popup-abstract="<p>Technological forecasts are often surprisingly prescient in terms of predicting that something was possible &amp;amp; desirable and what they predict eventually happens; but they are far less successful at predicting the timing, and almost always fail, with the success (and riches) going to another.</p><p>Why is their knowledge so useless? The right moment cannot be known exactly in advance, so attempts to forecast will typically be off by years or worse. For many claims, there is no way to invest in an idea except by going all in and launching a company, resulting in extreme variance in outcomes, even when the idea is good and the forecasts correct about the (eventual) outcome.</p><p>Progress can happen and can be foreseen long before, but the details and exact timing due to bottlenecks are too difficult to get right. Launching too early means failure, but being conservative &amp;amp; launching later is just as bad because regardless of forecasting, a good idea will draw overly-optimistic researchers or entrepreneurs to it like <a href=&quot;https://en.wikipedia.org/wiki/Winner%27s_curse&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Winner&amp;#39;s curse&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The &amp;lt;b&amp;gt;winner&amp;#39;s curse&amp;lt;/b&amp;gt; is a phenomenon that may occur in common value auctions, where all bidders have the same value for an item but receive different private signals about this value and wherein the winner is the bidder with the most optimistic evaluation of the asset and therefore will tend to overestimate and overpay. Accordingly, the winner will be &amp;quot;cursed&amp;quot; in one of two ways: either the winning bid will exceed the value of the auctioned asset making the winner worse off in absolute terms, or the value of the asset will be less than the bidder anticipated, so the bidder may garner a net gain but will be worse off than anticipated. However, an actual overpayment will generally occur only if the winner fails to account for the winner&amp;#39;s curse when bidding.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Winner&amp;#39;s curse&quot;>moths to a flame</a>: all get immolated but the one with the dumb luck to kiss the flame at the perfect instant, who then wins everything, at which point everyone can see that the optimal time is past. All major success stories overshadow their long list of predecessors who did the same thing, but got unlucky. So, ideas can be divided into the overly-optimistic &amp;amp; likely doomed, or the <em>fait accompli</em>. On an individual level, ideas are worthless because so many others have them tooââmultiple inventionâ is the rule, and not the exception.</p><p>This overall problem falls under the reinforcement learning paradigm, and successful approaches are analogous to Thompson sampling/posterior sampling: even an informed strategy canât reliably beat random exploration which gradually shifts towards successful areas while continuing to take occasional long shots. Since people tend to systematically over-exploit, how is this implemented? Apparently by individuals acting suboptimally on the personal level, but optimally on societal level by serving as random exploration.</p><p>A major benefit of R&amp;amp;D, then, is in laying fallow until the âripe timeâ when they can be immediately exploited in previously-unpredictable ways; applied R&amp;amp;D or VC strategies should focus on maintaining diversity of investments, while continuing to flexibly revisit previous failures which forecasts indicate may have reached âripe timeâ. This balances overall exploitation &amp;amp; exploration to progress as fast as possible, showing the usefulness of technological forecasting on a global level despite its uselessness to individuals.</p>">Timing Technology: Media Lab Lessons</a></li>
<li><a href="./Girl-Scouts-and-good-governance" class="docMetadata" data-popup-title="Girl Scouts &amp; Good Corporate Governance" data-popup-author="Gwern Branwen" data-popup-date="21 Apr 2011" data-popup-abstract="Cookie prices &amp; tax filings as evidence of corporate inefficiency">Girl Scouts &amp; Good Governance</a></li>
<li><a href="./plastination" class="docMetadata" data-popup-title="Plastination versus Cryonics" data-popup-author="Gwern Branwen" data-popup-date="24 Jul 2011" data-popup-abstract="Break down survival as Drake equation, see how plastination differs from cryonics, try to calculate advantage">Plastination vs Cryonics</a></li>
<li><a href="./Charity-is-not-about-helping" class="docMetadata" data-popup-title="Charity is not about Helping" data-popup-author="Gwern Branwen" data-popup-date="15 Sep 2011" data-popup-abstract="Simple cost-benefit: distributed computing considered harmful as scientific lemon projects run at high resource cost.">Charity is not about helping</a></li>
<li><a href="./Console-Insurance" class="docMetadata" data-popup-title="Console Insurance Is A Ripoff" data-popup-author="Gwern Branwen" data-popup-date="10 Mar 2009" data-popup-abstract="Back of envelope financial calculations: Warranties, fine; insurance, no!">Console Insurance</a></li>
<li><a href="./Life-contract" class="docMetadata" data-popup-title="Life contracts" data-popup-author="Gwern Branwen" data-popup-date="02 Nov 2009" data-popup-abstract="How I reinvented longevity insurance, which provides payouts if one lives to a certain point and thus might run out of savings.">Life contracts</a></li>
<li><a href="./Red" class="docMetadata" data-popup-title="Rubrication Design Examples" data-popup-author="Gwern Branwen" data-popup-date="30 May 2019" data-popup-abstract="<p>Dating back to medieval manuscripts, text has often been highlighted using a particular distinct bright red. The contrast of black and red on a white background is highly visible and striking, and this has been reused many times, in a way which I have not noticed for other colors. I call these uses <em>rubrication</em> and collate examples I have noticed from many time periods.</p><p>Why this design pattern? Why red, specifically, and not, say, orange or purple? Is it just a historical accident? Cross-cultural research suggests that for humans, red may be intrinsically more noticeable &amp; has a higher contrast with black, explaining its perennial appeal as a design pattern.</p><p>Regardless, it is a beautiful design pattern which has been used in many interesting ways over the millennia, and perhaps may inspire the reader.</p>">Rubrication Design Examples</a></li>
<li><a href="./Improvements" class="docMetadata" data-popup-title="My Ordinary Life: Improvements Since the 1990s" data-popup-author="Gwern Branwen" data-popup-date="2019-05-30" data-popup-abstract="It can be hard to see the gradual improvement of most goods over time, but I think one way to get a handle on them is to look at their <em>downstream</em> effects: all the small ordinary everyday things which nevertheless depend on obscure innovations and improving cost-performance ratios and gradually dropping costs and new material and... etc. All of these gradually drop the cost, drop the price, improve the quality at the same price, remove irritations or limits not explicitly noticed, or so on. It all adds up. So here is a personal list of some of the small ways in which my ordinary everyday daily life has been getting better since the late '80s/early '90s (as far back as I can clearly remember these things---I am sure the list of someone growing up in the 1940s would include many hassles I've never known at all).">Ordinary Life Improvements</a></li>
</ul>
</section>
<section id="politics" class="level1">
<h1><a href="#politics" title="Link to section: 'Politics'">Politics</a></h1>
<ul>
<li><a href="./The-Melancholy-of-Subculture-Society" class="docMetadata" data-popup-title="The Melancholy of Subculture Society" data-popup-author="Gwern Branwen" data-popup-date="12 Jan 2009" data-popup-abstract="Internet links small groups, helping dissolve big groups; good, bad? But a bit sad.">The Melancholy of Subculture Society</a></li>
<li><a href="./Terrorism-is-not-about-Terror" class="docMetadata" data-popup-title="Terrorism Is Not About Terror" data-popup-author="Gwern Branwen" data-popup-date="09 Apr 2009" data-popup-abstract="<p>Statistical analysis of terrorist groupsâ longevity, aims, methods and successes reveal that groups are self-contradictory and self-sabotaging, generally ineffective; common stereotypes like terrorists being poor or ultra-skilled are false. Superficially appealing counter-examples are discussed and rejected. Data on motivations and the dissolution of terrorist groups are brought into play and the surprising conclusion reached: terrorism is a form of socialization or status-seeking.</p>">Terrorism is not about Terror</a></li>
<li><a href="./Terrorism-is-not-Effective" class="docMetadata" data-popup-title="Terrorism Is Not Effective" data-popup-author="Gwern Branwen" data-popup-date="14 Apr 2009" data-popup-abstract="<p>Terrorism is not about causing terror or casualties, but about other things. Evidence of this is the fact that, despite often considerable resources spent, most terrorists are incompetent, impulsive, prepare poorly for attacks, are inconsistent in planning, tend towards exotic &amp;amp; difficult forms of attack such as bombings, and in practice ineffective: the modal number of casualties per terrorist attack is near-zero, and global terrorist annual casualty have been a rounding error for decades. This is despite the fact that there are many examples of extremely destructive easily-performed potential acts of terrorism, such as poisoning food supplies or renting large trucks &amp;amp; running crowds over or engaging in sporadic sniper attacks.</p>">Terrorism is not Effective</a></li>
<li><a href="./Slowing-Moores-Law" class="docMetadata" data-popup-title="Slowing Moore's Law: How It Could Happen" data-popup-author="Gwern Branwen" data-popup-date="16 Mar 2012" data-popup-abstract="<p><a href=&quot;https://en.wikipedia.org/wiki/Brain_emulation&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Mind uploading&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Whole brain emulation&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;WBE&amp;lt;/b&amp;gt;), &amp;lt;b&amp;gt;mind upload&amp;lt;/b&amp;gt; or &amp;lt;b&amp;gt;brain upload&amp;lt;/b&amp;gt; is the hypothetical futuristic process of scanning the mental state of a particular brain substrate and copying it to a computer. The computer could then run a simulation model of the brain&amp;#39;s information processing, such that it would respond in essentially the same way as the original brain and experience having a conscious mind.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Brain emulation&quot;>Brain emulation</a> requires enormous computing power; enormous computing power requires further progression of <a href=&quot;https://en.wikipedia.org/wiki/Moore%27s_law&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Moore&amp;#39;s law&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Moore&amp;#39;s law&amp;lt;/b&amp;gt; is the observation that the number of transistors in a dense integrated circuit doubles about every two years. The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and CEO of Intel, whose 1965 paper described a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41.4%.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Mooreâs law&quot;>Mooreâs law</a>; further Mooreâs law relies on large-scale production of cheap processors in ever more-advanced <a href=&quot;https://en.wikipedia.org/wiki/chip_fabs&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Semiconductor fabrication plant&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In the microelectronics industry, a &amp;lt;b&amp;gt;semiconductor fabrication plant&amp;lt;/b&amp;gt; is a factory where devices such as integrated circuits are manufactured.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: chip fabs&quot;>chip fabs</a>; cutting-edge chip fabs are both expensive and vulnerable to state actors (but <em>not</em> non-state actors <a href=&quot;#state-actors-why-not-terrorism&quot;>such as terrorists</a>). Therefore: the advent of brain emulation can be delayed by global regulation of chip fabs.</p>">Mooreâs law &amp; chip fabs</a></li>
<li><a href="./Littlewood" class="docMetadata" data-popup-title="Littlewood's Law and the Global Media" data-popup-author="Gwern Branwen" data-popup-date="15 Dec 2018" data-popup-abstract="<p>Online &amp;amp; mainstream media and social networking have become increasingly misleading as to the state of the world by focusing on âstoresâ and âeventsâ rather than trends and averages. This is because as the global population increases and the scope of media increases, mediaâs urge for narrative focuses on the most extreme outlier datapointsâbut such datapoints are, at a global scale, deeply misleading as they are driven by unusual processes such as the mentally ill or hoaxers.</p><p>At a global scale, anything that can happen will happen a small but nonzero times: this has been epitomized as <q>âLittlewoodâs Law: in the course of any normal personâs life, miracles happen at a rate of roughly one per month.â</q> Hence, there will be enough âmiraclesâ that all media coverage of events can potentially be composed of nothing but extreme chance chances, even though it would seem like an âextraordinaryâ claim to say that all media-reported events may be flukes.</p><p>Given this, it is important to maintain extreme skepticism of any individual anecdotes or stories which are selectively reported but still claimed (often implicitly) to be representative of a general trend or fact about the world. Standard techniques like critical thinking, emphasizing trends &amp;amp; averages, and demanding original sources can help fight the biasing effect of news.</p>">Littlewood's Law &amp; the Global Media</a></li>
<li><a href="./Colder-Wars" class="docMetadata" data-popup-title="Colder Wars" data-popup-author="Gwern Branwen" data-popup-date="07 Jun 2009" data-popup-abstract="MAD will not work in outer space; pre-emptive strikes are nigh-guaranteed.">Colder Wars</a></li>
</ul>
</section>
<section id="philosophy" class="level1">
<h1><a href="#philosophy" title="Link to section: 'Philosophy'">Philosophy</a></h1>
<ul>
<li><a href="./The-Existential-Risk-of-Mathematical-Error" class="docMetadata" data-popup-title="The Existential Risk of Math Errors" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Mathematical mistake/error-rates limit our understanding of rare risks and ability to defend against them">Existential Risks &amp; Math Errors</a></li>
<li><a href="./Language" class="docMetadata" data-popup-title="On the Existence of Powerful Natural Languages" data-popup-author="Gwern Branwen" data-popup-date="18 Dec 2016" data-popup-abstract="<p>Designed formal notations &amp;amp; distinct vocabularies are often employed in STEM fields, and these specialized languages are credited with greatly enhancing research &amp;amp; communication. Many philosophers and other thinkers have attempted to create more generally-applicable designed languages for use outside of specific technical fields to enhance human thinking, but the empirical track record is poor and no such designed language has demonstrated substantial improvements to human cognition such as resisting cognitive biases or logical fallacies. I suggest that the success of specialized languages in fields is inherently due to encoding large amounts of previously-discovered information specific to those fields, and this explains their inability to boost human cognition across a wide variety of domains.</p>">On Powerful Natural Languages</a></li>
<li><a href="./Culture-is-not-about-Esthetics" class="docMetadata" data-popup-title="Culture Is Not About Esthetics" data-popup-author="Gwern Branwen" data-popup-date="16 Jul 2009" data-popup-abstract="Aesthetically &amp; economically, maybe there is too much new art. Don't take this too seriously.">Culture is not about Esthetics</a></li>
<li><a href="./The-Narrowing-Circle" class="docMetadata" data-popup-title="The Narrowing Circle" data-popup-author="Gwern Branwen" data-popup-date="24 Apr 2012" data-popup-abstract="<p>The <q>âexpanding circleâ</q> historical thesis ignores all instances in which modern ethics <em>narrowed</em> the set of beings to be morally regarded, often backing its exclusion by asserting their non-existence, and thus assumes its conclusion: where the circle is expanded, itâs highlighted as moral âprogressâ, and where it is narrowed, what is outside is simply defined away. When one compares modern with ancient society, the religious differences are striking: almost every single supernatural entity (place, personage, or force) has been excluded from the circle of moral concern, where they used to be huge parts of the circle and one could almost say the entire circle. Further examples include estates, houses, fetuses, prisoners, and graves.</p>">The Narrowing Circle</a></li>
<li><a href="./Modus" class="docMetadata" data-popup-title="One Man's Modus Ponens" data-popup-author="Gwern Branwen" data-popup-date="1 May 2012" data-popup-abstract="<p>A logically-valid argument which takes the form of a <a href=&quot;https://en.wikipedia.org/wiki/modus_ponens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus ponens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus ponens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a rule of inference. It can be summarized as &amp;quot;&amp;lt;i&amp;gt;P implies Q&amp;lt;/i&amp;gt; and &amp;lt;i&amp;gt;P&amp;lt;/i&amp;gt; is asserted to be true, therefore &amp;lt;i&amp;gt;Q&amp;lt;/i&amp;gt; must be true.&amp;quot;&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus ponens&quot;>modus ponens</a> may be interpreted in several ways; a major one is to interpret it as a kind of <em><a href=&quot;https://en.wikipedia.org/wiki/reductio_ad_absurdum&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Reductio ad absurdum&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In logic, &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;reductio ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, also known as &amp;lt;b&amp;gt;&amp;lt;i lang=&amp;quot;la&amp;quot; title=&amp;quot;Latin language text&amp;quot;&amp;gt;argumentum ad absurdum&amp;lt;/i&amp;gt;&amp;lt;/b&amp;gt;, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;apagogical arguments&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; or the &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;appeal to extremes&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;, is a form of argument that attempts either to disprove a statement by showing it inevitably leads to a ridiculous, absurd, or impractical conclusion, or to prove one by showing that if it were not true, the result would be absurd or impossible. Traced back to classical Greek philosophy in Aristotle&amp;#39;s &amp;lt;i&amp;gt;Prior Analytics&amp;lt;/i&amp;gt;, this technique has been used throughout history in both formal mathematical and philosophical reasoning, as well as in debate.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: reductio ad absurdum&quot;>reductio ad absurdum</a></em>, where by âprovingâ a conclusion believed to be false, one might instead take it as a <a href=&quot;https://en.wikipedia.org/wiki/modus_tollens&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Modus tollens&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;In propositional logic, &amp;lt;i&amp;gt;&amp;lt;b&amp;gt;modus tollens&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt; is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contrapositive.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: modus tollens&quot;>modus tollens</a> which proves that one of the <em>premises</em> is false. This <q>âMoorean shiftâ</q> is aphorized as the snowclone, <q>âOne manâs modus ponens is another manâs modus tollensâ</q>. The Moorean shift is a powerful counter-argument which has been deployed against many skeptical &amp;amp; metaphysical claims in philosophy, where often the conclusion is extremely unlikely and little evidence can be provided for the premises used in the proofs; and it is relevant to many other debates, particularly methodological ones.</p>">One Man's Modus Ponens</a></li>
<li><a href="./Newton" class="docMetadata" data-popup-title="Newton's System of the World and Comets" data-popup-author="Gwern Branwen" data-popup-date="13 June 2016" data-popup-abstract="<p>Isaac Newton published few of his works, and only those he considered perfect after long delays. This leaves his system the world, as described in the <em>Principia</em> and elsewhere, incomplete, and many questions simply unaddressed, like the fate of the Sun or role of comets. But in 2 conversations with an admirer and his nephew, the elderly Newton sketched out the rest of his cosmogony.</p><p>According to Newton, the solar system is <em>not</em> stable and must be adjusted by angels; the Sun does not burn perpetually, but comets regularly fuel the Sun; and the final result is that humanity will be extinguished by a particularly large comet causing the sun to flare up, and requiring intelligent alien beings to arise on other planets or their moons. He further gives an anthropic argument: one reason we know that intelligent races regularly go extinct is that humanity itself arose only recently, as demonstrated by the recent innovations in every field, inconsistent with any belief that human beings have existed for hundreds of thousands or millions of years.</p><p>This is all interestingly wrong, particularly the anthropic argument. That Newton found it so absurd to imagine humanity existing for millions of years but only recently undergoing exponential improvements in technology demonstrates how counterintuitive and extraordinary the Industrial &amp;amp; Scientific Revolutions were.</p>">Newton's System of the World &amp; Comets</a></li>
<li><a href="./Subscripts" class="docMetadata" data-popup-title="Subscripts For Citations" data-popup-author="Gwern Branwen" data-popup-date="8 Jan 2020" data-popup-abstract="<p>I propose reviving an old General Semantics notation: borrow from scientific notation and use subscripts like âGwern<sub>2020</sub>â for denoting sources (like citation, timing, or medium). Using subscript indices is flexible, compact, universally technically supported, and intuitive. While (currently) unusual, subscripting might be a useful trick for clearer writing, compared to omitting such information or using standard cumbersome circumlocutions.</p>">On Subscripting Citations</a></li>
<li><a href="./Ontological-pantheism" class="docMetadata" data-popup-title="Ontological Pantheism" data-popup-author="Gwern Branwen" data-popup-date="09 Nov 2009" data-popup-abstract="Descartes's God is pantheism; a reductio ad absurdum of his ontology">Ontological pantheism</a></li>
<li><a href="./An-Abortion-Dialogue" class="docMetadata" data-popup-title="An Abortion Dialogue" data-popup-author="Gwern Branwen" data-popup-date="10 Nov 2008" data-popup-abstract="Dialogue pointing out some difficulties of materialist objections to abortion.">An Abortion Dialogue</a></li>
<li><a href="./On-Disrespect" class="docMetadata" data-popup-title="On Disrespect" data-popup-author="Gwern Branwen" data-popup-date="09 Feb 2009" data-popup-abstract="An attempt to reinvent classic theories of social interaction as expressions of power">On Disrespect</a></li>
<li><a href="./Justifications" class="docMetadata" data-popup-title="On Justifications" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Philosophical fiction or a prose poem about suffering innocents and the theodicy">Justifications</a></li>
<li><a href="./Against-The-Miletians" class="docMetadata" data-popup-title="Against the Miletians and the One True Element" data-popup-author="Gwern Branwen" data-popup-date="17 Oct 2008" data-popup-abstract="Exploring consequences of material monism and conflict with observations">Against The Miletians</a></li>
<li><a href="./Copyright" class="docMetadata" data-popup-title="Against Copyright" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Copyright considered paradoxical, incoherent, and harmful from an information theory and compression perspective as there is no natural kind corresponding to ">Copyright</a></li>
<li><a href="./Immoral-Books" class="docMetadata" data-popup-title="Immoral Books" data-popup-author="Gwern Branwen" data-popup-date="24 Jan 2010" data-popup-abstract="Argument that texts are neither moral nor immoral as they require active interpretation.">Immoral Books</a></li>
<li><a href="./Isomorphisms" class="docMetadata" data-popup-title="Isomorphisms" data-popup-author="Gwern Branwen" data-popup-date="25 Apr 2009" data-popup-abstract="Investigating when two theories or representations *mean* the same things.">Isomorphisms</a></li>
<li><a href="./Ethical-sperm-donation" class="docMetadata" data-popup-title="The Morality of Sperm Donation" data-popup-author="Gwern Branwen" data-popup-date="20 Jul 2012" data-popup-abstract="Is sperm donating a worthwhile form of positive eugenics?">Moral Sperm Donation</a></li>
<li><a href="./Zen-and-the-Art-of-Bicycle-Maintenance" class="docMetadata" data-popup-title="Zen and the Art of Bicycle Maintenance" data-popup-author="Gwern Branwen" data-popup-date="02 Feb 2009" data-popup-abstract="Mechanical parts teach you that small things matter.">Zen &amp; the Art of Bicycle Maintenance</a></li>
</ul>
</section>
<section id="fiction" class="level1">
<h1><a href="#fiction" title="Link to section: 'Fiction'">Fiction</a></h1>
<ul>
<li><p>Prose:</p>
<ul>
<li><a href="./fiction/The-Erl-King" class="docMetadata" data-popup-title="The Erl King" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Fairy tale tragedy, or, a lesson in courtesy and logic">âThe Erl Kingâ</a></li>
<li><a href="./fiction/The-Ones-Who-Walk-Towards-Acre" class="docMetadata" data-popup-title="The Ones Who Walk Towards Acre" data-popup-author="Gwern Branwen" data-popup-date="21 Dec 2010" data-popup-abstract="Short story on assassination markets.">âThe Ones Who Walk Towards Acreâ</a></li>
<li><a href="./fiction/Missing-Cities" class="docMetadata" data-popup-title="Missing Cities" data-popup-author="Gwern Branwen" data-popup-date="01 Feb 2009" data-popup-abstract="3 short stories in the style of Italo Calvino's 'Missing Cities'">âMissing Citiesâ</a></li>
<li><a href="./fiction/Men-of-Iron" class="docMetadata" data-popup-title="Men of Iron" data-popup-author="Gwern Branwen" data-popup-date="24 Dec 2012" data-popup-abstract="What-if Chiang-style SF story on iron vanishing and the Great Silence">âMen of Ironâ</a></li>
<li><a href="./fiction/Menard" class="docMetadata" data-popup-title="Gilles Goullet, Author of the Blindsight" data-popup-author="Gwern Branwen" data-popup-date="01 Sep 2010" data-popup-abstract="A parody of SF, the Internet, and Borges.">âGilles Goullet, Author of <em>Blindsight</em>â</a></li>
<li><a href="./fiction/How-the-Panther-got-Black" class="docMetadata" data-popup-title="How the Panther Got Black" data-popup-author="Gwern Branwen" data-popup-date="02 Feb 2009" data-popup-abstract="Mythic short story in the vein of Kipling's Just-so Stories">âHow the Panther got Blackâ</a></li>
<li><a href="./fiction/The-Palace-of-Wonders" class="docMetadata" data-popup-title="The Palace of Wonders" data-popup-author="Gwern Branwen" data-popup-date="23 May 2011" data-popup-abstract="A Borgesian fable about the Caliph and the Koran.">âThe Palace of Wondersâ</a></li>
<li><a href="./fiction/Batman" class="docMetadata" data-popup-title="The Gift of the Amygdali" data-popup-author="Gwern Branwen" data-popup-date="29 Oct 2017" data-popup-abstract="A high-concept 'Batman' short story in the style of a 1980s comic book script about the Scarecrow and the gifts no one appreciates: pain/guilt/fear/anxiety.">âThe Gift of the Amygdaliâ</a></li>
<li><a href="./fiction/Cloud-Nine" class="docMetadata" data-popup-title="Cloud Nine" data-popup-author="Gwern Branwen" data-popup-date="26 Sep 2008" data-popup-abstract="Unfinished fantasy/SF novel"><em>Cloud Nine</em></a></li>
<li><a href="./fiction/The-Last-Muezzin" class="docMetadata" data-popup-title="The Last Muezzin" data-popup-author="Gwern Branwen" data-popup-date="15 Jun 2011" data-popup-abstract="Another tribute to Borges; you can be me when I'm gone.">âThe Last Muezzinâ</a></li>
<li><a href="./fiction/Dinosaur-Comics" class="docMetadata" data-popup-title="_Dinosaur Comics_ comics" data-popup-author="Gwern Branwen" data-popup-date="10 Jun 2011" data-popup-abstract="Comics using the format of Ryan North's _Dinosaur Comics_"><em>Dinosaur Comics</em> comics</a></li>
<li><a href="./fiction/The-Buddhas-Wheel" class="docMetadata" data-popup-title="The Buddha's Wheel" data-popup-author="Gwern Branwen" data-popup-date="30 Nov 2008" data-popup-abstract="The enlightened is as one with cause and effect.">âThe Buddhaâs Wheelâ</a></li>
</ul></li>
<li><p>Verse:</p>
<ul>
<li><a href="./fiction/Poems" class="docMetadata" data-popup-title="Poems" data-popup-author="Gwern Branwen" data-popup-date="28 July 2011" data-popup-abstract="Miscellaneous waka/haiku, by season">Poems</a></li>
<li><a href="./fiction/Brave-poem" class="docMetadata" data-popup-title="Brave Poem" data-popup-author="Gwern Branwen" data-popup-date="13 Nov 2010" data-popup-abstract="Poem on memory inspired by the anime 'Angel Beats!'; do you remember love?">âBrave poemâ</a></li>
<li><a href="./fiction/Dying-Outside" class="docMetadata" data-popup-title="Dying Outside" data-popup-author="Gwern Branwen" data-popup-date="12 Dec 2009" data-popup-abstract="Poem about ALS">âDying Outsideâ</a></li>
<li><a href="./fiction/Genshiken" class="docMetadata" data-popup-title="Poems on the theme of 'Genshiken'" data-popup-author="Gwern Branwen" data-popup-date="24 Jul 2011" data-popup-abstract="Waka/haiku on Genshiken, clubs, and Comiket; allusions to Fujiwara no Teika and ShÅtetsu">Poems on the theme of <em>Genshiken</em></a></li>
<li><a href="./fiction/Safecracker" class="docMetadata" data-popup-title="Safecracker" data-popup-author="Gwern Branwen" data-popup-date="15 Aug 2009" data-popup-abstract="The thief of time gives us memories, the safecracker of hearts restores them">âSafecrackerâ</a></li>
<li><a href="./fiction/Hybrid-Rainbow" class="docMetadata" data-popup-title="Hybrid Rainbow" data-popup-author="Gwern Branwen" data-popup-date="01 Feb 2009" data-popup-abstract="Poem about man &amp; his machines.">âHybrid Rainbowâ</a></li>
</ul></li>
<li><p>Criticism:</p>
<ul>
<li><a href="./Story-Of-Your-Life" class="docMetadata" data-popup-title="'Story Of Your Life' Is Not A Time-Travel Story" data-popup-author="Gwern Branwen" data-popup-date="12 Dec 2012" data-popup-abstract="<p>One of Ted Chiangâs most noted philosophical SF short stories, âStory of Your Lifeâ, was made into a successful time-travel movie, <em>Arrival</em>, sparking interest in the original. However, movie viewers often misread the short story: âStoryâ is <em>not</em> a time-travel movie. At no point does the protagonist travel in time or enjoy precognitive powers, and interpreting the story this way leads to many serious plot holes and renders most of the exposition-heavy dialogue irrelevant.</p><p>Instead, Chiangâs story is about <em>psychology and physics</em>: classical physics allows usefully interpreting the laws of physics in both a âforwardâ way in which events happen step by step, but also a teleological way in which events are simply the unique optimal solution to a set of constraints including the final outcome and allows reasoning âbackwardsâ. The alien race exemplifies this other, equally valid, possible way of thinking and viewing the universe, and the protagonist learns their way of thinking by studying their language, which requires seeing written characters as a unified <em>gestalt</em>. This holistic view of the universe as an immutable âblock-universeâ in which events unfold as they must changes the protagonistâs attitude towards life and the tragic death of her daughter, teaching her in a somewhat Buddhist or Stoic fashion to embrace life in both its ups and downs.</p>">âStory Of Your Lifeâ Is Not Time-Travel</a></li>
<li><a href="./Dune-genetics" class="docMetadata" data-popup-title="Genetics and Eugenics in Frank Herbert's <em>Dune</em>" data-popup-author="Gwern Branwen" data-popup-date="5 May 2018" data-popup-abstract="<p>Frank Herbertâs SF <em>Dune</em> series features as a central mechanic a multi-millennium human eugenics breeding program by the Bene Gesserit, which produces the main character, Paul Atreides, with precognitive powers. The breeding program is described as oddly slow and ineffective and requiring roles for incest and inbreeding at some points, which contradict most proposed human eugenics methods. I describe the two main historical paradigms of complex trait genetics, the Fisherian infinitesimal model and the Mendelian monogenic model, the former of which is heavily used in human behavioral genetics and the latter of which is heavily used in agricultural breeding for novel traits, and argue that Herbert (incorrectly but understandably) believed the latter applied to most human traits, perhaps related to his longstanding autodidactic interest in plants &amp;amp; insects &amp;amp; farming, and this unstated but implicit intellectual background shaped <em>Dune</em> and resolves the anomalies.</p>">Genetics &amp; Eugenics in Herbert's <em>Dune</em></a></li>
<li><a href="./Suzanne-Delage" class="docMetadata" data-popup-title="Interpreting 'Suzanne Delage'" data-popup-author="Gwern Branwen" data-popup-date="23 Feb 2009" data-popup-abstract="The many interpretations of a Wolfe story which remains a mystery.">On Wolfeâs âSuzanne Delageâ</a></li>
<li><a href="./Scanners" class="docMetadata" data-popup-title="'Scanners Live in Vain' as realistic SF" data-popup-author="Gwern Branwen" data-popup-date="28 June 2013" data-popup-abstract="<p>Cordwainer Smithâs classic SF short story <q>âScanners Live in Vainâ</q> is remembered in part for its use of the space-madness trope, <q>âthe Great Pain of Spaceâ</q>, usually interpreted symbolically/psychologically by critics. I discuss the state of aerospace medicine in 1945 and subsequent research on <q>âthe breakaway effectâ</q>, <q>âthe overview effectâ</q>, and other unusual psychological states induced by air &amp;amp; space travel, and suggest Smithâs <q>âthe pain of spaceâ</q> is more founded on SF-style speculation &amp;amp; extrapolation of contemporary science/technology and anxieties than is appreciated due to the obscurity of the effects and the relative benignity of the subsequent best documented effects.</p>">âScanners Live in Vainâ as realistic SF</a></li>
<li><a href="./fiction/The-Mulberry-Tree" class="docMetadata" data-popup-title="The Mulberry Tree" data-popup-author="Gwern Branwen" data-popup-date="12 Oct 2010" data-popup-abstract="Essay on writing &amp; rewriting short tanka">The Mulberry Tree</a></li>
<li><a href="./fiction/The-Snowbanks-of-Time" class="docMetadata" data-popup-title="The Snowbanks of Time" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2011" data-popup-abstract="Essay on writing tanka on truth &amp; lies">The Snowbanks of Time</a></li>
</ul></li>
</ul>
</section>
<section id="anime" class="level1">
<h1><a href="#anime" title="Link to section: 'Anime'">Anime</a></h1>
<ul>
<li><a href="./The-Melancholy-of-Kyon" class="docMetadata" data-popup-title="The Melancholy of Kyon" data-popup-author="Gwern Branwen" data-popup-date="08 Jun 2009" data-popup-abstract="<p>The light novel series <em>The Melancholy of Haruhi Suzumiya</em>, featuring a character named Haruhi who is a god unawares and her search for novelty, has a number of anomalies and unclear overarching plot. I argue that these anomalies can be resolved, and greater literary depth achieved, by interpreting the first-person protagonist Kyon as the actual unaware god.</p>">The Melancholy of Kyon</a></li>
<li><em>Death Note</em>:
<ul>
<li><a href="./Death-Note-Anonymity" class="docMetadata" data-popup-title="Death Note: L, Anonymity &amp; Eluding Entropy" data-popup-author="Gwern Branwen" data-popup-date="04 May 2011" data-popup-abstract="<p>In the manga <em>Death Note</em>, the protagonist Light Yagami is given the supernatural weapon âDeath Noteâ which can kill anyone on demand, and begins using it to reshape the world. The genius detective L attempts to track him down with analysis and trickery, and ultimately succeeds. <em>Death Note</em> is almost a thought-experiment-given the perfect murder weapon, how can you screw up anyway? I consider the various steps of Lâs process from the perspective of computer security, cryptography, and information theory, to quantify Lightâs initial anonymity and how L gradually de-anonymizes him, and consider which mistake was the largest.</p><ol type=&quot;1&quot;><li>Mistake 1: Lightâs fundamental mistake is to kill in ways unrelated to his goal. Killing through heart attacks does not just make him visible early on, but the deaths reveals that his assassination method is impossibly precise and something profoundly anomalous is going on. L has been tipped off that Kira exists. Whatever the bogus justification may be, this is a major victory for his opponents. (To deter criminals and villains, it is not necessary for there to be a globally-known single anomalous or supernatural killer, when it would be equally effective to arrange for all the killings to be done naturalistically by ordinary mechanisms such as third parties/police/judiciary or used indirectly as parallel construction to crack cases.)</li><li>Mistake 2: Worse, the deaths are non-random in other waysâthey tend to occur at particular times! Just the scheduling of deaths cost Light 6 bits of anonymity</li><li>Mistake 3: Lightâs third mistake was reacting to the blatant provocation of Lind L. Tailor. L narrowed his target down to 1/3 the original Japanese population, for a gain of ~1.6 bits.</li><li>Mistake 4: Lightâs fourth mistake was to use confidential police information stolen using his policeman fatherâs credentials. This mistake was the largest in bits lost. This mistake cost him 11 bits of anonymity; in other words, this mistake cost him twice what his scheduling cost him and almost 8 times the murder of Tailor!</li><li>Mistake 5: If we assume Penbar was tasked 200 leads out of the 10,000, then murdering him and the fiancee dropped Light just 6 bits or a little over half the fourth mistake and comparable to the original scheduling mistake.</li><li>Endgame: At this point in the plot, L resorts to direct measures and enters Lightâs life directly, enrolling at the university. From this point on, Light is screwed as he is now playing a deadly game of Mafia with L &amp;amp; the investigative team. He frittered away &amp;gt;25 bits of anonymity and then L intuited the rest and suspected him all along.</li></ol><p>Finally, I suggest how Light could have most effectively employed the Death Note and limited his loss of anonymity. In an appendix, I discuss the maximum amount of information leakage possible from using a Death Note as a communication device.</p><p><em>(Note: This essay assumes a familiarity with the early plot of <em><a href=&quot;https://en.wikipedia.org/wiki/Death_Note&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note&quot;>Death Note</a></em> and <a href=&quot;https://en.wikipedia.org/wiki/Light_Yagami&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Light Yagami&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Light Yagami&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a fictional character and the main protagonist of the manga series &amp;lt;i&amp;gt;Death Note&amp;lt;/i&amp;gt;, created by Tsugumi Ohba and Takeshi Obata. He is portrayed as an accomplished yet bored teen genius who finds the Death Note, a supernatural notebook that allows the user to kill anyone by knowing their name and face, after it is dropped by the Shinigami Ryuk. In an effort to create a utopia, Light eventually uses the notebook to murder criminals as the vigilante &amp;lt;b&amp;gt;Kira&amp;lt;/b&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; (ã­ã©)&amp;lt;/span&amp;gt;.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Light Yagami&quot;>Light Yagami</a>; if you are unfamiliar with it, see my <a href=&quot;./Death-Note-Ending&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&amp;#39;s Ending&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;29 Sep 2008&quot; data-popup-abstract=&quot;Ambiguous ending means even the victor is unclear; who was right?&quot;><em>Death Note</em> Ending</a> essay or consult <a href=&quot;https://en.wikipedia.org/wiki/Death_Note#Plot_summary&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Death Note&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;i&amp;gt;&amp;lt;b&amp;gt;Death Note&amp;lt;/b&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;span style=&amp;quot;font-weight:normal&amp;quot;&amp;gt; &amp;lt;/span&amp;gt; is a Japanese manga series written by Tsugumi Ohba and illustrated by Takeshi Obata. The story follows Light Yagami, a teen genius who stumbles across a mysterious otherworldly notebook: the &amp;quot;Death Note&amp;quot;, which belonged to the Shinigami Ryuk, and grants the user the supernatural ability to kill anyone whose name is written in its pages. The series centers around Light&amp;#39;s subsequent attempts to use the Death Note to carry out a world-wide massacre of those whom he deems morally unworthy of life to change the world into a utopian society without crime using the alias of a god-like vigilante named &amp;quot;Kira&amp;quot; and the subsequent efforts of an elite task-force of law enforcement officers, consisting of members of the Japanese police force led by L, an enigmatic international detective whose past is shrouded in mystery, to apprehend him and end his reign of terror.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Death Note#Plot_summary&quot;>Wikipedia</a> or <a href=&quot;http://deathnote.wikia.com/wiki/Rules_of_the_Death_Note&quot; class=&quot;docMetadata&quot; data-popup-image-height=&quot;768&quot; data-popup-image-width=&quot;768&quot; title=&quot;Rules of the Death Note&quot;>read the DN rules</a>.)</em></p>">L, Anonymity &amp; Eluding Entropy</a></li>
<li><a href="./Death-Note-script" class="docMetadata" data-popup-title="Who wrote the 'Death Note' script?" data-popup-author="Gwern Branwen" data-popup-date="2 Nov 2009" data-popup-abstract="<p>I give a history of the 2009 leaked script, discuss internal &amp;amp; external evidence for its realness including stylometrics; and then give a simple step-by-step Bayesian analysis of each point. We finish with high confidence in the script being real, discussion of how this analysis was surprisingly enlightening, and what followup work the analysis suggests would be most valuable.</p>">Who Wrote The Movie?</a></li>
<li><a href="./Death-Note-Ending" class="docMetadata" data-popup-title="Death Note's Ending" data-popup-author="Gwern Branwen" data-popup-date="29 Sep 2008" data-popup-abstract="Ambiguous ending means even the victor is unclear; who was right?">On The Ending</a></li>
</ul></li>
<li><a href="./MLP" class="docMetadata" data-popup-title="MLP: Immanetizing The Equestrian" data-popup-author="Gwern Branwen" data-popup-date="24 Oct 2018" data-popup-abstract="<p>I watch the 2010 Western animated series <em>My Little Pony: Friendship is Magic</em> (seasons 1â8), delving deep into it and the MLP fandom, and reflect on it. What makes it good and powers its fandom subculture, producing a wide array of fanfictions, music, and art? Focusing on fandom, plot, development, and meaning of bronydom, I conclude that, among other things, it has surprisingly high-quality production &amp;amp; aesthetics which are easily adapted to fandom and which power a Westernized shonen animeâwhich depicts an underappreciated contemporary <em>capitalist</em> utopian perspective on self-actualization, reminiscent of other more explicitly self-help-oriented pop culture movements such as the recent Jordan B. Peterson movement. Included are my personal rankings of characters, seasons, episodes, and official &amp;amp; fan music.</p>"><em>My Little Pony</em>: Immanetizing The Equestrian</a></li>
<li><a href="./Hafu" class="docMetadata" data-popup-title="Hafu Gender Ratios in Anime" data-popup-author="Gwern Branwen" data-popup-date="06 Apr 2011" data-popup-abstract="Race as reflected in gender ratios within fictional bi-racial marriages in anime/manga show equal sex ratios and Western European overrepresentation with striking absence of Korean characters.">Hafu Gender Ratios in Anime</a></li>
<li><a href="./Arias-past-present-and-future" class="docMetadata" data-popup-title="Aria's Past, Present, and Future" data-popup-author="Gwern Branwen" data-popup-date="13 Jul 2011" data-popup-abstract="On divining the esoteric truth of Neo-Venezia through holes in world-building."><em>Aria</em>âs past, present, &amp; future</a></li>
<li><a href="./FMP-parody" class="docMetadata" data-popup-title="Parody in 'Full Metal Panic!'" data-popup-author="Gwern Branwen" data-popup-date="09 Oct 2008" data-popup-abstract="The unexpected critical depths of a single throw-away scene"><em><span class="smallcaps-auto">FMP</span></em> parody</a></li>
<li><a href="./komm-susser-tod" class="docMetadata" data-popup-title="Komm Susser Tod" data-popup-author="Gwern Branwen" data-popup-date="05 Nov 2010" data-popup-abstract="Perspective and interpretation">âKomm Susser Todâ</a></li>
</ul>
</section>
<section id="docs" class="level1">
<h1><a href="#docs" title="Link to section: 'Docs'">Docs</a></h1>
<ul>
<li><a href="./docs/radiance/2002-scholz-radiance" class="docMetadata" data-popup-title="Radiance: A Novel" data-popup-author="Carter Scholz, Gregory Benford, Hugh Gusterson, Sam Cohen, Curtis LeMay" data-popup-date="6 July 2013" data-popup-abstract="E-book edition of the 2002 Carter Scholz novel of post-Cold War science/technology, extensively annotated with references and related texts."><em>Radiance</em></a> (annotated Scholz novel)</li>
<li><a href="./docs/xrisks/1985-hofstadter" class="docMetadata" data-popup-title="Metamagical Themas: Sanity and Survival" data-popup-author="Douglas Hofstadter" data-popup-date="16 Apr 2012" data-popup-abstract="3 essays by AI researcher Douglas Hofstadter exploring cooperation/game theory/'superrationality' in the context of the failure of political coordination to prevent global nuclear war"><em>Metamagical Themas</em>: âSanity and Survivalâ</a> (Hofstadter)</li>
<li><a href="./docs/sociology/1987-rossi" class="docMetadata" data-popup-title="The Iron Law Of Evaluation And Other Metallic Rules" data-popup-author="Peter H. Rossi" data-popup-date="18 Sep 2012" data-popup-abstract="<p><q>â<span class=&quot;smallcaps&quot;>The Iron Law Of Evaluation</span> And Other Metallic Rulesâ</q> is a classic review paper by American <q>â<a href=&quot;https://en.wikipedia.org/wiki/Sociology&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Sociology&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Sociology&amp;lt;/b&amp;gt; is the study of society, patterns of social relationships, social interaction and culture of everyday life. It is a social science that uses various methods of empirical investigation and critical analysis to develop a body of knowledge about social order, acceptance, and change or social evolution. While some sociologists conduct research that may be applied directly to social policy and welfare, others focus primarily on refining the theoretical understanding of social processes. Subject matter ranges from the micro-sociology level of individual agency and interaction to the macro level of systems and the social structure.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Sociology&quot;>sociologist</a> <a href=&quot;http://www.asanet.org/footnotes/dec06/indextwo.html&quot;>Peter Rossi</a>, a dedicated progressive and the nationâs leading expert on social program evaluation from the 1960s through the 1980sâ</q>; it discusses the difficulties of creating a useful <a href=&quot;https://en.wikipedia.org/wiki/social_program&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Welfare&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Welfare&amp;lt;/b&amp;gt; is a type of government support for the citizens of that society. Welfare may be provided to people of any income level, as with social security, but it is usually intended to ensure that people can meet their basic human needs such as food and shelter. Welfare attempts to provide a minimal level of well-being, usually either a free- or a subsidized-supply of certain goods and social services, such as healthcare, education, and vocational training.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: social program&quot;>social program</a>, and proposed some aphoristic summary rules, including most famously: &amp;gt; &amp;gt; &amp;gt; - The Iron law: <q>âThe expected value of any net impact assessment of any large scale social program is zeroâ</q> &amp;gt; &amp;gt; - the Stainless Steel law: <q>âthe better designed the impact assessment of a social program, the more likely is the resulting estimate of net impact to be zero.â</q> &amp;gt; &amp;gt; It expands an earlier paper by Rossi (<a href=&quot;../../docs/sociology/1978-rossi.pdf&quot;><q>âIssues in the evaluation of human services deliveryâ</q></a>, Rossi 1978), where he coined the first, <q>âIron Lawâ</q>.</p>">âThe Iron Law of Evaluation And Other Metallic Rulesâ</a> (Rossi)</li>
<li><a href="./docs/cs/1955-nash" class="docMetadata" data-popup-title="John Nash on cryptography" data-popup-author="John Nash" data-popup-date="22 Feb 2012" data-popup-abstract="1955 letters of John Nash and the NSA on a cryptosystem and Nash's belief that near-perfect cryptography could exploit exponential difficulties">John Nash on cryptography &amp; P=NP (1955)</a></li>
<li><a href="./docs/statistics/bayes/1994-falk" class="docMetadata" data-popup-title="The Ups and Downs of the Hope Function In a Fruitless Search" data-popup-author="Ruma Falk, Abigail Lipson, Clifford Konold" data-popup-date="01 Jul 2012" data-popup-abstract="On Bayesian updating of beliefs in sequentially searching a set of possibilities where failure is possible, such as waiting for a bus; the psychologically counterintuitive implication is that success on the next search increases even as the total probability of success decreases.">âUps &amp; Downs of the Hope Function In a Fruitless Searchâ</a></li>
<li><a href="./docs/bitcoin/2008-nakamoto" class="docMetadata" data-popup-title="Wei Dai/Satoshi Nakamoto 2009 Bitcoin emails" data-popup-author="Satoshi Nakamoto, Wei Dai" data-popup-date="17 Mar 2014" data-popup-abstract="Emails in 2009 between Wei Dai and Satoshi Nakamoto discussing Bitcoin draft proposal and B-money">Wei Dai/Satoshi Nakamoto 2009 Bitcoin emails</a></li>
<li><a href="./docs/bitcoin/2011-davis" class="docMetadata" data-popup-title="The Crypto-Currency: Bitcoin and its mysterious inventor" data-popup-author="Joshua Davis" data-popup-date="18 Apr 2013" data-popup-abstract="<p><span class=&quot;smallcaps&quot;>Dept. Of Technology</span> about <a href=&quot;https://en.wikipedia.org/wiki/bitcoin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Bitcoin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Bitcoin&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;â¿&amp;lt;/b&amp;gt;) is a cryptocurrency. It is a decentralized digital currency without a central bank or single administrator that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: bitcoin&quot;>bitcoin</a> [sic] and its mysterious creator. There are lots of ways to make money: You can earn it, find it, counterfeit it, steal it. Or, if youâre <a href=&quot;https://en.wikipedia.org/wiki/Satoshi_Nakamoto&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Satoshi Nakamoto&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Satoshi Nakamoto&amp;lt;/b&amp;gt; is the name used by the pseudonymous person or persons who developed bitcoin, authored the bitcoin white paper, and created and deployed bitcoin&amp;#39;s original reference implementation. As part of the implementation, Nakamoto also devised the first blockchain database. In the process, Nakamoto was the first to solve the double-spending problem for digital currency using a peer-to-peer network. Nakamoto was active in the development of bitcoin up until December 2010.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Satoshi Nakamoto&quot;>Satoshi Nakamoto</a>, you can invent it. Thatâs what he did on the evening of January 3, 2009, when he pressed a button on his keyboard and created a new currency called Bitcoin. It was all bit, and no coin. There was no paper, copper, or silver-just thirty-one thousand lines of code and an announcement on the Internet. Nakamoto wanted to create a currency immune to the predations of bankers and politicians. The currency was controlled entirely by software. Every ten minutes or so, coins would be distributed through a process that resembled a lottery. This way, the bitcoin software would release a total of twenty-one million bitcoins, most all of them over the next twenty years. Interest in Nakamotoâs invention built steadily. More and more people dedicated their computers to the lottery, and forty-four exchanges popped up, allowing anyone with bitcoins to trade them for dollars, euros, or other currencies. At first, a single bitcoin was valued at less than a penny. But merchants gradually began to accept bitcoin, and at the end of 2010 the value began to appreciate rapidly. By June of 2011, a bitcoin was worth more than twenty-nine dollars. Market gyrations followed, and by September the exchange rate had fallen to five dollars. Still, with more than seven million bitcoins in circulation, Nakamoto had created thirty-five million dollars of value. And yet Nakamoto was a cipher. There was no trace of any coder with that name before the dÃ©but of bitcoin. He used an e-mail address and Web site that were untraceable. In 2009 and 2010, he wrote hundreds of posts in flawless English, invited other software developers to help him improve the code. Then, in April, 2011, he sent a note to a developer saying that he had âmoved on to other things.â He has not been heard from since. Tells about failed attempts to hack the bitcoin encryption code. Writer tries to deduce Nakamotoâs true identity from clues in his posts and his code. Describes the <a href=&quot;https://en.wikipedia.org/wiki/International_Cryptology_Conference&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;International Cryptology Conference&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;CRYPTO&amp;lt;/b&amp;gt;, the &amp;lt;b&amp;gt;International Cryptology Conference&amp;lt;/b&amp;gt;, is one of the largest academic conferences in cryptography and cryptanalysis. It is organized by the International Association for Cryptologic Research (IACR), and it is held yearly in August in Santa Barbara, California at the University of California, Santa Barbara.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: International Cryptology Conference&quot;>Crypto</a> 2011 conference of cryptographers, where the writer went looking for Nakamoto. Writer speaks with two possible candidates, Michael Clear and Vili Lehdonvirta, both of whom deny that they are Nakamoto. Also tells about Kevin Groce, who runs a bitcoin-mining operation in Kentucky. Over the summer, hackers targeted bitcoin, and though they were unable to break Nakamotoâs code, they were able to disrupt the exchanges and destroy Web sites that helped users store bitcoins. The number of transactions decreased and the exchange rate plummeted. Commentators predicted the end of the currency. In September, however, volume began to increase again, and the price stabilized, at least temporarily.</p>">âThe Crypto-Currency: Bitcoin &amp; its mysterious inventorâ</a></li>
<li><a href="./docs/japanese/2002-gibson" class="docMetadata" data-popup-title="Shiny balls of Mud: William Gibson Looks at Japanese Pursuits of Perfection" data-popup-author="William Gibson" data-popup-date="20 Apr 2012" data-popup-abstract="Essay on minimalism, otaku, and hikikomori as esthetic choices reflecting an obsessive focus on perfection of a single activity, exemplified by the unusual sculpture form _dorodango_  (hand-rolling mud into colorful spheres).">âShiny balls of Mudâ</a> (Gibson)</li>
<li><a href="./docs/culture/2007-wolfe" class="docMetadata" data-popup-title="Nor the Summers as Golden: Writing Multivolume Works" data-popup-author="Gene Wolfe" data-popup-date="02 Mar 2012" data-popup-abstract="Gene Wolfe on the depth and ending of novel series">âNor the Summers as Golden: Writing Multivolume Worksâ</a> (Wolfe)</li>
<li><a href="./docs/culture/1983-wolfe-thecitadeloftheautarch-thejustman" class="docMetadata" data-popup-title="Loyal to the Group of Seventeen's StoryâThe Just Man" data-popup-author="Gene Wolfe" data-popup-date="20 Jan 2018" data-popup-abstract="Short story on the limits of propaganda and 'Newspeak' using a constructed language; from Chapter 11 of Gene Wolfe's _The Book of the New Sun_, volume 4, _The Citadel of the Autarch_.">âLoyal to the Group of Seventeen's StoryâThe Just Manâ</a> (Wolfe)</li>
<li><a href="./docs/culture/1963-asimov" class="docMetadata" data-popup-title="The Sword of Achilles" data-popup-author="Isaac Asimov" data-popup-date="31 Aug 2011" data-popup-abstract="interest in science fiction predicts future scientific merit?">âThe Sword of Achillesâ</a> (Asimov)</li>
<li><a href="./docs/sr/2013-power" class="docMetadata" data-popup-title="Drugs 2.0: Your Crack's in the Post" data-popup-author="Mike Power" data-popup-date="19 Oct 2013" data-popup-abstract="<p>This is an annotated transcript of the chapter âYour Crackâs in the Postâ (pg219-244) &amp;amp; an excerpt from the chapter âProhibition in the Digital Ageâ (pg262), of <a href=&quot;https://www.amazon.com/Drugs-2-0-Revolution-Thats-Changing-ebook/dp/B00BLCAD4O?tag=gwernnet-20&quot;><em>Drugs 2.0: The Web Revolution Thatâs Changing How the World Gets High</em></a>, <a href=&quot;http://mikepower.pressfolios.com/&quot;>Mike Power</a> (2 May 2013); it is principally on the topic of <a href=&quot;https://en.wikipedia.org/wiki/Bitcoin&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Bitcoin&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Bitcoin&amp;lt;/b&amp;gt; (&amp;lt;b&amp;gt;â¿&amp;lt;/b&amp;gt;) is a cryptocurrency. It is a decentralized digital currency without a central bank or single administrator that can be sent from user to user on the peer-to-peer bitcoin network without the need for intermediaries.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Bitcoin&quot;>Bitcoin</a>, <a href=&quot;https://en.wikipedia.org/wiki/Tor_%28anonymity_network%29&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Tor (anonymity network)&quot; data-popup-author=&quot;English Wikipedia&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;&amp;lt;b&amp;gt;Tor&amp;lt;/b&amp;gt; is free and open-source software for enabling anonymous communication. The name is derived from an acronym for the original software project name &amp;quot;The Onion Router&amp;quot;. Tor directs Internet traffic through a free, worldwide, volunteer overlay network consisting of more than seven thousand relays to conceal a user&amp;#39;s location and usage from anyone conducting network surveillance or traffic analysis. Using Tor makes it more difficult to trace Internet activity to the user: this includes &amp;quot;visits to Web sites, online posts, instant messages, and other communication forms&amp;quot;. Tor&amp;#39;s intended use is to protect the personal privacy of its users, as well as their freedom and ability to conduct confidential communication by keeping their Internet activities from being monitored.&amp;lt;/p&amp;gt;&quot; title=&quot;Wikipedia: Tor (anonymity network)&quot;>Tor</a>, and <a href=&quot;../../Silk-Road&quot; class=&quot;docMetadata&quot; data-popup-title=&quot;Silk Road 1: Theory &amp;amp; Practice&quot; data-popup-author=&quot;gwern&quot; data-popup-date=&quot;gwern&quot; data-popup-abstract=&quot;&amp;lt;p&amp;gt;The cypherpunk movement laid the ideological roots of Bitcoin and the online drug market Silk Road; balancing previous emphasis on cryptography, I emphasize the non-cryptographic market aspects of Silk Road which is rooted in cypherpunk economic reasoning, and give a fully detailed account of how a buyer might use market information to rationally buy, and finish by discussing strengths and weaknesses of Silk Road, and what future developments are predicted by cypherpunk ideas.&amp;lt;/p&amp;gt;&quot;>Silk Road 1</a>.</p>"><em>Drugs 2.0</em>: âYour Crackâs in the Postâ</a> (Mike Power)</li>
<li><a href="./docs/bitcoin/2014-mccaleb" class="docMetadata" data-popup-title="2014 Jed McCaleb MtGox interview" data-popup-author="Jed McCaleb" data-popup-date="16 Feb 2014" data-popup-abstract="Email interview with McCaleb on early history of his MtGox, verifying it did not trade real but online Magic cards">2014 Jed McCaleb MtGox interview</a></li>
<li><a href="./Fulltext" class="docMetadata" data-popup-title="Research Bounties On Fulltexts" data-popup-author="Gwern Branwen" data-popup-date="31 Dec 2018" data-popup-abstract="A list of papers/books/materials I have failed to obtain, and financial bounties for anyone who can provide copies to me or the Internet.">Research Bounties On Documents</a></li>
</ul>
</section>
<section id="anime-docs" class="level1">
<h1><a href="#anime-docs" title="Link to section: 'Anime docs'">Anime docs</a></h1>
<ul>
<li><a href="./otaku" class="docMetadata" data-popup-title="Neon Genesis Evangelion source anthology" data-popup-author="Gwern Branwen" data-popup-date="30 Sep 2009" data-popup-abstract="<p>This page is an Extensive anthology of Gainax/Hideaki Anno/<em>Evangelion</em>-related quotes, excerpts, sources, references, &amp;amp; analyses, organized by reliability &amp;amp; year.</p><p>The purpose of compiling a large page of quotes &amp;amp; references classified by date &amp;amp; source level is to make it easier to put NGE into a historical context by tracing the evolution of plot or characters, cross-reference statements made in interviews, jump forward and backwards to flesh out otherwise obscure allusions to events, and enable easy keyword-based search for various concepts (eg. the connection of Kaworu to cats, Gainaxâs bafflement that viewers might think Misato killed Kaji, the influence of earthquakes on people, connections to Aum Shinrikyo, garbled information about suicide attempts, Annoâs conservative nationalist views or philosophy of âpoisonâ, retcons like swapping the Adam and Lilith plot devices, panspermia &amp;amp; First Ancestral Race being slowly removed from production materials and then post-NGE slowly restored, the many conflicting pieces of information on the end of NGE TV and <em>EoE</em>, Yamagaâs questionable reliability etc).</p><p>As I compile more material, I become increasingly convinced that far from <em>Evangelion</em> being a baffling mystery, it is in fact one of the most understandable anime out there, with a wealth of information about almost every detail, from the earliest planning meetings to how long particular episode productions took to the source of minor details like the âA-10 nerveâ, and that Hideaki Anno, far from being a reticent auteur of mystery, has collectively been forthcoming about anything one might ask - to the point where multiple interviews could justly be described as âbook-lengthâ (the books in question being_June_, <em>Schizo</em>, <em>Prano</em>, the <em>1.0 CRC</em>, &amp;amp; the <em>2.0 CRC</em>). There is so much material that half the difficulty is simply collating the existing materials, and some extensive sources seem to have been lost to both the Japanese and English fandoms (eg. there seem to be no mentions or quotations of the <em>Anata to Watashi no Gainax</em> interviews in the Japanese web).</p>"><em>Evangelion</em> sources anthology</a></li>
<li><a href="./docs/eva/2002-takeda-notenkimemoirs" class="docMetadata" data-popup-title="The Notenki Memoirs: Studio Gainax And The Men Who Created Evangelion" data-popup-author="Yasuhiro Takeda" data-popup-date="27 Dec 2010" data-popup-abstract="Fulltext annotated e-book of 2002 memoir by anime producer Yasuhiro Takeda, discussing Japanese SF conventions &amp; fandom, formation &amp; history of Gainax and its productions up to 2002, including the origins of Evangelion &amp; the tax raid"><em>The Notenki Memoirs</em></a> (Takeda)</li>
<li><a href="./docs/eva/2010-crc" class="docMetadata" data-popup-title="Evangelion 2.0 Complete Records Collection" data-popup-author="Ryusuke Hikawa, Hideaki Anno, Shinji Higuchi, YÅji Enokido, Kazuya Tsurumaki, Mohiro Kitoh, Shigeto Koyama, Yoshito Asari" data-popup-date="29 Aug 2011" data-popup-abstract="Translated interviews about making 'Evangelion 2.0' w/Anno, Higuchi, Enokido, &amp; Tsurumaki"><em><span class="smallcaps-auto">NGE</span> 2.0 Complete Records Collection (CRC)</em></a></li>
<li><a href="./docs/eva/1996-animerica-conscience-otaking" class="docMetadata" data-popup-title="The Conscience of the Otaking: The Studio Gainax Saga in Four Parts" data-popup-author="Toshio Okada" data-popup-date="03 Oct 2011" data-popup-abstract="Interview of former Gainax president Toshio Okada on Gainax's history, _Wings of Honneamise_, _Aoki Uru_, etc.">âConscience of the Otakingâ</a> (Okada)</li>
<li><a href="./docs/eva/2011-house" class="docMetadata" data-popup-title="Interviewing translator Michael House" data-popup-author="Michael House" data-popup-date="11 Nov 2011" data-popup-abstract="Working at Gainax, Evangelion's production &amp; censorship, anime translation">âGainax translator Michael House interviewâ</a></li>
<li><a href="./docs/eva/1996-newtype-anno-interview" title="English translation of French translation of controversial NewType interview at the end of the TV broadcast">1996 Hideaki Anno interview</a></li>
<li><a href="./docs/eva/1997-animeland-may-hideakianno-interview-english" class="docMetadata" data-popup-title="May 1997 AnimeLand Interview with Hideaki Anno (English)" data-popup-author="Hideaki Anno" data-popup-date="28 Feb 2012" data-popup-abstract="English translation of a French anime journalist's interview of Hideaki Anno on anime and Evangelion">1997 Hideaki Anno interview</a> (<a href="./docs/eva/1997-animeland-may-hideakianno-interview-french" class="docMetadata" data-popup-title="Interview with Hideaki Anno (French)" data-popup-author="Hideaki Anno" data-popup-date="28 Feb 2012" data-popup-abstract="Transcript of a French anime writer discussion of anime and Evangelion">French</a>)</li>
<li><a href="./docs/eva/2003-oshii-izubuchi" class="docMetadata" data-popup-title="Talk About RahXephon: In Search of Fantasy and Details" data-popup-author="Mamoru Oshii, Yutaka Izubuchi" data-popup-date="28 Feb 2012" data-popup-abstract="Oshii criticizes RahXephon and compares it to Neon Genesis Evangelion">âTalk About RahXephonâ</a></li>
<li><a href="./docs/eva/2003-rahxephoncomplete-anno-izubuchi" class="docMetadata" data-popup-title="Special Talk: Yutaka Izubuchi x Hideaki Anno" data-popup-author="Gwern Branwen" data-popup-date="28 Feb 2012" data-popup-abstract="Discussion by Izubuchi and Anno of classic mecha anime">âSpecial Talk: Yutaka Izubuchi x Hideaki Annoâ</a></li>
<li><a href="./docs/eva/2004-okada" class="docMetadata" data-popup-title="Otaku Talk" data-popup-author="Toshio Okada, Kaichiro Morikawa, Takashi Murakami, Reiko Tomii" data-popup-date="09 Apr 2012" data-popup-abstract="Definition of otaku, mania, moe, dame, anime, and generations">âOtaku Talkâ</a> (Okada, Morikawa etc)</li>
<li><a href="./docs/eva/2005-murakami" class="docMetadata" data-popup-title="Earth in My Window" data-popup-author="Takashi Murakami" data-popup-date="04 Mar 2012" data-popup-abstract="Essay by Pop Art artist Takashi Murakami on Japanese society and on WWII infantilizing Japanese culture as revealed by media, anime, and otaku.">âEarth in My Windowâ</a> (Murakami)</li>
<li><a href="./docs/anime/1997-utena" class="docMetadata" data-popup-title="Utena 2011 Boxset Booklet Commentary" data-popup-author="Kunihiko Ikuhara, Yuichirou Oguro, Hiroshi Kaneda, Haruyasu Yamazaki, Tomomi Takemura, Hideki Ito, Yo Yamada, Tomokazu Mii, Yoji Enokido, Shinya Hasegawa, J.A. Caesar, Toshimichi Otsuki, Chiho Saito, Sarah Alys Lindholm, C.A.P." data-popup-date="7 Feb 2013" data-popup-abstract="Kunihiko Ikuhara and staff episode and music commentary/discussion 1997-2011 on the anime 'Revolutionary Girl Utena'; discusses origins of ideas and meaning of themes, and video/audio remastering for the DVD box set."><em>Utena</em> 2011 Boxset Commentary</a></li>
<li><a href="./docs/eva/2005-sawaragi" class="docMetadata" data-popup-title="On The Battlefield of " data-popup-author="Noi Sawaragi" data-popup-date="18 May 2012" data-popup-abstract="Post-WWII ambiguity causes otakudom">âOn The Battlefield of âSuperflatââ</a> (Sawaragi)</li>
<li><a href="./docs/anime/2010-sarrazin" class="docMetadata" data-popup-title="Ero-Anime: Manga Comes Alive" data-popup-author="Stephen Sarrazin" data-popup-date="23 Dec 2011" data-popup-abstract="Short history of trends in Japanese erotic anime and manga, especially lolicon">âEro-Anime: Manga Comes Aliveâ</a> (Sarrazin)</li>
</ul>
</section>
<section id="wikipedia" class="level1">
<h1><a href="#wikipedia" title="Link to section: 'Wikipedia'">Wikipedia</a></h1>
<ul>
<li><a href="./In-Defense-Of-Inclusionism" class="docMetadata" data-popup-title="In Defense of Inclusionism" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="<p>English Wikipedia is in decline. As a long-time editor &amp;amp; former admin, I was deeply dismayed by the process. Here, I discuss UI principles, changes in Wikipedian culture, the large-scale statistical evidence of decline, run small-scale experiments demonstrating the harm, and conclude with parting thoughts.</p>">In Defense Of Inclusionism</a></li>
<li><a href="./Wikipedia-and-Dark-Side-Editing" class="docMetadata" data-popup-title="Wikipedia and Dark Side Editing" data-popup-author="Gwern Branwen" data-popup-date="24 May 2009" data-popup-abstract="Cynical tactics encouraged by Wikipedia's abdication of thought known as 'No Original Research' and 'Reliable Sources'">Wikipedia &amp; Darkside Editing</a></li>
<li><a href="./Wikipedia-and-Other-Wikis" class="docMetadata" data-popup-title="Wikipedia and Other Wikis" data-popup-author="Gwern Branwen" data-popup-date="27 Jan 2009" data-popup-abstract="Network effects &amp; benefits of gritting one's teeth &amp; submitting to a Wikipedia's rules, rather than using Wikia or one's own site.">Wikipedia &amp; Other Wikis</a></li>
<li><a href="./Wikipedia-and-YouTube" class="docMetadata" data-popup-title="Wikipedia &amp; YouTube" data-popup-author="Gwern Branwen" data-popup-date="15 Jan 2009" data-popup-abstract="Why Wikipedia and YouTube will never be integrated.">Wikipedia &amp; YouTube</a></li>
<li><a href="./Wikipedia-and-Knol" class="docMetadata" data-popup-title="Wikipedia &amp; Knol: Why Knol Already Failed" data-popup-author="Gwern Branwen" data-popup-date="21 Jan 2009" data-popup-abstract="Why Knol is worse than Wikipedia, has failed, and will continue to fail.">Wikipedia &amp; Knol</a></li>
<li><a href="./Wikipedia-resume" class="docMetadata" data-popup-title="Wikipedia RÃ©sumÃ©" data-popup-author="Gwern Branwen" data-popup-date="18 Oct 2010" data-popup-abstract="A precis of my work on the English Wikipedia, 2004-2017 (edit counts, articles by subject, and particularly notable articles)">My Wikipedia articles</a></li>
</ul>
</section>
<section id="personal" class="level1">
<h1><a href="#personal" title="Link to section: 'Personal'">Personal</a></h1>
<ul>
<li><a href="./About" class="docMetadata" data-popup-title="About This Website" data-popup-author="Gwern Branwen" data-popup-date="01 Oct 2010" data-popup-abstract="Meta page describing gwern.net site ideals of stable long-term essays which improve over time; technical decisions using Markdown and static hosting; idea sources and writing methodology; metadata definitions; semi-annual web traffic statistics; copyright license">About</a> / <a href="./Links" class="docMetadata" data-popup-title="Links" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Who am I online &amp; what have I done? Contact information; sites I use; computers and software tools; things I've worked on; psychological profiles">Links</a> / <a href="./tags/newsletter" class="docMetadata" data-popup-title="Gwern.net newsletter archives" data-popup-author="Gwern Branwen" data-popup-date="2013-12-01" data-popup-abstract="Newsletter tag: archive of all issues back to 2013 for the gwern.net newsletter (monthly updates, which will include summaries of projects I've worked on that month (the same as the <a href=&quot;https://www.gwern.net/Changelog&quot;>changelog</a>), collations of links or discussions from <a href=&quot;https://www.reddit.com/r/gwern/&quot;>my subreddit</a>, and book/movie reviews.)">newsletter archives</a></li>
<li><a href="./Notes" class="docMetadata" data-popup-title="Notes" data-popup-author="Gwern Branwen" data-popup-date="05 Aug 2009" data-popup-abstract="Misc thoughts, memories, proto-essays, musings, etc.">Notes</a></li>
<li><a href="./Epigrams" class="docMetadata" data-popup-title="Epigrams" data-popup-author="Gwern Branwen" data-popup-date="30 April 2014" data-popup-abstract="Witticisms, parodies, pointed observations, japeries, jocularity, Tom Swifties, examples of nominative determinism, and discursive drollery">Epigrams</a></li>
<li><a href="./Book-reviews" class="docMetadata" data-popup-title="Ratings and reviews" data-popup-author="Gwern Branwen" data-popup-date="23 Aug 2013" data-popup-abstract="A list of books I have read since ~1997, with reviews.">Book reviews/ratings</a></li>
<li><a href="./Tea" class="docMetadata" data-popup-title="Tea Reviews" data-popup-author="Gwern Branwen" data-popup-date="13 Apr 2011" data-popup-abstract="Teas I have drunk, with reviews and future purchases; focused primarily on oolongs and greens. Plus experiments on water.">Tea reviews</a>/<a href="./Mead" class="docMetadata" data-popup-title="Mead" data-popup-author="Gwern Branwen" data-popup-date="02 May 2012" data-popup-abstract="Ratings of mead and fruit wines I have tried.">Mead</a></li>
<li><a href="./Blackmail" class="docMetadata" data-popup-title="Blackmail fail" data-popup-author="Gwern Branwen" data-popup-date="10 Dec 2013" data-popup-abstract="<p>In September 2012, I was extorted for $32 for being gwern; I declined to pay. In November 2013, I called an encryption bluff that I was Dread Pirate Roberts. In December 2013, a crazy person tried to blackmail me for billions of dollars for being Satoshi Nakamoto; I declined to pay. In March 2014, the DNM Evolution threatened to dox me if I did not reveal information about their security vulnerabilities. In February 2015, an Agora user doxed me in an unexpected way and I paid a small bounty.</p>">Blackmail attempts</a></li>
</ul>
</section>
</div>
</article>
</main>

<link rel="stylesheet" type="text/css" href="/static/css/fonts.css">
<link rel="stylesheet" type="text/css" href="/static/css/default.css">


<script src="/static/js/popups.js" defer=""></script>

<script src="/static/js/darkmode.js" defer=""></script>

<script id="googleAnalytics" src="https://www.googletagmanager.com/gtag/js?id=UA-18912926-1" async=""></script>
<script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-18912926-1');
    </script>


<div id="ui-elements-container"><div id="mode-selector"><button type="button" class="select-mode-auto selected" tabindex="-1" data-name="auto" title="Set light or dark mode automatically, according to system-wide setting" disabled="">Auto</button><button type="button" class="select-mode-light active" tabindex="-1" data-name="light" title="Light mode at all times">Light</button><button type="button" class="select-mode-dark" tabindex="-1" data-name="dark" title="Dark mode at all times">Dark</button></div></div></body><div id="popup-container" class=""></div>
</html>
